{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"GeoEPIC"},{"location":"#python-package-for-geospatial-crop-simulations","title":"Python package for geospatial Crop Simulations","text":"<p>GeoEPIC is an open source package that expands the capabilities of the EPIC crop simulation model, to simulate crop growth and development across large geographies, such as entire states or counties by leveraging openly availabe remote sensing products and geospatial databases. Additionally, the package features a unique calibration module that allows fine-tuning of model parameters to reflect specific local conditions or experimental results. This toolkit allows researchers to assess crop production potential, management scenarios and risks at broader scales, informing decision-making for sustainable agricultural practices.</p> Go to Overview Install GeoEPIC"},{"location":"#what-is-epic-model","title":"What is EPIC Model?","text":"<p>The Environmental Policy Integrated Climate (EPIC) Model is a process-based tool designed to simulate agricultural systems at the field, farm, or small watershed scale under uniform climate, soil, land use, and topographic conditions. It is capable of addressing numerous facets of agricultural sustainability and environmental analysis.</p>"},{"location":"#key-features-of-epic","title":"Key Features of EPIC","text":"<ul> <li>Simulates crop yields under various management practices.</li> <li>Calculates leaf area index (LAI) and net ecosystem exchange (NEE) for plants.</li> <li>Evaluates soil erosion (wind, sheet, and channel) and water quality impacts.</li> <li>Models nutrient cycling for sustainable agricultural management.</li> <li>Incorporates pesticide fate including runoff, leaching, and degradation.</li> <li>Assesses impacts of climate change and CO\u2082 concentrations on agriculture.</li> <li>Designs systems for biomass production for energy and economic feasibility.</li> <li>Simulates diverse agricultural management practices like irrigation, crop rotation, and tillage.</li> </ul> <p>EPIC has evolved to address global challenges and is widely used in environmental and agricultural research. To extend its capabilities even further, the model can be integrated with GeoEPIC for enhanced spatial analyses.</p> <p>For more details on EPIC model, visit Texas A&amp;M AgriLife site.</p>"},{"location":"#what-can-you-do-with-geoepic","title":"What can you do with GeoEPIC?1. Getting Input Files2. Site Simulation3. Model Calibration4. Remote Sensing","text":"<p>GeoEPIC enables you to download and customize essential input files for crop simulations, including:</p> <ul> <li>Soil Data: Access soil properties from sources like USDA SSURGO and ISRIC SoilGrids.</li> <li>Site Data: Specify location parameters such as latitude, longitude, elevation, and slope.</li> <li>Crop Management Data: Define planting schedules, irrigation, fertilization, and tillage operations.</li> <li>Weather Data: Extract historical and forecasted weather information, including temperature, precipitation, and solar radiation, from datasets like AgERA5 on Google Earth Engine.</li> </ul> <p>Modifying these inputs to reflect local conditions ensures more accurate simulation results.</p> Learn More <p>GeoEPIC enhances the EPIC crop simulation model by enabling large-scale geospatial simulations. It allows users to model crop growth, yields, and water usage across extensive areas, such as entire states or counties. By integrating remote sensing data and geospatial databases, GeoEPIC facilitates the creation of detailed model inputs, supporting assessments of crop production potential and the evaluation of various management scenarios.</p> Learn More <p>GeoEPIC's Calibration Module enables precise tuning of the EPIC model parameters to align simulations with observed data, such as Leaf Area Index (LAI), Net Ecosystem Exchange (NEE), crop yield, or biomass. This calibration process ensures that the model accurately reflects specific local conditions or experimental results, enhancing the reliability of simulations.</p> Learn More <p>GeoEPIC's Remote Sensing Module leverages satellite imagery and geospatial data to enhance crop simulation accuracy. By integrating data from platforms like Google Earth Engine, users can incorporate up-to-date environmental information into their models, leading to more precise assessments of crop growth and productivity.</p> Learn More"},{"location":"#additional-resources","title":"Additional Resources","text":"<p>Explore more about GeoEPIC through the following resources:</p> <ol> <li> <p>Tutorials   These tutorials provide detailed examples of what you can achieve using GeoEPIC, including site simulation, model calibration, and creating site input files. Explore the guides below to enhance your understanding:</p> <ul> <li>Site Simulation Tutorial</li> <li>Calibration Example</li> <li>Creating Site Input Files Tutorial</li> <li>Spatial Crop Simulations Tutorial</li> <li>USDA Soil Data Access Guide</li> </ul> </li> <li> <p>Python API Reference Python API Reference contains detailed information about the various functions and classes available in GeoEPIC. This helps you to import the package into your Python scripts and utilize its functionalities directly, enabling efficient geospatial crop simulations and data analysis.  </p> </li> <li> <p>Contribution Contribution Guide contains instructions on how to contribute. This document outlines how to contribute to GeoEPIC through code changes, bug reports, feature requests, and other helpful contributions.</p> </li> </ol>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Bharath Irigireddy, Varaprasad Bandaru, Sachin Velmurgan, SMaRS Group</li> <li>Contact: prasad.bandaru@usda.gov</li> </ul>"},{"location":"docs/","title":"geo_epic_win","text":"<p>Windows version of geo epic</p>"},{"location":"docs/#pushing-docs","title":"pushing docs","text":"<p>This project uses MkDocs with the Material for MkDocs theme to generate a static site from Markdown files.</p>"},{"location":"docs/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following installed on your machine:</p> <ul> <li>Python 3.x</li> <li><code>pip</code> (Python package manager)</li> <li><code>sudo</code> privileges (if you're using Linux or macOS)</li> </ul>"},{"location":"docs/#installation","title":"Installation","text":""},{"location":"docs/#step-1-install-mkdocs","title":"Step 1: Install MkDocs","text":"<p>On Linux or macOS, you can use the following command to install MkDocs:</p> <pre><code>sudo apt install mkdocs\n</code></pre> <p>Alternatively, you can use pip to install MkDocs on any platform:</p> <pre><code>pip install mkdocs\n</code></pre>"},{"location":"docs/#step-2-install-the-material-for-mkdocs-theme","title":"Step 2: Install the Material for MkDocs theme","text":"<p>Install the Material theme for MkDocs using pip:</p> <pre><code>pip install mkdocs-material\n</code></pre>"},{"location":"docs/#step-3-install-the-pymdown-extensions-optional","title":"Step 3: Install the pymdown-extensions (Optional)","text":"<p>For additional Markdown extensions supported by the Material theme, install the pymdown-extensions:</p> <pre><code>pip install pymdown-extensions\n</code></pre> <p>Running Locally To preview your documentation site locally, use the following command:</p> <p><pre><code>mkdocs serve\n</code></pre> This will start a local development server. You can access the site at http://127.0.0.1:8000 in your browser to view changes in real-time.</p>"},{"location":"docs/#step-4-deploying-to-github-pages","title":"Step 4: Deploying to GitHub Pages","text":"<p>Once you're ready to deploy your MkDocs site to GitHub Pages, use the following command:</p> <p><pre><code>mkdocs gh-deploy --force\n</code></pre> This will build your site and push it to the gh-pages branch of your GitHub repository, which will automatically serve the site using GitHub Pages.</p>"},{"location":"docs/#editing-content","title":"Editing Content","text":"<p>To edit the content of your MkDocs site, modify the Markdown files in the docs/ folder. For detailed instructions on editing and configuring your MkDocs project, refer to the official documentation:</p> <p>Material for MkDocs - Getting Started </p>"},{"location":"api/api/","title":"Api","text":"<p>GeoEPIC provides a command-line interface to execute various tasks.   The general command structure is as follows:</p> <pre><code>geo_epic {module} {func} [options]\n</code></pre> <p>Example usage: <pre><code>geo_epic workspace new -n Test\n</code></pre></p>"},{"location":"api/api/#modules-and-functions","title":"Modules and Functions","text":""},{"location":"api/api/#workspace","title":"workspace","text":"<ul> <li>new: Create a new workspace with a predefined template structure.</li> <li>copy: Copy files between different folders.</li> <li>run: Execute simulations within the workspace.</li> </ul>"},{"location":"api/api/#utility","title":"utility","text":"<ul> <li>gee: Download required time-series data from Google Earth Engine.</li> </ul>"},{"location":"api/api/#weather","title":"weather","text":"<ul> <li>ee: Retrieve weather data from Earth Engine.</li> <li>download_daily: Download daily weather data from Daymet.</li> </ul>"},{"location":"api/api/#soil","title":"soil","text":"<ul> <li>usda: Fetch soil data from USDA SSURGO.</li> <li>process_gdb: Process SSURGO geodatabase (GDB) files.</li> </ul>"},{"location":"api/api/#sites","title":"sites","text":"<ul> <li>generate: Generate site files from processed data.</li> </ul> <p>For more details on each command and its available options, use the following command:</p> <pre><code>geo_epic {module} {func} --help\n</code></pre>"},{"location":"api/core/","title":"Core Module","text":""},{"location":"api/core/#core","title":"<code>core</code>","text":"<p>Classes:</p> Name Description <code>EPICModel</code> <p>This class handles the setup and execution of the EPIC model executable.</p> <code>Problem_Wrapper</code> <p>A wrapper class that provides a simplified interface for optimization and sensitivity analysis.</p> <code>PygmoProblem</code> <p>A class designed to define an optimization problem for use with the PyGMO library, </p> <code>Site</code> <p>Represents a site (ex: agricultural field) with paths to it's corresponding EPIC input files.</p> <code>Workspace</code> <p>This class organises the workspace for executing simulations, saving required results.</p>"},{"location":"api/core/#core.EPICModel","title":"<code>EPICModel</code>","text":"<p>This class handles the setup and execution of the EPIC model executable.</p> <p>Attributes:</p> Name Type Description <code>base_dir</code> <code>str</code> <p>The base directory for model runs.</p> <code>executable</code> <code>str</code> <p>Path to the executable model file.</p> <code>output_dir</code> <code>str</code> <p>Directory to store model outputs.</p> <code>log_dir</code> <code>str</code> <p>Directory to store logs.</p> <code>start_date</code> <code>date</code> <p>The start date of the EPIC model simulation.</p> <code>duration</code> <code>int</code> <p>The duration of the EPIC model simulation in years.</p> <code>output_types</code> <code>list</code> <p>A list of enabled output types for the EPIC model.</p> <code>model_dir</code> <code>str</code> <p>Directory path where the executable is located.</p> <code>executable_name</code> <code>str</code> <p>Name of the executable file.</p> <p>Methods:</p> Name Description <code>auto_Nfertilization</code> <p>Update the nitrogen settings in the EPICCONT.DAT file.</p> <code>auto_irrigation</code> <p>Update the irrigation settings in the EPICCONT.DAT file.</p> <code>close</code> <p>Release the lock on the model's directory by deleting the lock file.</p> <code>from_config</code> <p>Create an EPICModel instance from a configuration path.</p> <code>run</code> <p>Execute the model for the given site and handle output files.</p> <code>set_output_types</code> <p>Set the model output types and update the model's print file to enable specified outputs.</p> <code>setup</code> <p>Set up the model run configurations based on provided settings.</p>"},{"location":"api/core/#core.EPICModel.duration","title":"<code>duration</code>  <code>property</code> <code>writable</code>","text":"<p>Get the duration of the EPIC model simulation.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The duration of the simulation in years.</p>"},{"location":"api/core/#core.EPICModel.output_types","title":"<code>output_types</code>  <code>property</code> <code>writable</code>","text":"<p>Get the current output types of the EPIC model.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of enabled output types.</p>"},{"location":"api/core/#core.EPICModel.start_date","title":"<code>start_date</code>  <code>property</code> <code>writable</code>","text":"<p>Get the start date of the EPIC model simulation.</p> <p>Returns:</p> Type Description <p>datetime.date: The start date of the simulation.</p>"},{"location":"api/core/#core.EPICModel.auto_Nfertilization","title":"<code>auto_Nfertilization(bft0, fnp=None, fmx=None)</code>","text":"<p>Update the nitrogen settings in the EPICCONT.DAT file. Only BFT0 is required. Other parameters will be updated only if they are not None.</p> <p>:param file_path: Path to the EPICCONT.DAT file :param bft0: Nitrogen stress factor to trigger auto fertilization (BFT0) - required :param fnp: Fertilizer application variable (FNP) - optional :param fmx: Maximum annual N fertilizer applied for a crop (FMX) - optional</p>"},{"location":"api/core/#core.EPICModel.auto_irrigation","title":"<code>auto_irrigation(bir, efi=None, vimx=None, armn=None, armx=None)</code>","text":"<p>Update the irrigation settings in the EPICCONT.DAT file. Only BIR is required. Other parameters will be updated only if they are not None.</p> <p>:param file_path: Path to the EPICCONT.DAT file :param bir: Water stress factor to trigger automatic irrigation (BIR) - required :param efi: Runoff volume/Volume irrigation water applied (EFI) - optional :param vimx: Maximum annual irrigation volume (VIMX) in mm - optional :param armn: Minimum single application volume (ARMN) in mm - optional :param armx: Maximum single application volume (ARMX) in mm - optional</p>"},{"location":"api/core/#core.EPICModel.close","title":"<code>close()</code>","text":"<p>Release the lock on the model's directory by deleting the lock file.</p>"},{"location":"api/core/#core.EPICModel.from_config","title":"<code>from_config(config_path)</code>  <code>classmethod</code>","text":"<p>Create an EPICModel instance from a configuration path.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <p>Returns:</p> Name Type Description <code>EPICModel</code> <p>A configured instance of the EPICModel.</p>"},{"location":"api/core/#core.EPICModel.run","title":"<code>run(site, verbose=False, dest=None)</code>","text":"<p>Execute the model for the given site and handle output files.</p> <p>Parameters:</p> Name Type Description Default <code>site</code> <code>Site</code> <p>A site instance containing site-specific configuration.</p> required <code>dest</code> <code>str</code> <p>Destination directory for the run. If None, a temporary directory is used.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If any output file is not generated or is empty.</p>"},{"location":"api/core/#core.EPICModel.set_output_types","title":"<code>set_output_types(output_types)</code>","text":"<p>Set the model output types and update the model's print file to enable specified outputs.</p> <p>Parameters:</p> Name Type Description Default <code>output_types</code> <code>list of str</code> <p>List of output types to be enabled.</p> required"},{"location":"api/core/#core.EPICModel.setup","title":"<code>setup(config)</code>","text":"<p>Set up the model run configurations based on provided settings.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing model settings.</p> required"},{"location":"api/core/#core.Problem_Wrapper","title":"<code>Problem_Wrapper</code>","text":"<p>A wrapper class that provides a simplified interface for optimization and sensitivity analysis.</p> <p>Attributes:</p> Name Type Description <code>problem</code> <code>PygmoProblem</code> <p>The PygmoProblem instance.</p> <code>pg_problem</code> <code>problem</code> <p>The wrapped PyGMO problem instance.</p> <code>algorithm</code> <p>The PyGMO algorithm instance for optimization.</p> <code>population</code> <p>The PyGMO population instance.</p> <code>population_size</code> <code>int</code> <p>Size of the population for optimization.</p> <p>Methods:</p> Name Description <code>init</code> <p>Initialize the optimization algorithm and population.</p> <code>optimize</code> <p>Run the optimization process.</p> <code>sensitivity_analysis</code> <p>Perform sensitivity analysis using SALib with status updates.</p>"},{"location":"api/core/#core.Problem_Wrapper.init","title":"<code>init(algorithm, **kwargs)</code>","text":"<p>Initialize the optimization algorithm and population.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <p>PyGMO algorithm class (e.g., pg.pso_gen)</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the algorithm</p> <code>{}</code>"},{"location":"api/core/#core.Problem_Wrapper.optimize","title":"<code>optimize(population_size, generations)</code>","text":"<p>Run the optimization process.</p> <p>Parameters:</p> Name Type Description Default <code>population_size</code> <code>int</code> <p>Size of the population for optimization</p> required <code>generations</code> <code>int</code> <p>Number of generations to run</p> required <p>Returns:</p> Type Description <p>The evolved population after optimization</p>"},{"location":"api/core/#core.Problem_Wrapper.sensitivity_analysis","title":"<code>sensitivity_analysis(base_no_of_samples, method)</code>","text":"<p>Perform sensitivity analysis using SALib with status updates.</p> <p>Parameters: - base_no_of_samples (int): Base number of samples to generate. - method (str): Sensitivity analysis method ('sobol', 'efast', 'morris').</p> <p>Returns: - dict: Results of the sensitivity analysis.</p>"},{"location":"api/core/#core.PygmoProblem","title":"<code>PygmoProblem</code>","text":"<p>A class designed to define an optimization problem for use with the PyGMO library, </p> <p>Attributes:</p> Name Type Description <code>workspace</code> <code>Workspace</code> <p>The workspace object managing the environment in which the model runs.</p> <code>dfs</code> <code>tuple</code> <p>A tuple of DataFrame-like objects that hold constraints and parameters for the problem.</p> <code>bounds</code> <code>ndarray</code> <p>An array of parameter bounds, each specified as (min, max).</p> <code>lens</code> <code>ndarray</code> <p>An array of cumulative lengths that help in splitting parameters for each DataFrame.</p> <p>Methods:</p> Name Description <code>apply_solution</code> <p>Apply a solution vector to update parameters in all dataframes.</p> <code>fitness</code> <p>Evaluate the fitness of a solution vector 'x'.</p> <code>get_bounds</code> <p>Get the bounds for parameters as tuples of (min, max) values for each parameter across all data frames.</p>"},{"location":"api/core/#core.PygmoProblem.current","title":"<code>current</code>  <code>property</code>","text":"<p>Retrieve the current parameter values from all data frames.</p> <p>Returns:</p> Type Description <p>np.array: A concatenated array of current parameter values from all data frames.</p>"},{"location":"api/core/#core.PygmoProblem.var_names","title":"<code>var_names</code>  <code>property</code>","text":"<p>Get the variable names from all data frames.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of variable names concatenated from all data frames. Each data frame's   var_names() method is called and the results are combined into a single list.</p>"},{"location":"api/core/#core.PygmoProblem.apply_solution","title":"<code>apply_solution(x)</code>","text":"<p>Apply a solution vector to update parameters in all dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>A solution vector containing parameter values for all data frames.</p> required"},{"location":"api/core/#core.PygmoProblem.fitness","title":"<code>fitness(x)</code>","text":"<p>Evaluate the fitness of a solution vector 'x'.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>A solution vector containing parameter values for all data frames.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The fitness value as determined by the workspace's fitness function.</p>"},{"location":"api/core/#core.PygmoProblem.get_bounds","title":"<code>get_bounds()</code>","text":"<p>Get the bounds for parameters as tuples of (min, max) values for each parameter across all data frames.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Two numpy arrays representing the lower and upper bounds of the parameters.</p>"},{"location":"api/core/#core.Site","title":"<code>Site</code>","text":"<p>Represents a site (ex: agricultural field) with paths to it's corresponding EPIC input files.</p> <p>Attributes:</p> Name Type Description <code>opc_path</code> <code>str</code> <p>Path to the operational practice code file.</p> <code>dly_path</code> <code>str</code> <p>Path to the daily weather data file.</p> <code>sol_path</code> <code>str</code> <p>Path to the soil data file.</p> <code>sit_path</code> <code>str</code> <p>Path to the site information file.</p> <code>site_id</code> <code>str</code> <p>Identifier for the site, derived from the sit file name if not provided.</p> <code>outputs</code> <code>dict</code> <p>Dictionary to store output file paths.</p> <p>Methods:</p> Name Description <code>copy</code> <p>Copy or symlink site files to a destination folder.</p> <code>fetch_usa</code> <p>Fetch all required EPIC input files for a location in the conterminous USA.</p> <code>from_config</code> <p>Factory method to create a Site instance from a configuration dictionary and additional site information.</p> <code>get_dly</code> <p>Retrieve daily weather data from a DLY file.</p> <code>get_opc</code> <p>Retrieve operation schedule data from an OPC file.</p> <code>get_sit</code> <p>Retrieve site data from a SIT file.</p> <code>get_sol</code> <p>Retrieve soil data from a SOL file.</p>"},{"location":"api/core/#core.Site.elevation","title":"<code>elevation</code>  <code>property</code>","text":"<p>Elevation of the site.</p>"},{"location":"api/core/#core.Site.latitude","title":"<code>latitude</code>  <code>property</code>","text":"<p>Latitude of the site.</p>"},{"location":"api/core/#core.Site.longitude","title":"<code>longitude</code>  <code>property</code>","text":"<p>Longitude of the site.</p>"},{"location":"api/core/#core.Site.copy","title":"<code>copy(dest_folder, use_symlink=False)</code>","text":"<p>Copy or symlink site files to a destination folder.</p> <p>Parameters:</p> Name Type Description Default <code>dest_folder</code> <code>str</code> <p>Destination folder path</p> required <code>use_symlinks</code> <code>bool</code> <p>If True, create symbolic links instead of copying files. Defaults to False.</p> required <p>Returns:</p> Name Type Description <code>Site</code> <p>A new Site instance pointing to the copied/linked files</p>"},{"location":"api/core/#core.Site.fetch_usa","title":"<code>fetch_usa(lat, lon, opc, site_id=None, start_date=None, end_date=None)</code>  <code>classmethod</code>","text":"<p>Fetch all required EPIC input files for a location in the conterminous USA.</p> <p>This method automatically retrieves: - Soil data from SSURGO via Soil Data Access - Weather data from Daymet - Elevation and slope from DEM (GLO-30)</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of the site</p> required <code>lon</code> <code>float</code> <p>Longitude of the site</p> required <code>opc</code> <code>str</code> <p>Path to the OPC (operation schedule) file</p> required <code>site_id</code> <code>str</code> <p>Site identifier. Auto-generated if None.</p> <code>None</code> <code>start_date</code> <code>str</code> <p>Weather data start date (YYYY-MM-DD format)</p> <code>None</code> <code>end_date</code> <code>str</code> <p>Weather data end date (YYYY-MM-DD format)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Site</code> <p>A Site object with all input files fetched and ready for simulation</p> Example <p>site = Site.fetch_usa(41.1686, -96.4736, opc='./irrigated_corn.OPC') print(site)</p>"},{"location":"api/core/#core.Site.from_config","title":"<code>from_config(config, **site_info)</code>  <code>classmethod</code>","text":"<p>Factory method to create a Site instance from a configuration dictionary and additional site information.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with paths and settings.</p> required <code>**site_info</code> <code>dict</code> <p>Keyword arguments containing site-specific information such as 'opc', 'dly', 'soil', 'SiteID', 'lat', 'lon', and 'ele'.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Site</code> <p>An instance of the Site class configured according to the provided settings.</p>"},{"location":"api/core/#core.Site.get_dly","title":"<code>get_dly()</code>","text":"<p>Retrieve daily weather data from a DLY file.</p> <p>Returns:</p> Name Type Description <code>DailyWeather</code> <p>An instance of the DailyWeather class containing weather data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the DLY file does not exist at the specified path.</p>"},{"location":"api/core/#core.Site.get_opc","title":"<code>get_opc()</code>","text":"<p>Retrieve operation schedule data from an OPC file.</p> <p>Returns:</p> Name Type Description <code>Operation</code> <p>An instance of the Operation class containing operation schedule data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the OPC file does not exist at the specified path.</p>"},{"location":"api/core/#core.Site.get_sit","title":"<code>get_sit()</code>","text":"<p>Retrieve site data from a SIT file.</p> <p>Returns:</p> Name Type Description <code>Site</code> <p>An instance of the Site class containing site data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the SIT file does not exist at the specified path.</p>"},{"location":"api/core/#core.Site.get_sol","title":"<code>get_sol()</code>","text":"<p>Retrieve soil data from a SOL file.</p> <p>Returns:</p> Name Type Description <code>Soil</code> <p>An instance of the Soil class containing soil data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the SOL file does not exist at the specified path.</p>"},{"location":"api/core/#core.Workspace","title":"<code>Workspace</code>","text":"<p>This class organises the workspace for executing simulations, saving required results.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>str</code> <p>Unique ID assigned to each workspace instance</p> <code>config</code> <code>dict</code> <p>Configuration data loaded from a config file.</p> <code>base_dir</code> <code>str</code> <p>Base directory for the workspace.</p> <code>routines</code> <code>dict</code> <p>Dictionary to store functions as routines.</p> <code>objective_function</code> <code>callable</code> <p>Function to be executed as the objective.</p> <code>dataframes</code> <code>dict</code> <p>Cache for dataframes.</p> <code>delete_after_use</code> <code>bool</code> <p>Whether to delete temporary files after use.</p> <code>model</code> <code>EPICModel</code> <p>Instance of the EPIC model.</p> <code>data_logger</code> <code>DataLogger</code> <p>Instance of the DataLogger for logging data.</p> <p>Methods:</p> Name Description <code>clear_logs</code> <p>Clear all log files and temporary run directories.</p> <code>clear_outputs</code> <p>Clear all output files.</p> <code>close</code> <p>Explicit cleanup (use this in notebooks).</p> <code>fetch_log</code> <p>Retrieve the logs for a specific function.</p> <code>logger</code> <p>Decorator to log the results of a function.</p> <code>make_problem</code> <p>Create a PygmoProblem instance after validating inputs.</p> <code>objective</code> <p>Set the objective function to be executed after simulations.</p> <code>routine</code> <p>Decorator to add a function as a routine without logging or returning values.</p> <code>run</code> <p>Run simulations for all sites or filtered by a selection string.</p> <code>run_simulation</code> <p>Run simulation for a given site or site information.</p>"},{"location":"api/core/#core.Workspace.clear_logs","title":"<code>clear_logs()</code>","text":"<p>Clear all log files and temporary run directories.</p>"},{"location":"api/core/#core.Workspace.clear_outputs","title":"<code>clear_outputs()</code>","text":"<p>Clear all output files.</p>"},{"location":"api/core/#core.Workspace.close","title":"<code>close()</code>","text":"<p>Explicit cleanup (use this in notebooks).</p>"},{"location":"api/core/#core.Workspace.fetch_log","title":"<code>fetch_log(func, keep=False)</code>","text":"<p>Retrieve the logs for a specific function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>str</code> <p>The name of the function whose logs are to be retrieved.</p> required <code>keep</code> <code>bool</code> <p>If True, preserve the logs after retrieval. If False, logs are deleted after reading. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: DataFrame containing the logs for the specified function.</p>"},{"location":"api/core/#core.Workspace.logger","title":"<code>logger(func)</code>","text":"<p>Decorator to log the results of a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function that logs its output.</p>"},{"location":"api/core/#core.Workspace.make_problem","title":"<code>make_problem(*dfs)</code>","text":"<p>Create a PygmoProblem instance after validating inputs.</p> <p>Parameters:</p> Name Type Description Default <code>*dfs</code> <p>Variable number of dataframes to pass to PygmoProblem</p> <code>()</code> <p>Returns:</p> Name Type Description <code>PygmoProblem</code> <p>A configured optimization problem instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no dataframes provided or if any dataframe lacks constraints</p>"},{"location":"api/core/#core.Workspace.objective","title":"<code>objective(func)</code>","text":"<p>Set the objective function to be executed after simulations.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The objective function to be set.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorator function that sets the objective function.</p>"},{"location":"api/core/#core.Workspace.routine","title":"<code>routine(func)</code>","text":"<p>Decorator to add a function as a routine without logging or returning values.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function that executes without logging.</p>"},{"location":"api/core/#core.Workspace.run","title":"<code>run(select_str=None, progress_bar=True)</code>","text":"<p>Run simulations for all sites or filtered by a selection string.</p> <p>Parameters:</p> Name Type Description Default <code>select_str</code> <code>str</code> <p>String to filter sites. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the objective function if set, otherwise None.</p>"},{"location":"api/core/#core.Workspace.run_simulation","title":"<code>run_simulation(site_or_info)</code>","text":"<p>Run simulation for a given site or site information.</p> <p>Parameters:</p> Name Type Description Default <code>site_or_info</code> <code>Site or dict</code> <p>A Site object or a dictionary containing site information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The results from the post-processing routines. Output files are saved based on the options selected</p>"},{"location":"api/gee/","title":"GEE Module","text":""},{"location":"api/gee/#gee","title":"<code>gee</code>","text":"<p>Classes:</p> Name Description <code>CompositeCollection</code> <p>A class to handle composite collection of Earth Engine data.</p>"},{"location":"api/gee/#gee.CompositeCollection","title":"<code>CompositeCollection</code>","text":"<p>A class to handle composite collection of Earth Engine data.</p> <p>This class initializes collections of Earth Engine based on a provided YAML configuration file, applies specified formulas and selections, and allows for the extraction of temporal data for a given Area of Interest (AOI).</p> <p>Methods:</p> Name Description <code>extract</code> <p>Extracts temporal data for a given AOI and returns it as a pandas DataFrame.</p>"},{"location":"api/gee/#gee.CompositeCollection.extract","title":"<code>extract(aoi_coords)</code>","text":"<p>Extracts temporal data for a given Area of Interest (AOI).</p> <p>Parameters:</p> Name Type Description Default <code>aoi_coords</code> <code>tuple / list</code> <p>Coordinates representing the AOI, either as a Point or as vertices of a Polygon.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A pandas DataFrame containing the extracted data.</p>"},{"location":"api/gee/#gee.CompositeCollection.merged","title":"<code>merged()</code>","text":"<p>Merges all collections in self.collections into a single ImageCollection.</p> <p>Returns:</p> Type Description <p>ee.ImageCollection: A merged collection containing all images from all collections.</p>"},{"location":"api/io/","title":"IO Module","text":""},{"location":"api/io/#io","title":"<code>io</code>","text":"<p>Classes:</p> Name Description <code>ACY</code> <code>DGN</code> <code>DLY</code> <code>DSL</code> <code>DWC</code> <code>OPC</code> <code>SIT</code> <code>SOL</code>"},{"location":"api/io/#io.ACY","title":"<code>ACY</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the ACY data.</p>"},{"location":"api/io/#io.ACY.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the ACY data.</p>"},{"location":"api/io/#io.DGN","title":"<code>DGN</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DGN data.</p>"},{"location":"api/io/#io.DGN.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DGN data.</p>"},{"location":"api/io/#io.DLY","title":"<code>DLY</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>load</code> <p>Load data from a DLY file into DataFrame.</p> <code>save</code> <p>Save DataFrame into a DLY file.</p> <code>to_monthly</code> <p>Save as monthly file</p> <code>validate</code> <p>Validate the DataFrame to ensure it contains a continuous range of dates </p>"},{"location":"api/io/#io.DLY.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load data from a DLY file into DataFrame.</p>"},{"location":"api/io/#io.DLY.save","title":"<code>save(path=None)</code>","text":"<p>Save DataFrame into a DLY file.</p>"},{"location":"api/io/#io.DLY.to_monthly","title":"<code>to_monthly(path=None)</code>","text":"<p>Save as monthly file</p>"},{"location":"api/io/#io.DLY.validate","title":"<code>validate(start_date, end_date)</code>","text":"<p>Validate the DataFrame to ensure it contains a continuous range of dates  between start_date and end_date, without duplicates.</p>"},{"location":"api/io/#io.DSL","title":"<code>DSL</code>","text":"<p>Methods:</p> Name Description <code>get_data</code> <p>Return stored water data.</p>"},{"location":"api/io/#io.DSL.get_data","title":"<code>get_data()</code>","text":"<p>Return stored water data.</p>"},{"location":"api/io/#io.DWC","title":"<code>DWC</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DWC data.</p>"},{"location":"api/io/#io.DWC.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DWC data.</p>"},{"location":"api/io/#io.OPC","title":"<code>OPC</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>append</code> <p>Append another OPC or DataFrame to the current OPC instance.</p> <code>edit_crop_season</code> <p>Edit the planting and/or harvest dates for a given year and crop.</p> <code>edit_fertilizer_rate</code> <p>Edit the fertilizer rate for a given year.</p> <code>edit_harvest_date</code> <p>Edit the harvest date for a given year and crop.</p> <code>edit_operation_date</code> <p>Edit the operation date for a given year.</p> <code>edit_operation_value</code> <p>Edit the operation value for a given year.</p> <code>edit_plantation_date</code> <p>Edit the plantation date for a given year and crop.</p> <code>get_harvest_date</code> <p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <code>get_plantation_date</code> <p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <code>iter_seasons</code> <p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <code>load</code> <p>Load data from an OPC file into DataFrame.</p> <code>new</code> <p>Create a new OPC instance with an empty DataFrame with preset columns, a default header,</p> <code>remove</code> <p>Remove operation(s) from the OPC file that match all provided criteria.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>update</code> <p>Add or update an operation in the OPC file.</p> <code>update_phu</code> <p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <code>validate</code> <p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p>"},{"location":"api/io/#io.OPC.IAUI","title":"<code>IAUI</code>  <code>property</code> <code>writable</code>","text":"<p>Get the auto-irrigation implement ID from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if auto-irrigation is enabled (72), False if disabled (0)</p>"},{"location":"api/io/#io.OPC.LUN","title":"<code>LUN</code>  <code>property</code> <code>writable</code>","text":"<p>Get the land use number from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The land use number from the first 4 characters of header line 2</p>"},{"location":"api/io/#io.OPC.append","title":"<code>append(second_opc)</code>","text":"<p>Append another OPC or DataFrame to the current OPC instance. Args:     second_opc (pd.DataFrame or OPC): The data to append. Returns:     OPC: A new OPC instance with combined data. Raises:     ValueError: If second_opc is not a pandas DataFrame or OPC instance.</p>"},{"location":"api/io/#io.OPC.edit_crop_season","title":"<code>edit_crop_season(new_planting_date=None, new_harvest_date=None, crop_code=None)</code>","text":"<p>Edit the planting and/or harvest dates for a given year and crop.</p> <p>Parameters: year (int): Year. new_planting_date (datetime, optional): New planting date. If not provided, only harvest date will be updated. new_harvest_date (datetime, optional): New harvest date. If not provided, only planting date will be updated. crop_code (int, optional): Crop code. If not provided, changes the first crop found.</p>"},{"location":"api/io/#io.OPC.edit_fertilizer_rate","title":"<code>edit_fertilizer_rate(rate, year=2020, month=None, day=None)</code>","text":"<p>Edit the fertilizer rate for a given year.</p> <p>Parameters: rate (float): Fertilizer rate to be set. year (int, optional): Year for the fertilizer rate application. Defaults to 2020. month (int, optional): Month for the fertilizer rate application. If not provided, the first instance is changed. day (int, optional): Day for the fertilizer rate application. Defaults to None.</p>"},{"location":"api/io/#io.OPC.edit_harvest_date","title":"<code>edit_harvest_date(date, crop_code)</code>","text":"<p>Edit the harvest date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of harvest in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"api/io/#io.OPC.edit_operation_date","title":"<code>edit_operation_date(code, year, month, day, crop_code=None)</code>","text":"<p>Edit the operation date for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. month (int): Month of operation. day (int): Day of operation. crop_code (int, optional): Crop code.</p>"},{"location":"api/io/#io.OPC.edit_operation_value","title":"<code>edit_operation_value(code, year, value, crop_code=None)</code>","text":"<p>Edit the operation value for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. value (float): New operation value. crop_code (int, optional): Crop code.</p>"},{"location":"api/io/#io.OPC.edit_plantation_date","title":"<code>edit_plantation_date(date, crop_code)</code>","text":"<p>Edit the plantation date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of plantation in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"api/io/#io.OPC.get_harvest_date","title":"<code>get_harvest_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their harvest dates with row indices.</p>"},{"location":"api/io/#io.OPC.get_plantation_date","title":"<code>get_plantation_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their plantation dates with row indices.</p>"},{"location":"api/io/#io.OPC.iter_seasons","title":"<code>iter_seasons(start_year=None, end_year=None)</code>","text":"<p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <p>Parameters: start_year (int, optional): The starting year to consider. Defaults to None. end_year (int, optional): The ending year to consider. Defaults to None.</p> <p>dict: A dictionary containing:     - plantation_date: The date of plantation     - harvest_date: The date of harvest     - crop_code: The crop code     - operations: A subset of OPC rows for this season     - plantation_index: The index of the plantation row</p>"},{"location":"api/io/#io.OPC.load","title":"<code>load(path, start_year=None)</code>  <code>classmethod</code>","text":"<p>Load data from an OPC file into DataFrame.</p> <p>Parameters: path (str): Path to the OPC file. start_year (int, optional): Start year for the OPC file. If not provided, it will be read from the file header.</p> <p>Returns: OPC: An instance of the OPC class containing the loaded data.</p>"},{"location":"api/io/#io.OPC.new","title":"<code>new(name, start_year)</code>  <code>classmethod</code>","text":"<p>Create a new OPC instance with an empty DataFrame with preset columns, a default header, and provided name and start year.</p> <p>Parameters: name (str): The name for this OPC file/instance. start_year (int): The start year to assign in the OPC instance.</p> <p>Returns: OPC: An OPC instance with no data but with the required metadata set.</p>"},{"location":"api/io/#io.OPC.remove","title":"<code>remove(opID=None, date=None, cropID=None, XMTU=None, fertID=None, year=None)</code>","text":"<p>Remove operation(s) from the OPC file that match all provided criteria.</p> <p>Parameters:</p> Name Type Description Default <code>opID</code> <code>int</code> <p>Operation ID to match</p> <code>None</code> <code>date</code> <code>str</code> <p>Date to match in format 'YYYY-MM-DD'</p> <code>None</code> <code>cropID</code> <code>int</code> <p>Crop ID to match</p> <code>None</code> <code>XMTU/LYR/pestID/fertID</code> <code>int</code> <p>Machine type/layer/pesticide ID/fertilizer ID to match</p> required <code>year</code> <code>int</code> <p>Year to match</p> <code>None</code>"},{"location":"api/io/#io.OPC.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p> <p>Parameters: path (str): Path to save the OPC file.</p>"},{"location":"api/io/#io.OPC.update","title":"<code>update(operation)</code>","text":"<p>Add or update an operation in the OPC file.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Dictionary containing operation details with keys: - opID: Operation ID (required) - cropID: Crop ID (required) - date: Operation date as string 'YYYY-MM-DD' (required) - XMTU/LYR/pestID/fertID: Machine type/years/pesticide ID/fertilizer ID (optional, default 0) - OPV1-OPV8: Additional operation values (optional, default 0)</p> required"},{"location":"api/io/#io.OPC.update_phu","title":"<code>update_phu(dly, cropcom)</code>","text":"<p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <p>Parameters: dly (DLY): DLY object containing weather data. cropcom (DataFrame): DataFrame containing crop code and TBS values.</p>"},{"location":"api/io/#io.OPC.validate","title":"<code>validate(duration=None)</code>","text":"<p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p> <p>Parameters: duration (int, optional): Duration of the simulation in years. If None, uses the maximum Yid.</p> <p>Returns: bool: True if the data is valid, False otherwise.</p>"},{"location":"api/io/#io.SIT","title":"<code>SIT</code>","text":"<p>Methods:</p> Name Description <code>load</code> <p>Class method to load the .sit file and return a SiteFile instance.</p> <code>save</code> <p>Save the current site information to a .SIT file.</p>"},{"location":"api/io/#io.SIT.elevation","title":"<code>elevation</code>  <code>property</code> <code>writable</code>","text":"<p>Get elevation value.</p>"},{"location":"api/io/#io.SIT.lat","title":"<code>lat</code>  <code>property</code> <code>writable</code>","text":"<p>Get latitude value.</p>"},{"location":"api/io/#io.SIT.lon","title":"<code>lon</code>  <code>property</code> <code>writable</code>","text":"<p>Get longitude value.</p>"},{"location":"api/io/#io.SIT.slope","title":"<code>slope</code>  <code>property</code> <code>writable</code>","text":"<p>Get slope steep value.</p>"},{"location":"api/io/#io.SIT.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Class method to load the .sit file and return a SiteFile instance.</p> <p>Parameters: file_path (str): Path to the .sit file.</p> <p>Returns: SiteFile: An instance of the SiteFile class with loaded data.</p>"},{"location":"api/io/#io.SIT.save","title":"<code>save(output_dir)</code>","text":"<p>Save the current site information to a .SIT file.</p>"},{"location":"api/io/#io.SOL","title":"<code>SOL</code>","text":"<p>Methods:</p> Name Description <code>from_sda</code> <p>Create a Soil object from Soil Data Access using a query.</p> <code>load</code> <p>Load soil data from a file and return a Soil object.</p> <code>save</code> <p>Save the soil data to a file using a template.</p>"},{"location":"api/io/#io.SOL.from_sda","title":"<code>from_sda(query)</code>  <code>classmethod</code>","text":"<p>Create a Soil object from Soil Data Access using a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>int or str</code> <p>Query string for SoilDataAccess. (mukey or WKT str) ( \"POINT(-123.4567 45.6789)\" )</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from SDA.</p>"},{"location":"api/io/#io.SOL.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Load soil data from a file and return a Soil object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the soil file.</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from the file.</p>"},{"location":"api/io/#io.SOL.save","title":"<code>save(filepath, template=None)</code>","text":"<p>Save the soil data to a file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the soil file.</p> required <code>template</code> <code>list</code> <p>Optional list of template lines.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If soil properties DataFrame is empty.</p>"},{"location":"api/io/#io.config_parser","title":"<code>config_parser</code>","text":"<p>Classes:</p> Name Description <code>ConfigParser</code>"},{"location":"api/io/#io.config_parser.ConfigParser","title":"<code>ConfigParser</code>","text":"<p>Methods:</p> Name Description <code>get</code> <p>Retrieve a value from the configuration.</p> <code>load</code> <p>Load data from the YAML file.</p> <code>save</code> <p>Save data to the YAML file.</p> <code>update</code> <p>Update the current config with new values.</p>"},{"location":"api/io/#io.config_parser.ConfigParser.get","title":"<code>get(key, default=None)</code>","text":"<p>Retrieve a value from the configuration.</p>"},{"location":"api/io/#io.config_parser.ConfigParser.load","title":"<code>load()</code>","text":"<p>Load data from the YAML file.</p>"},{"location":"api/io/#io.config_parser.ConfigParser.save","title":"<code>save()</code>","text":"<p>Save data to the YAML file.</p>"},{"location":"api/io/#io.config_parser.ConfigParser.update","title":"<code>update(updates)</code>","text":"<p>Update the current config with new values.</p>"},{"location":"api/io/#io.cropcom","title":"<code>cropcom</code>","text":"<p>Classes:</p> Name Description <code>CropCom</code> <p>Class for handling CROPCOM.DAT file.</p>"},{"location":"api/io/#io.cropcom.CropCom","title":"<code>CropCom</code>","text":"<p>Class for handling CROPCOM.DAT file.</p> <p>Methods:</p> Name Description <code>constraints</code> <p>Returns the constraints (min, max ranges) for the parameters.</p> <code>edit</code> <p>Updates the parameters in the DataFrame with new values.</p> <code>get_vars</code> <p>Returns the vars DataFrame with an additional column containing current values.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>set_sensitive</code> <p>Sets sensitive parameters based on a CSV path or list of parameter names.</p>"},{"location":"api/io/#io.cropcom.CropCom.current","title":"<code>current</code>  <code>property</code>","text":"<p>Returns the current values of parameters in the DataFrame.</p>"},{"location":"api/io/#io.cropcom.CropCom.constraints","title":"<code>constraints()</code>","text":"<p>Returns the constraints (min, max ranges) for the parameters.</p>"},{"location":"api/io/#io.cropcom.CropCom.edit","title":"<code>edit(values)</code>","text":"<p>Updates the parameters in the DataFrame with new values.</p>"},{"location":"api/io/#io.cropcom.CropCom.get_vars","title":"<code>get_vars()</code>","text":"<p>Returns the vars DataFrame with an additional column containing current values.</p>"},{"location":"api/io/#io.cropcom.CropCom.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p>"},{"location":"api/io/#io.cropcom.CropCom.set_sensitive","title":"<code>set_sensitive(parms_input, crop_codes, all=False)</code>","text":"<p>Sets sensitive parameters based on a CSV path or list of parameter names. If <code>all</code> is True, all parameters are considered sensitive.</p> <p>Parameters:</p> Name Type Description Default <code>parms_input</code> <code>str or list</code> <p>Either a CSV file path or list of parameter names to select</p> required <code>crop_codes</code> <code>list</code> <p>List of crop codes to apply parameters to</p> required <code>all</code> <code>bool</code> <p>If True, all parameters are considered sensitive regardless of input</p> <code>False</code>"},{"location":"api/io/#io.data_logger","title":"<code>data_logger</code>","text":"<p>Classes:</p> Name Description <code>DataLogger</code> <p>A class to handle logging of data using different backends: Redis, SQL, or LMDB.</p> <code>LMDBTableWriter</code> <p>Minimal, efficient row store with auto-increment row_id.</p> <code>RedisWriter</code>"},{"location":"api/io/#io.data_logger.DataLogger","title":"<code>DataLogger</code>","text":"<p>A class to handle logging of data using different backends: Redis, SQL, or LMDB. It supports logging dictionaries and retrieving logged data.</p> <p>Attributes:</p> Name Type Description <code>output_folder</code> <code>str</code> <p>Directory where files are stored (if applicable).</p> <code>delete_on_read</code> <code>bool</code> <p>Whether to delete the data after retrieving it.</p> <code>backend</code> <code>str</code> <p>The backend to use ('redis', 'sql', 'lmdb').</p> <p>Methods:</p> Name Description <code>get</code> <p>Retrieve logged data using the specified backend.</p> <code>get_writer</code> <p>Get the appropriate writer based on the backend.</p> <code>log_dict</code> <p>Log a dictionary of results using the specified backend.</p>"},{"location":"api/io/#io.data_logger.DataLogger.get","title":"<code>get(func_name, keep=False)</code>","text":"<p>Retrieve logged data using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function whose data needs to be retrieved.</p> required <code>keep</code> <code>bool</code> <p>If True, do not delete the table even if delete_on_read is True.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: The DataFrame containing the logged data.</p>"},{"location":"api/io/#io.data_logger.DataLogger.get_writer","title":"<code>get_writer(func_name)</code>","text":"<p>Get the appropriate writer based on the backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to create a writer for.</p> required <p>Returns:</p> Name Type Description <code>Writer</code> <p>An instance of the appropriate writer class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported backend is specified.</p>"},{"location":"api/io/#io.data_logger.DataLogger.log_dict","title":"<code>log_dict(func_name, result)</code>","text":"<p>Log a dictionary of results using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to log the data for.</p> required <code>result</code> <code>dict</code> <p>Dictionary of results to log.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result is not a dictionary.</p>"},{"location":"api/io/#io.data_logger.LMDBTableWriter","title":"<code>LMDBTableWriter</code>","text":"<p>Minimal, efficient row store with auto-increment row_id. - write_row(row_id=None, **kwargs) -&gt; returns the row_id (string) - read_row(row_id) -&gt; dict or None - query_rows() -&gt; DataFrame with row_id index - delete_table(), open(), close(), context manager</p> <p>Methods:</p> Name Description <code>query_rows</code> <p>Return all rows as a DataFrame with numeric-sorted row_id.</p> <code>write_row</code> <p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"api/io/#io.data_logger.LMDBTableWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Return all rows as a DataFrame with numeric-sorted row_id.</p>"},{"location":"api/io/#io.data_logger.LMDBTableWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"api/io/#io.data_logger.RedisWriter","title":"<code>RedisWriter</code>","text":"<p>Methods:</p> Name Description <code>close</code> <p>Close the connection to Redis (flag only; redis client is pooled).</p> <code>delete_table</code> <p>Delete all entries associated with the table name, including the counter.</p> <code>open</code> <p>Establish connection to Redis and initialize counter if needed.</p> <code>query_rows</code> <p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p> <code>read_row</code> <p>Read a row from Redis hash.</p> <code>write_row</code> <p>Write a row to Redis hash under the specified table name.</p>"},{"location":"api/io/#io.data_logger.RedisWriter.close","title":"<code>close()</code>","text":"<p>Close the connection to Redis (flag only; redis client is pooled).</p>"},{"location":"api/io/#io.data_logger.RedisWriter.delete_table","title":"<code>delete_table()</code>","text":"<p>Delete all entries associated with the table name, including the counter.</p>"},{"location":"api/io/#io.data_logger.RedisWriter.open","title":"<code>open()</code>","text":"<p>Establish connection to Redis and initialize counter if needed.</p>"},{"location":"api/io/#io.data_logger.RedisWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p>"},{"location":"api/io/#io.data_logger.RedisWriter.read_row","title":"<code>read_row(row_id)</code>","text":"<p>Read a row from Redis hash.</p>"},{"location":"api/io/#io.data_logger.RedisWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Write a row to Redis hash under the specified table name.</p>"},{"location":"api/io/#io.data_logger.lmdb_writer","title":"<code>lmdb_writer</code>","text":"<p>Classes:</p> Name Description <code>LMDBTableWriter</code> <p>Minimal, efficient row store with auto-increment row_id.</p>"},{"location":"api/io/#io.data_logger.lmdb_writer.LMDBTableWriter","title":"<code>LMDBTableWriter</code>","text":"<p>Minimal, efficient row store with auto-increment row_id. - write_row(row_id=None, **kwargs) -&gt; returns the row_id (string) - read_row(row_id) -&gt; dict or None - query_rows() -&gt; DataFrame with row_id index - delete_table(), open(), close(), context manager</p> <p>Methods:</p> Name Description <code>query_rows</code> <p>Return all rows as a DataFrame with numeric-sorted row_id.</p> <code>write_row</code> <p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"api/io/#io.data_logger.lmdb_writer.LMDBTableWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Return all rows as a DataFrame with numeric-sorted row_id.</p>"},{"location":"api/io/#io.data_logger.lmdb_writer.LMDBTableWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"api/io/#io.data_logger.main","title":"<code>main</code>","text":"<p>Classes:</p> Name Description <code>DataLogger</code> <p>A class to handle logging of data using different backends: Redis, SQL, or LMDB.</p>"},{"location":"api/io/#io.data_logger.main.DataLogger","title":"<code>DataLogger</code>","text":"<p>A class to handle logging of data using different backends: Redis, SQL, or LMDB. It supports logging dictionaries and retrieving logged data.</p> <p>Attributes:</p> Name Type Description <code>output_folder</code> <code>str</code> <p>Directory where files are stored (if applicable).</p> <code>delete_on_read</code> <code>bool</code> <p>Whether to delete the data after retrieving it.</p> <code>backend</code> <code>str</code> <p>The backend to use ('redis', 'sql', 'lmdb').</p> <p>Methods:</p> Name Description <code>get</code> <p>Retrieve logged data using the specified backend.</p> <code>get_writer</code> <p>Get the appropriate writer based on the backend.</p> <code>log_dict</code> <p>Log a dictionary of results using the specified backend.</p>"},{"location":"api/io/#io.data_logger.main.DataLogger.get","title":"<code>get(func_name, keep=False)</code>","text":"<p>Retrieve logged data using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function whose data needs to be retrieved.</p> required <code>keep</code> <code>bool</code> <p>If True, do not delete the table even if delete_on_read is True.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: The DataFrame containing the logged data.</p>"},{"location":"api/io/#io.data_logger.main.DataLogger.get_writer","title":"<code>get_writer(func_name)</code>","text":"<p>Get the appropriate writer based on the backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to create a writer for.</p> required <p>Returns:</p> Name Type Description <code>Writer</code> <p>An instance of the appropriate writer class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported backend is specified.</p>"},{"location":"api/io/#io.data_logger.main.DataLogger.log_dict","title":"<code>log_dict(func_name, result)</code>","text":"<p>Log a dictionary of results using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to log the data for.</p> required <code>result</code> <code>dict</code> <p>Dictionary of results to log.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result is not a dictionary.</p>"},{"location":"api/io/#io.data_logger.redis_writer","title":"<code>redis_writer</code>","text":"<p>Classes:</p> Name Description <code>RedisWriter</code>"},{"location":"api/io/#io.data_logger.redis_writer.RedisWriter","title":"<code>RedisWriter</code>","text":"<p>Methods:</p> Name Description <code>close</code> <p>Close the connection to Redis (flag only; redis client is pooled).</p> <code>delete_table</code> <p>Delete all entries associated with the table name, including the counter.</p> <code>open</code> <p>Establish connection to Redis and initialize counter if needed.</p> <code>query_rows</code> <p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p> <code>read_row</code> <p>Read a row from Redis hash.</p> <code>write_row</code> <p>Write a row to Redis hash under the specified table name.</p>"},{"location":"api/io/#io.data_logger.redis_writer.RedisWriter.close","title":"<code>close()</code>","text":"<p>Close the connection to Redis (flag only; redis client is pooled).</p>"},{"location":"api/io/#io.data_logger.redis_writer.RedisWriter.delete_table","title":"<code>delete_table()</code>","text":"<p>Delete all entries associated with the table name, including the counter.</p>"},{"location":"api/io/#io.data_logger.redis_writer.RedisWriter.open","title":"<code>open()</code>","text":"<p>Establish connection to Redis and initialize counter if needed.</p>"},{"location":"api/io/#io.data_logger.redis_writer.RedisWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p>"},{"location":"api/io/#io.data_logger.redis_writer.RedisWriter.read_row","title":"<code>read_row(row_id)</code>","text":"<p>Read a row from Redis hash.</p>"},{"location":"api/io/#io.data_logger.redis_writer.RedisWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Write a row to Redis hash under the specified table name.</p>"},{"location":"api/io/#io.inputs","title":"<code>inputs</code>","text":"<p>Classes:</p> Name Description <code>DLY</code> <code>OPC</code> <code>SIT</code> <code>SOL</code>"},{"location":"api/io/#io.inputs.DLY","title":"<code>DLY</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>load</code> <p>Load data from a DLY file into DataFrame.</p> <code>save</code> <p>Save DataFrame into a DLY file.</p> <code>to_monthly</code> <p>Save as monthly file</p> <code>validate</code> <p>Validate the DataFrame to ensure it contains a continuous range of dates </p>"},{"location":"api/io/#io.inputs.DLY.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load data from a DLY file into DataFrame.</p>"},{"location":"api/io/#io.inputs.DLY.save","title":"<code>save(path=None)</code>","text":"<p>Save DataFrame into a DLY file.</p>"},{"location":"api/io/#io.inputs.DLY.to_monthly","title":"<code>to_monthly(path=None)</code>","text":"<p>Save as monthly file</p>"},{"location":"api/io/#io.inputs.DLY.validate","title":"<code>validate(start_date, end_date)</code>","text":"<p>Validate the DataFrame to ensure it contains a continuous range of dates  between start_date and end_date, without duplicates.</p>"},{"location":"api/io/#io.inputs.OPC","title":"<code>OPC</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>append</code> <p>Append another OPC or DataFrame to the current OPC instance.</p> <code>edit_crop_season</code> <p>Edit the planting and/or harvest dates for a given year and crop.</p> <code>edit_fertilizer_rate</code> <p>Edit the fertilizer rate for a given year.</p> <code>edit_harvest_date</code> <p>Edit the harvest date for a given year and crop.</p> <code>edit_operation_date</code> <p>Edit the operation date for a given year.</p> <code>edit_operation_value</code> <p>Edit the operation value for a given year.</p> <code>edit_plantation_date</code> <p>Edit the plantation date for a given year and crop.</p> <code>get_harvest_date</code> <p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <code>get_plantation_date</code> <p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <code>iter_seasons</code> <p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <code>load</code> <p>Load data from an OPC file into DataFrame.</p> <code>new</code> <p>Create a new OPC instance with an empty DataFrame with preset columns, a default header,</p> <code>remove</code> <p>Remove operation(s) from the OPC file that match all provided criteria.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>update</code> <p>Add or update an operation in the OPC file.</p> <code>update_phu</code> <p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <code>validate</code> <p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p>"},{"location":"api/io/#io.inputs.OPC.IAUI","title":"<code>IAUI</code>  <code>property</code> <code>writable</code>","text":"<p>Get the auto-irrigation implement ID from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if auto-irrigation is enabled (72), False if disabled (0)</p>"},{"location":"api/io/#io.inputs.OPC.LUN","title":"<code>LUN</code>  <code>property</code> <code>writable</code>","text":"<p>Get the land use number from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The land use number from the first 4 characters of header line 2</p>"},{"location":"api/io/#io.inputs.OPC.append","title":"<code>append(second_opc)</code>","text":"<p>Append another OPC or DataFrame to the current OPC instance. Args:     second_opc (pd.DataFrame or OPC): The data to append. Returns:     OPC: A new OPC instance with combined data. Raises:     ValueError: If second_opc is not a pandas DataFrame or OPC instance.</p>"},{"location":"api/io/#io.inputs.OPC.edit_crop_season","title":"<code>edit_crop_season(new_planting_date=None, new_harvest_date=None, crop_code=None)</code>","text":"<p>Edit the planting and/or harvest dates for a given year and crop.</p> <p>Parameters: year (int): Year. new_planting_date (datetime, optional): New planting date. If not provided, only harvest date will be updated. new_harvest_date (datetime, optional): New harvest date. If not provided, only planting date will be updated. crop_code (int, optional): Crop code. If not provided, changes the first crop found.</p>"},{"location":"api/io/#io.inputs.OPC.edit_fertilizer_rate","title":"<code>edit_fertilizer_rate(rate, year=2020, month=None, day=None)</code>","text":"<p>Edit the fertilizer rate for a given year.</p> <p>Parameters: rate (float): Fertilizer rate to be set. year (int, optional): Year for the fertilizer rate application. Defaults to 2020. month (int, optional): Month for the fertilizer rate application. If not provided, the first instance is changed. day (int, optional): Day for the fertilizer rate application. Defaults to None.</p>"},{"location":"api/io/#io.inputs.OPC.edit_harvest_date","title":"<code>edit_harvest_date(date, crop_code)</code>","text":"<p>Edit the harvest date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of harvest in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"api/io/#io.inputs.OPC.edit_operation_date","title":"<code>edit_operation_date(code, year, month, day, crop_code=None)</code>","text":"<p>Edit the operation date for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. month (int): Month of operation. day (int): Day of operation. crop_code (int, optional): Crop code.</p>"},{"location":"api/io/#io.inputs.OPC.edit_operation_value","title":"<code>edit_operation_value(code, year, value, crop_code=None)</code>","text":"<p>Edit the operation value for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. value (float): New operation value. crop_code (int, optional): Crop code.</p>"},{"location":"api/io/#io.inputs.OPC.edit_plantation_date","title":"<code>edit_plantation_date(date, crop_code)</code>","text":"<p>Edit the plantation date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of plantation in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"api/io/#io.inputs.OPC.get_harvest_date","title":"<code>get_harvest_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their harvest dates with row indices.</p>"},{"location":"api/io/#io.inputs.OPC.get_plantation_date","title":"<code>get_plantation_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their plantation dates with row indices.</p>"},{"location":"api/io/#io.inputs.OPC.iter_seasons","title":"<code>iter_seasons(start_year=None, end_year=None)</code>","text":"<p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <p>Parameters: start_year (int, optional): The starting year to consider. Defaults to None. end_year (int, optional): The ending year to consider. Defaults to None.</p> <p>dict: A dictionary containing:     - plantation_date: The date of plantation     - harvest_date: The date of harvest     - crop_code: The crop code     - operations: A subset of OPC rows for this season     - plantation_index: The index of the plantation row</p>"},{"location":"api/io/#io.inputs.OPC.load","title":"<code>load(path, start_year=None)</code>  <code>classmethod</code>","text":"<p>Load data from an OPC file into DataFrame.</p> <p>Parameters: path (str): Path to the OPC file. start_year (int, optional): Start year for the OPC file. If not provided, it will be read from the file header.</p> <p>Returns: OPC: An instance of the OPC class containing the loaded data.</p>"},{"location":"api/io/#io.inputs.OPC.new","title":"<code>new(name, start_year)</code>  <code>classmethod</code>","text":"<p>Create a new OPC instance with an empty DataFrame with preset columns, a default header, and provided name and start year.</p> <p>Parameters: name (str): The name for this OPC file/instance. start_year (int): The start year to assign in the OPC instance.</p> <p>Returns: OPC: An OPC instance with no data but with the required metadata set.</p>"},{"location":"api/io/#io.inputs.OPC.remove","title":"<code>remove(opID=None, date=None, cropID=None, XMTU=None, fertID=None, year=None)</code>","text":"<p>Remove operation(s) from the OPC file that match all provided criteria.</p> <p>Parameters:</p> Name Type Description Default <code>opID</code> <code>int</code> <p>Operation ID to match</p> <code>None</code> <code>date</code> <code>str</code> <p>Date to match in format 'YYYY-MM-DD'</p> <code>None</code> <code>cropID</code> <code>int</code> <p>Crop ID to match</p> <code>None</code> <code>XMTU/LYR/pestID/fertID</code> <code>int</code> <p>Machine type/layer/pesticide ID/fertilizer ID to match</p> required <code>year</code> <code>int</code> <p>Year to match</p> <code>None</code>"},{"location":"api/io/#io.inputs.OPC.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p> <p>Parameters: path (str): Path to save the OPC file.</p>"},{"location":"api/io/#io.inputs.OPC.update","title":"<code>update(operation)</code>","text":"<p>Add or update an operation in the OPC file.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Dictionary containing operation details with keys: - opID: Operation ID (required) - cropID: Crop ID (required) - date: Operation date as string 'YYYY-MM-DD' (required) - XMTU/LYR/pestID/fertID: Machine type/years/pesticide ID/fertilizer ID (optional, default 0) - OPV1-OPV8: Additional operation values (optional, default 0)</p> required"},{"location":"api/io/#io.inputs.OPC.update_phu","title":"<code>update_phu(dly, cropcom)</code>","text":"<p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <p>Parameters: dly (DLY): DLY object containing weather data. cropcom (DataFrame): DataFrame containing crop code and TBS values.</p>"},{"location":"api/io/#io.inputs.OPC.validate","title":"<code>validate(duration=None)</code>","text":"<p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p> <p>Parameters: duration (int, optional): Duration of the simulation in years. If None, uses the maximum Yid.</p> <p>Returns: bool: True if the data is valid, False otherwise.</p>"},{"location":"api/io/#io.inputs.SIT","title":"<code>SIT</code>","text":"<p>Methods:</p> Name Description <code>load</code> <p>Class method to load the .sit file and return a SiteFile instance.</p> <code>save</code> <p>Save the current site information to a .SIT file.</p>"},{"location":"api/io/#io.inputs.SIT.elevation","title":"<code>elevation</code>  <code>property</code> <code>writable</code>","text":"<p>Get elevation value.</p>"},{"location":"api/io/#io.inputs.SIT.lat","title":"<code>lat</code>  <code>property</code> <code>writable</code>","text":"<p>Get latitude value.</p>"},{"location":"api/io/#io.inputs.SIT.lon","title":"<code>lon</code>  <code>property</code> <code>writable</code>","text":"<p>Get longitude value.</p>"},{"location":"api/io/#io.inputs.SIT.slope","title":"<code>slope</code>  <code>property</code> <code>writable</code>","text":"<p>Get slope steep value.</p>"},{"location":"api/io/#io.inputs.SIT.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Class method to load the .sit file and return a SiteFile instance.</p> <p>Parameters: file_path (str): Path to the .sit file.</p> <p>Returns: SiteFile: An instance of the SiteFile class with loaded data.</p>"},{"location":"api/io/#io.inputs.SIT.save","title":"<code>save(output_dir)</code>","text":"<p>Save the current site information to a .SIT file.</p>"},{"location":"api/io/#io.inputs.SOL","title":"<code>SOL</code>","text":"<p>Methods:</p> Name Description <code>from_sda</code> <p>Create a Soil object from Soil Data Access using a query.</p> <code>load</code> <p>Load soil data from a file and return a Soil object.</p> <code>save</code> <p>Save the soil data to a file using a template.</p>"},{"location":"api/io/#io.inputs.SOL.from_sda","title":"<code>from_sda(query)</code>  <code>classmethod</code>","text":"<p>Create a Soil object from Soil Data Access using a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>int or str</code> <p>Query string for SoilDataAccess. (mukey or WKT str) ( \"POINT(-123.4567 45.6789)\" )</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from SDA.</p>"},{"location":"api/io/#io.inputs.SOL.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Load soil data from a file and return a Soil object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the soil file.</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from the file.</p>"},{"location":"api/io/#io.inputs.SOL.save","title":"<code>save(filepath, template=None)</code>","text":"<p>Save the soil data to a file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the soil file.</p> required <code>template</code> <code>list</code> <p>Optional list of template lines.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If soil properties DataFrame is empty.</p>"},{"location":"api/io/#io.inputs.dly","title":"<code>dly</code>","text":"<p>Classes:</p> Name Description <code>DLY</code>"},{"location":"api/io/#io.inputs.dly.DLY","title":"<code>DLY</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>load</code> <p>Load data from a DLY file into DataFrame.</p> <code>save</code> <p>Save DataFrame into a DLY file.</p> <code>to_monthly</code> <p>Save as monthly file</p> <code>validate</code> <p>Validate the DataFrame to ensure it contains a continuous range of dates </p>"},{"location":"api/io/#io.inputs.dly.DLY.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load data from a DLY file into DataFrame.</p>"},{"location":"api/io/#io.inputs.dly.DLY.save","title":"<code>save(path=None)</code>","text":"<p>Save DataFrame into a DLY file.</p>"},{"location":"api/io/#io.inputs.dly.DLY.to_monthly","title":"<code>to_monthly(path=None)</code>","text":"<p>Save as monthly file</p>"},{"location":"api/io/#io.inputs.dly.DLY.validate","title":"<code>validate(start_date, end_date)</code>","text":"<p>Validate the DataFrame to ensure it contains a continuous range of dates  between start_date and end_date, without duplicates.</p>"},{"location":"api/io/#io.inputs.opc","title":"<code>opc</code>","text":"<p>Classes:</p> Name Description <code>OPC</code>"},{"location":"api/io/#io.inputs.opc.OPC","title":"<code>OPC</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>append</code> <p>Append another OPC or DataFrame to the current OPC instance.</p> <code>edit_crop_season</code> <p>Edit the planting and/or harvest dates for a given year and crop.</p> <code>edit_fertilizer_rate</code> <p>Edit the fertilizer rate for a given year.</p> <code>edit_harvest_date</code> <p>Edit the harvest date for a given year and crop.</p> <code>edit_operation_date</code> <p>Edit the operation date for a given year.</p> <code>edit_operation_value</code> <p>Edit the operation value for a given year.</p> <code>edit_plantation_date</code> <p>Edit the plantation date for a given year and crop.</p> <code>get_harvest_date</code> <p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <code>get_plantation_date</code> <p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <code>iter_seasons</code> <p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <code>load</code> <p>Load data from an OPC file into DataFrame.</p> <code>new</code> <p>Create a new OPC instance with an empty DataFrame with preset columns, a default header,</p> <code>remove</code> <p>Remove operation(s) from the OPC file that match all provided criteria.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>update</code> <p>Add or update an operation in the OPC file.</p> <code>update_phu</code> <p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <code>validate</code> <p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p>"},{"location":"api/io/#io.inputs.opc.OPC.IAUI","title":"<code>IAUI</code>  <code>property</code> <code>writable</code>","text":"<p>Get the auto-irrigation implement ID from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if auto-irrigation is enabled (72), False if disabled (0)</p>"},{"location":"api/io/#io.inputs.opc.OPC.LUN","title":"<code>LUN</code>  <code>property</code> <code>writable</code>","text":"<p>Get the land use number from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The land use number from the first 4 characters of header line 2</p>"},{"location":"api/io/#io.inputs.opc.OPC.append","title":"<code>append(second_opc)</code>","text":"<p>Append another OPC or DataFrame to the current OPC instance. Args:     second_opc (pd.DataFrame or OPC): The data to append. Returns:     OPC: A new OPC instance with combined data. Raises:     ValueError: If second_opc is not a pandas DataFrame or OPC instance.</p>"},{"location":"api/io/#io.inputs.opc.OPC.edit_crop_season","title":"<code>edit_crop_season(new_planting_date=None, new_harvest_date=None, crop_code=None)</code>","text":"<p>Edit the planting and/or harvest dates for a given year and crop.</p> <p>Parameters: year (int): Year. new_planting_date (datetime, optional): New planting date. If not provided, only harvest date will be updated. new_harvest_date (datetime, optional): New harvest date. If not provided, only planting date will be updated. crop_code (int, optional): Crop code. If not provided, changes the first crop found.</p>"},{"location":"api/io/#io.inputs.opc.OPC.edit_fertilizer_rate","title":"<code>edit_fertilizer_rate(rate, year=2020, month=None, day=None)</code>","text":"<p>Edit the fertilizer rate for a given year.</p> <p>Parameters: rate (float): Fertilizer rate to be set. year (int, optional): Year for the fertilizer rate application. Defaults to 2020. month (int, optional): Month for the fertilizer rate application. If not provided, the first instance is changed. day (int, optional): Day for the fertilizer rate application. Defaults to None.</p>"},{"location":"api/io/#io.inputs.opc.OPC.edit_harvest_date","title":"<code>edit_harvest_date(date, crop_code)</code>","text":"<p>Edit the harvest date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of harvest in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"api/io/#io.inputs.opc.OPC.edit_operation_date","title":"<code>edit_operation_date(code, year, month, day, crop_code=None)</code>","text":"<p>Edit the operation date for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. month (int): Month of operation. day (int): Day of operation. crop_code (int, optional): Crop code.</p>"},{"location":"api/io/#io.inputs.opc.OPC.edit_operation_value","title":"<code>edit_operation_value(code, year, value, crop_code=None)</code>","text":"<p>Edit the operation value for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. value (float): New operation value. crop_code (int, optional): Crop code.</p>"},{"location":"api/io/#io.inputs.opc.OPC.edit_plantation_date","title":"<code>edit_plantation_date(date, crop_code)</code>","text":"<p>Edit the plantation date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of plantation in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"api/io/#io.inputs.opc.OPC.get_harvest_date","title":"<code>get_harvest_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their harvest dates with row indices.</p>"},{"location":"api/io/#io.inputs.opc.OPC.get_plantation_date","title":"<code>get_plantation_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their plantation dates with row indices.</p>"},{"location":"api/io/#io.inputs.opc.OPC.iter_seasons","title":"<code>iter_seasons(start_year=None, end_year=None)</code>","text":"<p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <p>Parameters: start_year (int, optional): The starting year to consider. Defaults to None. end_year (int, optional): The ending year to consider. Defaults to None.</p> <p>dict: A dictionary containing:     - plantation_date: The date of plantation     - harvest_date: The date of harvest     - crop_code: The crop code     - operations: A subset of OPC rows for this season     - plantation_index: The index of the plantation row</p>"},{"location":"api/io/#io.inputs.opc.OPC.load","title":"<code>load(path, start_year=None)</code>  <code>classmethod</code>","text":"<p>Load data from an OPC file into DataFrame.</p> <p>Parameters: path (str): Path to the OPC file. start_year (int, optional): Start year for the OPC file. If not provided, it will be read from the file header.</p> <p>Returns: OPC: An instance of the OPC class containing the loaded data.</p>"},{"location":"api/io/#io.inputs.opc.OPC.new","title":"<code>new(name, start_year)</code>  <code>classmethod</code>","text":"<p>Create a new OPC instance with an empty DataFrame with preset columns, a default header, and provided name and start year.</p> <p>Parameters: name (str): The name for this OPC file/instance. start_year (int): The start year to assign in the OPC instance.</p> <p>Returns: OPC: An OPC instance with no data but with the required metadata set.</p>"},{"location":"api/io/#io.inputs.opc.OPC.remove","title":"<code>remove(opID=None, date=None, cropID=None, XMTU=None, fertID=None, year=None)</code>","text":"<p>Remove operation(s) from the OPC file that match all provided criteria.</p> <p>Parameters:</p> Name Type Description Default <code>opID</code> <code>int</code> <p>Operation ID to match</p> <code>None</code> <code>date</code> <code>str</code> <p>Date to match in format 'YYYY-MM-DD'</p> <code>None</code> <code>cropID</code> <code>int</code> <p>Crop ID to match</p> <code>None</code> <code>XMTU/LYR/pestID/fertID</code> <code>int</code> <p>Machine type/layer/pesticide ID/fertilizer ID to match</p> required <code>year</code> <code>int</code> <p>Year to match</p> <code>None</code>"},{"location":"api/io/#io.inputs.opc.OPC.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p> <p>Parameters: path (str): Path to save the OPC file.</p>"},{"location":"api/io/#io.inputs.opc.OPC.update","title":"<code>update(operation)</code>","text":"<p>Add or update an operation in the OPC file.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Dictionary containing operation details with keys: - opID: Operation ID (required) - cropID: Crop ID (required) - date: Operation date as string 'YYYY-MM-DD' (required) - XMTU/LYR/pestID/fertID: Machine type/years/pesticide ID/fertilizer ID (optional, default 0) - OPV1-OPV8: Additional operation values (optional, default 0)</p> required"},{"location":"api/io/#io.inputs.opc.OPC.update_phu","title":"<code>update_phu(dly, cropcom)</code>","text":"<p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <p>Parameters: dly (DLY): DLY object containing weather data. cropcom (DataFrame): DataFrame containing crop code and TBS values.</p>"},{"location":"api/io/#io.inputs.opc.OPC.validate","title":"<code>validate(duration=None)</code>","text":"<p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p> <p>Parameters: duration (int, optional): Duration of the simulation in years. If None, uses the maximum Yid.</p> <p>Returns: bool: True if the data is valid, False otherwise.</p>"},{"location":"api/io/#io.inputs.sit","title":"<code>sit</code>","text":"<p>Classes:</p> Name Description <code>SIT</code>"},{"location":"api/io/#io.inputs.sit.SIT","title":"<code>SIT</code>","text":"<p>Methods:</p> Name Description <code>load</code> <p>Class method to load the .sit file and return a SiteFile instance.</p> <code>save</code> <p>Save the current site information to a .SIT file.</p>"},{"location":"api/io/#io.inputs.sit.SIT.elevation","title":"<code>elevation</code>  <code>property</code> <code>writable</code>","text":"<p>Get elevation value.</p>"},{"location":"api/io/#io.inputs.sit.SIT.lat","title":"<code>lat</code>  <code>property</code> <code>writable</code>","text":"<p>Get latitude value.</p>"},{"location":"api/io/#io.inputs.sit.SIT.lon","title":"<code>lon</code>  <code>property</code> <code>writable</code>","text":"<p>Get longitude value.</p>"},{"location":"api/io/#io.inputs.sit.SIT.slope","title":"<code>slope</code>  <code>property</code> <code>writable</code>","text":"<p>Get slope steep value.</p>"},{"location":"api/io/#io.inputs.sit.SIT.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Class method to load the .sit file and return a SiteFile instance.</p> <p>Parameters: file_path (str): Path to the .sit file.</p> <p>Returns: SiteFile: An instance of the SiteFile class with loaded data.</p>"},{"location":"api/io/#io.inputs.sit.SIT.save","title":"<code>save(output_dir)</code>","text":"<p>Save the current site information to a .SIT file.</p>"},{"location":"api/io/#io.inputs.sol","title":"<code>sol</code>","text":"<p>Classes:</p> Name Description <code>SOL</code>"},{"location":"api/io/#io.inputs.sol.SOL","title":"<code>SOL</code>","text":"<p>Methods:</p> Name Description <code>from_sda</code> <p>Create a Soil object from Soil Data Access using a query.</p> <code>load</code> <p>Load soil data from a file and return a Soil object.</p> <code>save</code> <p>Save the soil data to a file using a template.</p>"},{"location":"api/io/#io.inputs.sol.SOL.from_sda","title":"<code>from_sda(query)</code>  <code>classmethod</code>","text":"<p>Create a Soil object from Soil Data Access using a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>int or str</code> <p>Query string for SoilDataAccess. (mukey or WKT str) ( \"POINT(-123.4567 45.6789)\" )</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from SDA.</p>"},{"location":"api/io/#io.inputs.sol.SOL.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Load soil data from a file and return a Soil object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the soil file.</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from the file.</p>"},{"location":"api/io/#io.inputs.sol.SOL.save","title":"<code>save(filepath, template=None)</code>","text":"<p>Save the soil data to a file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the soil file.</p> required <code>template</code> <code>list</code> <p>Optional list of template lines.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If soil properties DataFrame is empty.</p>"},{"location":"api/io/#io.outputs","title":"<code>outputs</code>","text":"<p>Classes:</p> Name Description <code>ACY</code> <code>DGN</code> <code>DSL</code> <code>DWC</code>"},{"location":"api/io/#io.outputs.ACY","title":"<code>ACY</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the ACY data.</p>"},{"location":"api/io/#io.outputs.ACY.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the ACY data.</p>"},{"location":"api/io/#io.outputs.DGN","title":"<code>DGN</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DGN data.</p>"},{"location":"api/io/#io.outputs.DGN.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DGN data.</p>"},{"location":"api/io/#io.outputs.DSL","title":"<code>DSL</code>","text":"<p>Methods:</p> Name Description <code>get_data</code> <p>Return stored water data.</p>"},{"location":"api/io/#io.outputs.DSL.get_data","title":"<code>get_data()</code>","text":"<p>Return stored water data.</p>"},{"location":"api/io/#io.outputs.DWC","title":"<code>DWC</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DWC data.</p>"},{"location":"api/io/#io.outputs.DWC.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DWC data.</p>"},{"location":"api/io/#io.outputs.all","title":"<code>all</code>","text":"<p>Classes:</p> Name Description <code>ACY</code> <code>DGN</code> <code>DWC</code>"},{"location":"api/io/#io.outputs.all.ACY","title":"<code>ACY</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the ACY data.</p>"},{"location":"api/io/#io.outputs.all.ACY.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the ACY data.</p>"},{"location":"api/io/#io.outputs.all.DGN","title":"<code>DGN</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DGN data.</p>"},{"location":"api/io/#io.outputs.all.DGN.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DGN data.</p>"},{"location":"api/io/#io.outputs.all.DWC","title":"<code>DWC</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DWC data.</p>"},{"location":"api/io/#io.outputs.all.DWC.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DWC data.</p>"},{"location":"api/io/#io.outputs.sw","title":"<code>sw</code>","text":"<p>Classes:</p> Name Description <code>DSL</code>"},{"location":"api/io/#io.outputs.sw.DSL","title":"<code>DSL</code>","text":"<p>Methods:</p> Name Description <code>get_data</code> <p>Return stored water data.</p>"},{"location":"api/io/#io.outputs.sw.DSL.get_data","title":"<code>get_data()</code>","text":"<p>Return stored water data.</p>"},{"location":"api/io/#io.parm","title":"<code>parm</code>","text":"<p>Classes:</p> Name Description <code>Parm</code>"},{"location":"api/io/#io.parm.Parm","title":"<code>Parm</code>","text":"<p>Methods:</p> Name Description <code>constraints</code> <p>Returns the constraints (min, max ranges) for the parameters.</p> <code>edit</code> <p>Updates the parameters in the DataFrame with new values.</p> <code>get_vars</code> <p>Returns the vars DataFrame with an additional column containing current values.</p> <code>read_parm</code> <p>Reads and constructs a DataFrame from a .DAT file.</p> <code>save</code> <p>Saves the current DataFrame to a .DAT file.</p> <code>set_sensitive</code> <p>Sets sensitive parameters based on a CSV path or list of parameter names.</p>"},{"location":"api/io/#io.parm.Parm.current","title":"<code>current</code>  <code>property</code>","text":"<p>Returns the current values of parameters in the DataFrame.</p>"},{"location":"api/io/#io.parm.Parm.constraints","title":"<code>constraints()</code>","text":"<p>Returns the constraints (min, max ranges) for the parameters.</p>"},{"location":"api/io/#io.parm.Parm.edit","title":"<code>edit(values)</code>","text":"<p>Updates the parameters in the DataFrame with new values.</p>"},{"location":"api/io/#io.parm.Parm.get_vars","title":"<code>get_vars()</code>","text":"<p>Returns the vars DataFrame with an additional column containing current values.</p>"},{"location":"api/io/#io.parm.Parm.read_parm","title":"<code>read_parm(file_name)</code>","text":"<p>Reads and constructs a DataFrame from a .DAT file.</p>"},{"location":"api/io/#io.parm.Parm.save","title":"<code>save(path)</code>","text":"<p>Saves the current DataFrame to a .DAT file.</p>"},{"location":"api/io/#io.parm.Parm.set_sensitive","title":"<code>set_sensitive(parms_input, all=False)</code>","text":"<p>Sets sensitive parameters based on a CSV path or list of parameter names. If <code>all</code> is True, all parameters are considered sensitive.</p> <p>Parameters:</p> Name Type Description Default <code>parms_input</code> <code>str or list</code> <p>Either a CSV file path or list of parameter names to select</p> required <code>all</code> <code>bool</code> <p>If True, all parameters are considered sensitive regardless of input</p> <code>False</code>"},{"location":"api/soil/","title":"Soil Module","text":""},{"location":"api/soil/#soil","title":"<code>soil</code>","text":"<p>Classes:</p> Name Description <code>SoilDataAccess</code>"},{"location":"api/soil/#soil.SoilDataAccess","title":"<code>SoilDataAccess</code>","text":"<p>Methods:</p> Name Description <code>fetch_properties</code> <p>Fetches soil data based on the input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <code>fetch_slope_length</code> <p>Fetches the slope length for a given input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <code>fetch_value</code> <p>Fetches specific values from a given table for a given input value.</p> <code>get_cokey_from_wkt</code> <p>Fetches a list of cokey (Component Key) values based on a WKT spatial location.</p> <code>get_mukey</code> <p>Fetches the mukey for a given WKT location.</p> <code>get_mukey_list</code> <p>Fetches the mukey for a given WKT location.</p> <code>query</code> <p>Performs a query to the NRCS Soil Data Access service.</p>"},{"location":"api/soil/#soil.SoilDataAccess.fetch_properties","title":"<code>fetch_properties(input_value)</code>  <code>staticmethod</code>","text":"<p>Fetches soil data based on the input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <p>Args: input (int or str): The input value representing either a mukey (int) or a WKT location (str).</p> <p>Returns: pd.DataFrame: A DataFrame containing the soil data for the specified input.</p>"},{"location":"api/soil/#soil.SoilDataAccess.fetch_slope_length","title":"<code>fetch_slope_length(input_value)</code>  <code>staticmethod</code>","text":"<p>Fetches the slope length for a given input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <p>Args: input (int or str): The input value representing either a mukey (int) or a WKT location (str).</p> <p>Returns: float: The slope length for the specified input value.</p>"},{"location":"api/soil/#soil.SoilDataAccess.fetch_value","title":"<code>fetch_value(input_value, values, table)</code>  <code>staticmethod</code>","text":"<p>Fetches specific values from a given table for a given input value.</p> <p>Args: input_value (int or str): The input value representing either a mukey (int) or a WKT location (str). values (List[str]): A list of column names for the values to fetch. table (str): The name of the table to query.</p> <p>Returns: List[Dict[str, Any]]: A list of dictionaries containing the key (mukey, cokey, etc.) and fetched values.</p>"},{"location":"api/soil/#soil.SoilDataAccess.get_cokey_from_wkt","title":"<code>get_cokey_from_wkt(wkt_string)</code>  <code>staticmethod</code>","text":"<p>Fetches a list of cokey (Component Key) values based on a WKT spatial location.</p> <p>Args: wkt_string (str): A WKT string representing the spatial location (e.g., POLYGON, POINT).</p> <p>Returns: List[int]: A list of cokey values corresponding to the input WKT location.</p>"},{"location":"api/soil/#soil.SoilDataAccess.get_mukey","title":"<code>get_mukey(wkt)</code>  <code>staticmethod</code>","text":"<p>Fetches the mukey for a given WKT location.</p> <p>Args: wkt (str): The WKT location.</p> <p>Returns: int: The mukey for the specified location.</p>"},{"location":"api/soil/#soil.SoilDataAccess.get_mukey_list","title":"<code>get_mukey_list(wkt)</code>  <code>staticmethod</code>","text":"<p>Fetches the mukey for a given WKT location.</p> <p>Args: wkt (str): The WKT location.</p> <p>Returns: int: The mukey for the specified location.</p>"},{"location":"api/soil/#soil.SoilDataAccess.query","title":"<code>query(query)</code>  <code>staticmethod</code>","text":"<p>Performs a query to the NRCS Soil Data Access service.</p> <p>Args: query (str): The SQL query to execute.</p> <p>Returns: pd.DataFrame: A DataFrame containing the results of the query.</p> <p>Raises: ValueError: If no data is found for the provided query. requests.RequestException: If there is an issue with the network request.</p>"},{"location":"api/utils/","title":"Utils Module","text":""},{"location":"api/utils/#utils","title":"<code>utils</code>","text":"<p>Classes:</p> Name Description <code>GeoInterface</code> <p>A class that provides a unified interface for working with geospatial data from various sources.</p> <p>Functions:</p> Name Description <code>sample_raster_nearest</code> <p>Sample a raster file at specific coordinates, taking the nearest pixel.</p> <code>reproject_crop_raster</code> <p>Reproject and crop a raster file.</p> <code>copy_file</code> <p>Copy a file from source to destination, optionally creating a symbolic link instead.</p> <code>parallel_executor</code> <p>Pebble-only parallel executor.</p> <code>read_gdb_layer</code> <p>Reads selected columns from a GDB layer and returns them in a pandas DataFrame.</p>"},{"location":"api/utils/#utils.GeoInterface","title":"<code>GeoInterface</code>","text":"<p>A class that provides a unified interface for working with geospatial data from various sources.</p> <p>This class can load and process data from raster files (.tif/.tiff), CSV files, shapefiles, or pandas DataFrames. It provides methods for finding nearest neighbors using haversine distances between geographic coordinates.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The loaded data, containing at minimum 'lat' and 'lon' columns.</p> <code>points_rad</code> <code>ndarray</code> <p>The latitude/longitude points converted to radians.</p> <code>tree</code> <code>BallTree</code> <p>A BallTree structure for efficient nearest neighbor queries.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>str or DataFrame</code> <p>The input data source. Can be: - Path to a raster file (.tif/.tiff) - Path to a CSV file (.csv) - Path to a shapefile (.shp) - A pandas DataFrame with 'lat' and 'lon' columns</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data source format is unsupported or required columns are missing.</p> <p>Methods:</p> Name Description <code>find_nearest</code> <p>Find the nearest 'k' data points for each latitude and longitude provided separately.</p> <code>lookup</code> <p>Find the nearest data point to a single latitude and longitude.</p>"},{"location":"api/utils/#utils.GeoInterface.find_nearest","title":"<code>find_nearest(lats, lons, k=1)</code>","text":"<p>Find the nearest 'k' data points for each latitude and longitude provided separately.</p> <p>Parameters:</p> Name Type Description Default <code>lats</code> <code>list of float</code> <p>A list of latitudes.</p> required <code>lons</code> <code>list of float</code> <p>A list of longitudes.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors to find.</p> <code>1</code> <p>Returns:</p> Type Description <p>list or pandas.DataFrame: Depending on 'k', returns a DataFrame or a list of DataFrames with the nearest points.</p>"},{"location":"api/utils/#utils.GeoInterface.lookup","title":"<code>lookup(lat, lon)</code>","text":"<p>Find the nearest data point to a single latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of the query point.</p> required <code>lon</code> <code>float</code> <p>Longitude of the query point.</p> required <p>Returns:</p> Type Description <p>pandas.Series: The row from the DataFrame corresponding to the nearest point.</p>"},{"location":"api/utils/#utils.sample_raster_nearest","title":"<code>sample_raster_nearest(raster_file, coords, crs='EPSG:4326')</code>","text":"<p>Sample a raster file at specific coordinates, taking the nearest pixel.</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>str</code> <p>Path to the raster file.</p> required <code>coords</code> <code>list of tuples</code> <p>List of (x, y)/(lon, lat) tuples.</p> required <code>crs</code> <code>str</code> <p>The CRS the coords are in.</p> <code>'EPSG:4326'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with band names as keys and lists of pixel values at the given coordinates as values.</p>"},{"location":"api/utils/#utils.reproject_crop_raster","title":"<code>reproject_crop_raster(src, dst, out_epsg, min_coords, max_coords)</code>","text":"<p>Reproject and crop a raster file. src_filename: Source file path. dst_filename: Destination file path. out_epsg: Output coordinate system as EPSG code. min_lon, min_lat, max_lon, max_lat: Bounding box coordinates.</p>"},{"location":"api/utils/#utils.copy_file","title":"<code>copy_file(src, dest, symlink=False)</code>","text":"<p>Copy a file from source to destination, optionally creating a symbolic link instead.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>Path to the source file</p> required <code>dest</code> <code>str</code> <p>Path to the destination file/link</p> required <code>symlink</code> <code>bool</code> <p>Whether to create a symbolic link instead of copying.  Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>str | None: Path to the destination file if successful, None if source doesn't exist</p> Note <p>If symlink is True and the destination already exists, it will be removed first.</p>"},{"location":"api/utils/#utils.parallel_executor","title":"<code>parallel_executor(func, args, method='Process', max_workers=10, return_value=False, bar=True, timeout=None, verbose=True)</code>","text":"<p>Pebble-only parallel executor. - method: 'Process' or 'Thread' - args: iterable of items; each item can be a single arg or a tuple for *args - timeout: per-task timeout (sec). For ProcessPool it is enforced at schedule(),            for ThreadPool it's enforced at result(). Returns: (results, failed_indices)</p>"},{"location":"api/utils/#utils.read_gdb_layer","title":"<code>read_gdb_layer(gdb_data, layer_name, columns=None, names=None)</code>","text":"<p>Reads selected columns from a GDB layer and returns them in a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>gdb</code> <code>gdb</code> <p>The GDB file opened by ogr.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer to read.</p> required <code>columns</code> <code>list</code> <p>List of column indices to read. If None, all columns are read.</p> <code>None</code> <code>names</code> <code>list</code> <p>List of column names corresponding to the indices in <code>columns</code>. If None, all column names are inferred from the layer definition.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: The resulting dataframe.</p>"},{"location":"api/weather/","title":"Weather Module","text":""},{"location":"api/weather/#weather","title":"<code>weather</code>","text":"<p>Functions:</p> Name Description <code>fetch_list</code> <p>Fetches weather data based on the input type which could be coordinates, a CSV file, or a shapefile.</p>"},{"location":"api/weather/#weather.fetch_list","title":"<code>fetch_list(config_file, input_data, output_dir, raw=False)</code>","text":"<p>Fetches weather data based on the input type which could be coordinates, a CSV file, or a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>str</code> <p>Could be latitude and longitude as a string, path to a CSV file, or path to a shapefile.</p> required <code>output_dir</code> <code>str</code> <p>Directory or file path where the output should be saved.</p> required <code>raw</code> <code>bool</code> <p>Whether to save as raw CSV (True) or DLY format (False). Defaults to False.</p> <code>False</code>"},{"location":"blog/","title":"Blog","text":""},{"location":"getting_started/calibration/","title":"Model Parameter Calibration using GeoEPIC","text":"<p>This guide explains how to use GeoEPIC's calibration module to tune EPIC model parameters based on observational data (e.g., yield, LAI, NEE). By adjusting parameters, the model can better reflect specific local conditions or experimental results. GeoEPIC integrates with the PyGMO library for access to various optimization algorithms.</p> <p>This process assumes you have a workspace folder already set up with necessary EPIC input files and a target data file (e.g., <code>target_yields.csv</code> with <code>SiteID</code> and <code>Yield</code> columns).</p> <p>Here are the steps involved:</p>"},{"location":"getting_started/calibration/#1-prepare-your-environment-and-load-data","title":"1. Prepare Your Environment and Load Data","text":"<p>Import necessary libraries, initialize the GeoEPIC workspace, and load your target observational data.</p> <ul> <li>Import Python modules: <code>numpy</code> and <code>pandas</code> for data manipulation, GeoEPIC's <code>Parm</code>, <code>CropCom</code>, and <code>Workspace</code> for model interaction, <code>os</code> for file operations, and <code>pygmo</code> for the optimization algorithms.</li> <li>Initialize the <code>Workspace</code> object by pointing it to your main configuration file (<code>config.yml</code>). Ensure this file has the correct paths and settings for your calibration experiment.</li> <li>Optionally, clear previous logs and outputs to start fresh.</li> <li>Load your target data (e.g., measured yields) into a pandas DataFrame and convert it into a dictionary format where keys are <code>SiteID</code>s and values are the target yields. Assign this dictionary to <code>exp.target_yields</code>.</li> </ul> <pre><code>import numpy as np\nimport pandas as pd\nfrom geoEpic.io import Parm, CropCom, ACY # Added ACY import here as it's used later\nfrom geoEpic.core import Workspace\nimport os\nimport pygmo as pg\n\n# Initiate the Workspace Class (edit config.yml first)\nexp = Workspace('./config.yml')\n\n# Clear the logs and output directory (optional)\nexp.clear_logs()\nexp.clear_outputs()\n\n# Load Target Yields File (replace with your actual path)\ntarget_yields = pd.read_csv('path/to/target.csv')\nexp.target_yields = target_yields.set_index('SiteID')['yields'].to_dict()\n</code></pre>"},{"location":"getting_started/calibration/#2-define-sensitive-parameters","title":"2. Define Sensitive Parameters","text":"<p>Specify which parameters within the EPIC model's input files should be adjusted ('tuned') during the calibration process.</p> <ul> <li>Load the relevant parameter files using GeoEPIC's <code>io</code> classes (e.g., <code>CropCom</code> for crop-specific parameters in <code>cropcom.DAT</code>, <code>Parm</code> for general parameters typically in <code>EPIC.PARM</code>). Load default versions usually kept in a separate calibration folder.</li> <li>Use the <code>.set_sensitive()</code> method on these objects to mark parameters for calibration. You can specify parameters directly (like 'WA', 'HI' for a specific crop code in <code>CropCom</code>) or provide a path to a file listing sensitive parameters (useful for <code>Parm</code>).</li> <li>Save the modified parameter files (which now know which parameters are sensitive) to your main model directory (<code>./model</code>), overwriting the existing ones.</li> <li>You can verify which parameters are marked as sensitive by accessing the <code>.prms</code> attribute of the parameter objects.</li> </ul> <pre><code># Load default cropcom.DAT file from a calibration files directory\ncropcom = CropCom('./calibration_files/defaults')\n\n# Set 'WA', 'HI', 'WSYF' as sensitive for crop code 2 (e.g., corn)\ncropcom.set_sensitive(['WA', 'HI', 'WSYF'], [2])\n\n# Save the loaded cropcom.DAT to the model folder\ncropcom.save(f'./model')\n\n# Verify sensitive CropCom parameters (optional)\n# cropcom.prms\n\n# Load default general parameters (e.g., EPIC.PARM)\nieparm = Parm('./calibration_files/defaults')\n# Set sensitive parameters based on a list in a file\nieparm.set_sensitive(['./calibration_files/sensitivity/parm_yld.csv'])\n# Save the modified PARM file to the model folder\nieparm.save(f'./model')\n\n# Verify sensitive general parameters (optional)\n# ieparm.prms\n</code></pre>"},{"location":"getting_started/calibration/#3-define-the-objective-function","title":"3. Define the Objective Function","text":"<p>Create functions that tell the optimization algorithm how 'good' a set of parameter values is by comparing simulation results to the target data.</p> <ul> <li>Use the <code>@exp.logger</code> decorator to define a function (e.g., <code>yield_error</code>) that runs after each site simulation. This function receives the <code>site</code> object, extracts the relevant simulated output (e.g., last year's yield using <code>ACY</code>), compares it to the target yield for that site (retrieved from <code>exp.target_yields</code>), calculates an error metric (e.g., absolute difference), and returns it in a dictionary. These logged values are stored by the workspace.</li> <li>Use the <code>@exp.objective</code> decorator to define a function (e.g., <code>aggregate</code>) that runs after all sites in an optimization iteration are simulated. This function fetches the logged results (using <code>exp.fetch_log</code>), aggregates the errors across all sites (e.g., calculates the mean error), and returns a single value (or list of values for multi-objective optimization). This aggregated value is the 'fitness' score that the optimization algorithm tries to minimize.</li> </ul> <pre><code>@exp.logger\ndef yield_error(site):\n  '''\n  Calculates yield error for a single site after simulation.\n  '''\n  target_yield = exp.target_yields[site.site_id]\n  # Ensure ACY is imported: from geoEpic.io import ACY\n  simulated_yields = ACY(site.outputs['ACY']).get_var('YLDG')\n  # Handle cases where simulation might not produce yield (e.g., return a large error or filter later)\n  if simulated_yields is None or simulated_yields.empty:\n      return {'error': np.inf} # Assign high error if no yield simulated\n  last_year_yield = simulated_yields['YLDG'].iloc[-1] # Use iloc for position\n  return {'error': np.abs(target_yield - last_year_yield)}\n\n@exp.objective\ndef aggregate():\n  '''\n  Aggregates errors from all sites to provide a single fitness value.\n  '''\n  logged_data = exp.fetch_log('yield_error')\n  logged_data = logged_data.dropna()\n  # Handle case with no valid logged data\n  if logged_data.empty or not np.isfinite(logged_data['error']).any():\n      return [np.inf] # Return high fitness if no valid errors\n  # Calculate mean of finite errors\n  valid_errors = logged_data.loc[np.isfinite(logged_data['error']), 'error']\n  return [valid_errors.mean()] if not valid_errors.empty else [np.inf]\n</code></pre>"},{"location":"getting_started/calibration/#4-configure-and-run-the-calibration","title":"4. Configure and Run the Calibration","text":"<p>Set up the optimization algorithm using PyGMO, link it to the GeoEPIC workspace and parameters, and execute the calibration process.</p> <ul> <li>Create a <code>PygmoProblem</code> object, passing it the initialized <code>Workspace</code> (<code>exp</code>) and the parameter objects (<code>cropcom</code>, <code>ieparm</code>) that have sensitive parameters defined. This wraps your setup for PyGMO.</li> <li>Optionally, run <code>exp.run()</code> before optimization to see the initial fitness (error) using the default parameter values.</li> <li>Choose a PyGMO optimization algorithm (e.g., <code>pg.pso_gen</code> for Particle Swarm Optimization) and configure its settings (e.g., number of generations <code>gen</code>). Set verbosity to control how much information the algorithm prints during execution.</li> <li>Create an initial <code>population</code> for the algorithm. This represents the starting set of parameter combinations that the algorithm will evaluate and improve upon.</li> <li>Run the optimization using <code>algo.evolve(population)</code>. This is the main calibration step where the algorithm iteratively adjusts the sensitive parameters, runs EPIC simulations via the workspace, evaluates the results using your objective function, and converges towards parameter values that minimize the error.</li> <li>After the evolution completes, run <code>exp.run()</code> again. This will now use the best parameter set found by the optimization algorithm, showing the final, hopefully improved (lower), fitness value.</li> </ul> <pre><code>import pygmo as pg\nfrom geoEpic.core import PygmoProblem\n\n# Define pygmo problem linking workspace, CropCom, and Parm objects\nproblem = PygmoProblem(exp, cropcom, ieparm)\n\n# Check initial fitness with default parameters (optional)\nprint('Fitness before Optimization:', exp.run())\n\n# Choose an algorithm (PSO_gen) and settings\nalgo = pg.algorithm(pg.pso_gen(gen = 45, memory = True)) # 45 generations\nalgo.set_verbosity(1) # Print progress every generation\n\n# Create initial population\nprint(\"Initial Population\")\npopulation = pg.population(problem, size = 50) # Population size of 50\n\n# Run the optimization\nprint(\"Optimizing...\")\npopulation = algo.evolve(population)\n\n# Check final fitness with optimized parameters\nprint('Fitness After Optimization:', exp.run())\n</code></pre>"},{"location":"getting_started/calibration/#result-of-calibration","title":"Result of Calibration","text":"<ul> <li>The primary result of the calibration is a set of optimized parameter values for the sensitive parameters you defined in Step 2. These optimized values are automatically written back into the corresponding parameter files (e.g., <code>cropcom.DAT</code>, <code>EPIC.PARM</code>) located in your <code>./model</code> directory.</li> <li>The optimization process aims to find parameter values that minimize the objective function defined in Step 3 (e.g., minimize the average absolute error between simulated and target yields).</li> <li>The final fitness value printed after optimization indicates how well the model reproduces the target data using the newly calibrated parameters. A lower fitness value generally signifies a better match between the simulation and the observations.</li> </ul>"},{"location":"getting_started/contributing/","title":"Contributing to GeoEPIC","text":"<p>This document outlines how to contribute to GeoEPIC through code changes, bug reports, feature requests, and other helpful contributions.</p>"},{"location":"getting_started/contributing/#reporting-bugs","title":"Reporting Bugs","text":"<ul> <li>Please review the Issues page to check if a similar problem has already been reported.</li> <li>If you do not find an existing report, create a new issue with the following information:<ul> <li>A clear title that summarizes the problem</li> <li>A detailed description of the bug, outlining what went wrong</li> <li>Steps to reproduce the bug.</li> </ul> </li> </ul>"},{"location":"getting_started/contributing/#feature-requests","title":"Feature Requests","text":"<ul> <li>To propose a new feature, submit your request via the Issues page.</li> <li>Make sure to include:<ul> <li>A comprehensive description of the feature you are suggesting</li> <li>The expected benefits of this feature will enhance the project</li> </ul> </li> </ul>"},{"location":"getting_started/contributing/#code-contributions","title":"Code Contributions","text":"<ul> <li>Contact the maintainers directly for any contributions or potential collaborations.</li> </ul>"},{"location":"getting_started/contributing/#questions-support","title":"Questions &amp; Support","text":"<ul> <li>Open an issue for general questions</li> <li>Or you can reach out to the maintainers directly</li> </ul> <p>Thank you for your interest in contributing to GeoEPIC!</p>"},{"location":"getting_started/gee/","title":"Gee","text":"<p>Google Earth Engine (GEE) is a cloud-based platform for planetary-scale environmental data analysis. It hosts a vast collection of satellite imagery and other geospatial datasets, enabling users to perform large-scale data analysis. To explore the available datasets, visit Google Earth Engine's dataset catalog and GEE Community Catalog. Private assets can also be uploaded to Earth Engine, to use them in combination with existing datasets.</p> <p>This module can be used to combine various datasets and extract the required timeseries directly from Google Earth Engine.</p> <p></p>"},{"location":"getting_started/gee/#1-configuration-file-breakdown","title":"1. Configuration file breakdown","text":"<p>This module utilizes a configuration file in YAML format to extract data from Google Earth Engine. The configuration file defines global parameters, Earth Engine collections, and derived variables. Below is a detailed explanation of each section.</p> <pre><code># filename: config.yml\n\n# Global parameters\nglobal_scope:\n  time_range: ['2016-01-01', '2021-12-31']\n  variables: ['nir', 'red', 'green', 'ndvi']  \n  resolution: 10\n\n# Specify Earth Engine collections and their respective variables\ncollections:\n\n  le07:\n    collection: LANDSAT/LE07/C02/T1_L2\n    select:  (b('QA_PIXEL') &gt;&gt; 6) &amp; 1\n    variables:\n      nir: b('SR_B4')*0.0000275 - 0.2\n      red: b('SR_B3')*0.0000275 - 0.2\n      green: b('SR_B2')*0.0000275 - 0.2\n\n# Derived variables\nderived_variables:\n  ndvi: '(nir - red)/(nir + red)'\n</code></pre>"},{"location":"getting_started/gee/#a-global-parameters","title":"a) Global Parameters","text":"<p>The <code>global_scope</code> section contains general settings applicable to the entire data extraction process.</p> <ul> <li> <p><code>time_range</code>: Specifies the period for which the satellite data should be fetched.</p> </li> <li> <p><code>variables</code>: Lists the key variables to be extracted. These are typically satellite bands or derived products such as vegetation indices, or any other relevant parameters like tempurature etc.,</p> </li> <li> <p><code>resolution</code>: Defines the spatial resolution (in meters) for the output data.</p> </li> </ul>"},{"location":"getting_started/gee/#b-earth-engine-collections","title":"b) Earth Engine Collections","text":"<p>Each Earth Engine dataset is defined under its own subsection within collections. The key components are collection, select, and variables. Multiple datasets can be specified, and if data from multiple collections are present for the same day, the module will return the mean of the variables across all collections.</p> <ul> <li> <p><code>collection</code>: Refers to the specific Google Earth Engine dataset identifier.</p> </li> <li> <p><code>select</code>: Defines a masking or selection condition for the data extraction.</p> </li> <li> <p><code>variables</code>: Specifies the bands or parameters to extract, along with any scaling factors or transformations.</p> </li> </ul>"},{"location":"getting_started/gee/#c-derived-variables","title":"c) Derived Variables","text":"<p>Derived variables are calculated from the raw bands using mathematical expressions and functions available in numpy package.</p>"},{"location":"getting_started/gee/#2-fetching-the-data","title":"2. Fetching the Data","text":"<p>You can use the following command-line interface to fetch time-series of required variables from Google Earth Engine into a CSV file:</p> <pre><code>geo_epic gee &lt;config-file&gt; --fetch &lt;roi&gt; --out &lt;output-path&gt;\n</code></pre> <p>The region of interest for fetching data can be provided in three formats:</p> <ul> <li>Latitude and Longitude: Use direct coordinates [latitude, longitude] <pre><code>geo_epic gee ./landsat_ndvi.yml --fetch 40.5677 98.5505 --out ./out/sample.csv\n</code></pre></li> <li> <p>Shapefile (.shp): A shapefile containing the polygons of interest. It must contain a SiteID or FieldID column. The data will be stored in the name of FieldID or SiteID. Fetches one file for each polygon. <pre><code>geo_epic gee ./landsat_ndvi.yml --fetch ./input/region.shp --out ./out\n</code></pre></p> </li> <li> <p>CSV File: A CSV file must contain a SiteID or FieldID column. Additionally, if using a CSV, it must include lat, lon columns. The data will be stored in the name of FieldID or SiteID. <pre><code>geo_epic gee ./landsat_ndvi.yml --fetch ./input/region.csv --out ./out\n</code></pre></p> </li> </ul>"},{"location":"getting_started/installation/","title":"GeoEPIC Toolkit Installation Guide","text":"<p>This guide provides instructions for installing the GeoEPIC Toolkit.</p>"},{"location":"getting_started/installation/#1-prerequisites","title":"1. Prerequisites","text":"<p>Before you begin, ensure you have Conda installed. You can find installation guides here: Conda Installation.</p>"},{"location":"getting_started/installation/#2-automatic-installation-recommended","title":"2. Automatic Installation (Recommended)","text":"<p>This method automates the installation process using a script.</p> <ol> <li> <p>Download the setup script:</p> <ul> <li>Download the <code>epic_setup.bat</code> script to your local machine.</li> </ul> </li> <li> <p>Run the setup script:</p> <ul> <li>Open your command prompt or PowerShell.</li> <li>Navigate to the directory where you saved the <code>epic_setup.bat</code> script.</li> <li>Execute the script using the following command:     <pre><code>call epic_setup.bat\n</code></pre></li> <li>What to expect: The script will automatically create a Conda environment, install the GeoEPIC Toolkit, and configure necessary dependencies. You should see the progress of these actions in the command prompt.</li> </ul> </li> </ol>"},{"location":"getting_started/installation/#3-manual-installation-alternative","title":"3. Manual Installation (Alternative)","text":"<p>This method allows for more control over the installation process. Choose either this method or the automatic installation method (section 2), not both.</p> <ol> <li> <p>Create a virtual environment in Conda:</p> <ul> <li>Open your command prompt or terminal.</li> <li>Run the following command to create a new Conda environment named <code>epic_env</code> with Python 3.11:     <pre><code>conda create --name epic_env python=3.11\n</code></pre></li> <li>What to expect: Conda will create the environment and install Python 3.11.</li> </ul> </li> <li> <p>Activate the environment:</p> <ul> <li>After the environment is created, activate it using:     <pre><code>conda activate epic_env\n</code></pre></li> <li>What to expect: Your command prompt or terminal will show <code>(epic_env)</code> at the beginning of the line, indicating the environment is active.</li> </ul> </li> <li> <p>Install the GeoEPIC Toolkit:</p> <ul> <li> <p>Option 1: Install Directly from GitHub (Recommended for general users):</p> <ul> <li>Run the following command:     <pre><code>pip install git+[https://github.com/smarsGroup/geo_epic_win.git](https://github.com/smarsGroup/geo_epic_win.git)\n</code></pre></li> <li>What to expect: <code>pip</code> will download and install the latest version of the GeoEPIC Toolkit from the GitHub repository.</li> </ul> </li> <li> <p>Option 2: Install Locally (for developers):</p> <ul> <li>Clone the repository:     <pre><code>git clone [https://github.com/smarsGroup/geo_epic_win.git](https://github.com/smarsGroup/geo_epic_win.git)\n</code></pre></li> <li>Navigate to the cloned directory:     <pre><code>cd geo_epic_win\n</code></pre></li> <li>Install the toolkit:     <pre><code>pip install .\n</code></pre></li> <li>What to expect: The repository will be downloaded, and the toolkit will be installed from your local files, allowing you to make and test changes.</li> </ul> </li> </ul> </li> </ol>"},{"location":"getting_started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<ol> <li>Activate the environment (if not already active): <pre><code>conda activate epic_env\n</code></pre></li> <li>Initialize GeoEPIC: <pre><code>geo_epic init\n</code></pre><ul> <li>What to expect: If the installation was successful, the <code>geo_epic init</code> command will run without errors. If you see an error, recheck your installation steps.</li> </ul> </li> </ol>"},{"location":"getting_started/installation/#5-additional-notes","title":"5. Additional Notes","text":"<ul> <li> <p>Updating GeoEPIC Toolkit:</p> <ul> <li>To update to the latest version, use:     <pre><code>pip install --upgrade git+[https://github.com/smarsGroup/geo_epic_win.git](https://github.com/smarsGroup/geo_epic_win.git)\n</code></pre></li> </ul> </li> <li> <p>Uninstalling GeoEPIC Toolkit:</p> <ul> <li>To uninstall, use:     <pre><code>pip uninstall geo_epic_win\n</code></pre></li> </ul> </li> <li> <p>Troubleshooting:</p> <ul> <li>If you encounter issues, refer to the GeoEPIC Toolkit documentation or seek help from the community.</li> </ul> </li> </ul>"},{"location":"getting_started/installation/#6-example-usage","title":"6. Example Usage","text":"<p>After successful installation, you can begin using the GeoEPIC Toolkit.</p> <ol> <li> <p>Initialize a new GeoEPIC project: <pre><code>geo_epic init my_project\n</code></pre></p> <ul> <li>What to expect: A new directory named <code>my_project</code> will be created with the necessary project files.</li> </ul> </li> <li> <p>Navigate to your project directory: <pre><code>cd my_project\n</code></pre></p> </li> <li> <p>Run a sample analysis: <pre><code>geo_epic analyze sample_data.csv\n</code></pre></p> <ul> <li>What to expect: The <code>geo_epic analyze</code> command will run an analysis on the <code>sample_data.csv</code> file within your project directory. Ensure that you have a file called sample_data.csv in your my_project folder, or replace sample_data.csv with the correct filename.</li> </ul> </li> </ol>"},{"location":"getting_started/license/","title":"BSD 3-Clause License","text":"<p>Copyright (c) 2024, smarsGroup</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this    list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,    this list of conditions and the following disclaimer in the documentation    and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"getting_started/opc/","title":"Operation Control (OPC) Module","text":"<p>The crop management file (<code>.OPC</code>) in the EPIC model contains essential information about various management practices carried out on an agricultural field. Each row in the management file represents a specific management operation. These operations include planting a specific crop, fertilizer application, irrigation methods and timing, harvesting, and tillage details. The interpretation of entries in certain columns changes depending on the operation being simulated. This detailed setup enables the EPIC model to accurately simulate the effects of management decisions on crop yield and environmental impact. For more information on the structure of management files, refer to the EPIC user manual.</p> <p></p>"},{"location":"getting_started/opc/#1-creating-opc-file","title":"1. Creating OPC File","text":"<p>This section describes how to generate EPIC-ready operation control files (<code>.OPC</code>) using GeoEPIC tools.</p>"},{"location":"getting_started/opc/#11-requirements-and-configuration","title":"1.1 Requirements and Configuration","text":"<p>To generate an <code>.OPC</code> file using the GeoEPIC command-line utility, you need the following inputs:</p> <ul> <li> <p>Crop Data CSV File:</p> <ul> <li>A comma-separated values file (<code>.csv</code>) containing the crop rotation information for the simulation period.</li> <li>Required Columns: <code>year</code>, <code>crop_code</code>, <code>planting_date</code>, <code>harvest_date</code>.</li> <li>Format Example: <pre><code>year,crop_code,planting_date,harvest_date\n2006,2,2006-05-08,2006-12-05\n2007,1,2007-03-18,2007-12-18\n2008,2,2008-03-31,2008-12-05\n2009,1,2009-04-09,2009-10-25\n2010,2,2010-05-23,2010-12-03\n</code></pre></li> </ul> </li> <li> <p>Templates Folder:</p> <ul> <li>A folder containing template operation schedules for different crops.</li> <li>Contents:<ul> <li>Crop Template Files: Individual <code>.OPC</code> files named according to the <code>template_code</code> used in the <code>MAPPING</code> file (e.g., <code>CORN.OPC</code>, <code>SOYB.OPC</code>). Each file details the standard sequence of operations for that crop, typically relative to a nominal planting date.<ul> <li>Example (<code>CORN.OPC</code> snippet): <pre><code> 3  0\n 1  4 22   30    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n 1  4 23   33    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n 1  4 24   71    0    2   52 160.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n 1  4 25    2    0    2    01700.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n 1  9 25  650    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n 1  9 26  740    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n 1  9 27   41    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n</code></pre></li> </ul> </li> <li>Mapping File: A file named <code>MAPPING</code> (case-sensitive, no extension) that links the <code>crop_code</code> from your CSV file to the corresponding <code>template_code</code> (which matches the template <code>.OPC</code> filename without the extension).<ul> <li>Required Columns: <code>crop_code</code>, <code>template_code</code> (comma-separated).</li> <li>Format Example (<code>MAPPING</code> file): <pre><code>crop_code,template_code\n1,SOYB\n2,CORN\n3,GRSG\n4,COTS\n18,RICE\n</code></pre></li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Output Path: A specified location and filename for the generated <code>.OPC</code> file.</p> </li> </ul> <p>Note: If you create a GeoEPIC workspace using its tools, a sample templates folder (<code>opc/template</code>) is often included, which you can adapt.</p>"},{"location":"getting_started/opc/#12-using-command-line-cli","title":"1.2 Using Command Line (CLI)","text":"<p>The <code>geo_epic generate_opc</code> command assembles the final <code>.OPC</code> file based on your crop rotation CSV and the templates.</p> <ul> <li> <p>Command Syntax: <pre><code>geo_epic generate_opc -c /path/to/crop_data.csv -t /path/to/templates_folder -o /path/to/output_filename.OPC\n</code></pre></p> </li> <li> <p>Arguments:</p> <ul> <li><code>-c</code> / <code>--cdl</code>: Path to the input crop data CSV file (as described in Sec 1.1).</li> <li><code>-t</code> / <code>--template</code>: Path to the folder containing the crop template <code>.OPC</code> files and the <code>MAPPING</code> file (as described in Sec 1.1).</li> <li><code>-o</code> / <code>--output</code>: Path where the final generated crop management file (<code>.OPC</code>) will be saved.</li> </ul> </li> <li> <p>Date Handling: The command uses the <code>planting_date</code> and <code>harvest_date</code> from the input CSV (<code>-c</code>) file to adjust the relative dates/timing of operations defined within the corresponding crop template file found via the <code>MAPPING</code> file. If <code>planting_date</code> and <code>harvest_date</code> are not provided in the CSV for a given year/crop, the dates specified within the template <code>.OPC</code> file itself will be used directly without adjustment for that year's operations.</p> </li> </ul>"},{"location":"getting_started/opc/#2-editing-opc-file","title":"2. Editing OPC File","text":"<p>Once an <code>.OPC</code> file is generated or obtained, you may need to modify specific operations or parameters. The <code>geoEpic.io.OPC</code> class provides methods to load, edit, and save these files programmatically.</p> <p>Key Concepts:</p> <ul> <li>The <code>.OPC</code> file represents management operations row by row.</li> <li>The <code>geoEpic.io.OPC</code> class loads this file into an object that behaves like a <code>pandas.DataFrame</code>, allowing familiar data manipulation techniques alongside specialized methods.</li> <li>Operations like adding, updating, or removing specific management practices can be performed programmatically.</li> </ul> <p>Example Usage:</p> <p>The following code demonstrates loading an <code>.OPC</code> file, modifying an irrigation parameter, adding a fertilizer application, removing that same application, and saving the changes.</p> <pre><code>from geoEpic.io import OPC\n\n# Define file path\nopc_file_path = './path/to/your/umstead.OPC'\noutput_opc_path = './path/to/your/umstead_modified.OPC' # Save changes to a new file\n\ntry:\n    # Load the existing .OPC file\n    opc = OPC.load(opc_file_path)\n\n    # Modify a parameter: Select auto irrigation implement ID\n    original_iaui = opc.IAUI\n    opc.IAUI = 72\n    print(f\"Changed Auto Irrigation Implement ID (IAUI) from {original_iaui} to {opc.IAUI}\")\n\n    # Define a new fertilizer operation as a dictionary\n    fertilizer_op = {'opID': 71, 'cropID': 2, 'fertID': 52,\n                     'date': '2015-04-01', 'OPV1': 160}\n\n    # Add/Update the fertilizer operation using the update method\n    opc.update(fertilizer_op)\n    print(f\"Added/Updated fertilizer operation on {fertilizer_op['date']}\")\n    # Note: 'update' might replace an existing operation on the same date or add a new one,\n    # depending on its implementation. Check documentation for specific behavior.\n\n    # Remove the specific fertilizer operation just added\n    # Use parameters that uniquely identify the operation to remove\n    opc.remove(opID=71, date='2015-04-01')\n    print(f\"Removed operation with opID 71 on date 2015-04-01\")\n\n    # Save the OPC file with all accumulated changes\n    opc.save(output_opc_path)\n    print(f\"Saved modified OPC file to {output_opc_path}\")\n\nexcept FileNotFoundError:\n    print(f\"Error: Input OPC file not found at {opc_file_path}\")\nexcept Exception as e:\n    print(f\"An error occurred during OPC file editing: {e}\")\n</code></pre> <p>This example showcases key functionalities: loading an <code>.OPC</code> file, programmatically modifying general parameters (like <code>IAUI</code>), adding new operations (fertilizer application via <code>update</code>), removing specific operations (<code>remove</code>), and saving the modified file. Since the <code>OPC</code> class inherits from <code>pandas.DataFrame</code>, users can leverage familiar DataFrame operations in addition to these specialized methods.</p> <p>For a comprehensive list of available methods and their specific behaviors, consult the GeoEPIC documentation. The structure, valid operation codes (<code>opID</code>), and parameter meanings (<code>OPV1</code> etc.) for <code>.OPC</code> files are detailed in the official EPIC user manual (e.g., EPIC 1102).</p>"},{"location":"getting_started/overview/","title":"Overview","text":"<p>GeoEPIC is organized into multiple modules, each contributing to distinct stages of the EPIC model workflow. Below is an overview of the modules, grouped by their roles in the workflow:</p> <p> </p>"},{"location":"getting_started/overview/#1-input-generation-and-processing-modules","title":"1. Input Generation and Processing Modules","text":"<p>Input generation is critical for EPIC simulations, requiring accurate soil, weather, and management practice data. These modules automate the process by sourcing data, preprocessing it, and storing it in EPIC-compatible formats.</p>"},{"location":"getting_started/overview/#geoepicweather","title":"geoEpic.weather","text":"<p>\"This module handles weather data acquisition and preprocessing. It downloads weather information from various sources, including Daymet and Google Earth Engine. The data is processed and stored in EPIC-compatible formats, such as daily weather files (.DLY) and monthly weather files (.INP), to ensure seamless integration into simulations.</p> <ul> <li>API Documentation for weather module</li> <li>Usage Instructions for weather module</li> </ul>"},{"location":"getting_started/overview/#geoepicsoil","title":"geoEpic.soil","text":"<p>The soil module focuses on preparing soil properties data. It sources soil information from SSURGO and ISRIC, processes the data into appropriate units and formats, and generates the resulting file in the .SOL format required by the EPIC model. This ensures accurate and compatible soil inputs for simulations.</p> <ul> <li>API Documentation for soil module</li> <li>Usage Instructions for soil module</li> </ul>"},{"location":"getting_started/overview/#geoepicopc","title":"geoEpic.opc","text":"<p>This module processes crop management data into the .OPC format required by EPIC simulations. The .OPC file contains detailed information about management practices conducted at a specific site, such as planting crops, fertilizer applications, irrigation schedules, harvesting operations, and tillage practices. Users can programmatically manipulate these files through GeoEPIC's OPC class, which provides methods to add new operations, modify existing ones, and save changes in the appropriate format.</p> <ul> <li>Usage Instructions for opc module</li> </ul>"},{"location":"getting_started/overview/#2-input-and-output-file-management","title":"2. Input and Output File Management","text":"<p>Once input data is prepared, this module facilitates user interactions with both input and output files. It supports efficient data handling and visualization, helping users analyze simulation results effectively.</p>"},{"location":"getting_started/overview/#geoepicio","title":"geoEpic.io","text":"<p>The input/output module provides classes for managing various EPIC files, including input files (.DLY, .SOL, .OPC, .SIT) and output files (.ACY, .DGN, etc.). It serves multiple roles:</p> <ul> <li>It enables input generation modules and the model execution module to interact with and modify files seamlessly.</li> <li>It allows users to directly access and edit files for customization.</li> </ul> <p>Usage instructions for this module will be detailed in the dedicated input and output pages, ensuring clear guidance for working with specific file types.</p> <ul> <li>API Documentation for io module</li> </ul>"},{"location":"getting_started/overview/#3-model-execution-and-calibration","title":"3. Model Execution and Calibration","text":"<p>The geoEPIC.core module is central to executing the EPIC model, configuring simulations, and fine-tuning model parameters through calibration workflows. This module provides comprehensive tools for managing every aspect of model execution and calibration. The functionalities include:</p> <ul> <li> <p>Model Execution:   The module facilitates the execution of EPIC simulations, supporting both single-site and multi-site setups. It ensures seamless compatibility and execution, enabling precise simulations tailored to various scenarios and generating comprehensive outputs for further analysis.</p> </li> <li> <p>Model Configuration:   Users can set up and customize various parameters for simulations, such as start and end dates, simulation duration, and the types of outputs required. This flexibility allows users to adapt simulations to site-specific conditions or project goals.</p> </li> <li> <p>Calibration:   This module aligns the EPIC model's outputs with real-world observations (e.g., crop yields, biomass, LAI, NEE) by leveraging advanced optimization algorithms through the PyGMO library, such as Particle Swarm Optimization (PSO) and Differential Evolution (DE). Users can define routines and objectives to log errors, aggregate results, and guide the calibration process, ensuring flexibility and precision tailored to specific simulation needs.</p> </li> <li> <p>API Documentation for core module</p> </li> <li>Usage Guide for simulation</li> <li>Usage Guide for calibration</li> </ul>"},{"location":"getting_started/simulation/","title":"Simulation","text":"<p>The GeoEPIC package facilitates running simulations and examining outputs, complementing its input file setup capabilities.</p>"},{"location":"getting_started/simulation/#single-site-simulation","title":"Single Site Simulation","text":""},{"location":"getting_started/simulation/#quick-start-crop-yield-estimation","title":"Quick Start: Crop Yield Estimation","text":"<p>For locations in the conterminous USA, the <code>Site.fetch_usa</code> method provides a streamlined workflow that automatically retrieves soil (SSURGO), weather (Daymet), and terrain (DEM) data:</p> <pre><code>from geoEpic.core import Site, EPICModel\nfrom geoEpic.io import ACY, DGN\n\n# Location of interest (Mead, Nebraska, USA)\nlat, lon = 41.1686, -96.4736\n\n# Fetch required input data and create Site object\nsite = Site.fetch_usa(lat, lon, opc='./irrigated_corn.OPC')\n\n# Configure and run model\nmodel = EPICModel('./model/EPIC1102.exe')\nmodel.start_date = '2015-01-01'\nmodel.duration = 5  # in years\nmodel.run(site)\nmodel.close()\n\n# Extract outputs\nyields = ACY(site).get_var('YLDG')\nlai = DGN(site).get_var('LAI')\n</code></pre> <p>This example simulates corn growth for 5 years (2015-2019) near Mead, Nebraska. The <code>Site.fetch_usa</code> method handles all input file generation automatically.</p>"},{"location":"getting_started/simulation/#manual-site-setup","title":"Manual Site Setup","text":"<p>For more control over input files, follow these steps to run a simulation for an individual site:</p> <ol> <li> <p>Prepare the Site Object:     First, create a <code>Site</code> object. This object encapsulates necessary input file references: the crop management file (<code>OPC</code>), weather data file (<code>DLY</code>), soil file (<code>SOL</code>), and site file (<code>SIT</code>). The <code>Site``` object also keeps track of output files generated during the simulation, like the annual crop yield file (</code>ACY`).</p> <pre><code>from geoEpic.core import Site, EPICModel\nfrom geoEpic.io import ACY, DGN\n\nsite = Site(opc='./opc/files/umstead.OPC',\n            dly='./weather/NCRDU.DLY',\n            sol='./soil/files/umstead.SOL',\n            sit='./sites/umstead.SIT')\n</code></pre> </li> <li> <p>Configure and Execute the EPIC Model:     Next, initialize an <code>EPICModel</code> object, providing the path to the EPIC executable file. Configure it with required settings such as the simulation start date, duration (in years), and the types of output files needed (e.g., <code>ACY</code>, <code>DGN</code>). Execute the simulation by passing the <code>Site</code> object to the model's <code>run</code> method. Remember to close the model instance afterwards. The package documentation describes multiple ways to initialize and configure the <code>EPICModel</code>.</p> <pre><code>model = EPICModel('./model/EPIC1102.exe')\nmodel.start_date = '2015-01-01'\nmodel.duration = 5  # in years\nmodel.output_types = ['ACY', 'DGN']\nmodel.run(site)\nmodel.close()\n</code></pre> </li> <li> <p>Analyze Simulation Outputs:     Finally, examine the outputs. Use the class interfaces provided in the <code>io</code> module (like <code>ACY</code> and <code>DGN</code>) to easily read output files and extract data. These classes integrate with packages like <code>matplotlib</code> for generating figures.</p> <p><code>yields = ACY(site.outputs['ACY']).get_var('YLDG') lai = DGN(site.outputs['DGN']).get_var('LAI')</code> The first line above uses the <code>ACY</code> class to get yearly yield values (<code>YLDG</code>) from the <code>ACY</code> output file. The second line uses the <code>DGN</code> class to retrieve the simulated daily Leaf Area Index (<code>LAI</code>) from the <code>DGN</code> file.</p> </li> </ol>"},{"location":"getting_started/simulation/#regional-simulation","title":"Regional Simulation","text":"<p>The <code>Workspace</code> class streamlines managing and running EPIC simulations across many sites, ideal for regional studies. It's best used within a Jupyter notebook environment for interactive results exploration.</p> <ol> <li> <p>Initialize the Workspace:     Create a <code>Workspace</code> object using two main configuration elements:</p> <ul> <li>A configuration file (e.g., <code>config.yml</code>) specifying global settings (model path, dates, directories, etc.).</li> <li>A <code>sites_info</code> file (typically CSV) containing metadata and input file paths for each individual site. You can optionally clear previous logs and outputs before running.</li> </ul> <pre><code>from geoEpic.core import Workspace\n\nexp = Workspace('./config.yml')\n\n# Clear the logs and output directory (optional)\nexp.clear_logs()\nexp.clear_outputs()\n</code></pre> </li> <li> <p>Configure the Workspace (Example <code>config.yml</code>):     The configuration file defines the experiment settings. A template is usually provided.</p> <p><pre><code># Experiment Name\nEXPName: Kansas Yield Estimation\n\n# Model configuration\nEPICModel: ./model/EPIC1102.exe\nstart_date: '2014-01-01'\nduration: 6  # years\noutput_types:\n  - ACY  # Annual Crop data file\n  - DGN  # Daily general output file\nlog_dir: ./log\noutput_dir: ./output\n\n# Path to folders containing input files\nweather_dir: ./weather/Daily\nsoil_dir: ./soil/files\nsite_dir: ./sites\nopc_dir: ./opc/files\n\n# Path to csv file with sites' input files info\nsites_info: ./info.csv\n# Select specific sites from the info file\nselect: Random(0.1) # Selects 10% of sites randomly\n# Timeout for a simulation execution in seconds.\ntimeout: 10\n</code></pre> The <code>sites_info</code> file referenced here contains paths to site-specific inputs. The <code>select</code> option allows flexible site filtering (e.g., <code>Random(0.1)</code> for 10% random sampling, <code>Range(0, 1)</code> for all sites).</p> </li> <li> <p>Define Custom Post-Processing Routines (Optional):     You can attach custom functions to the <code>Workspace</code> object using the <code>@exp.routine</code> decorator. These functions execute automatically after each site's simulation completes, allowing for automated output processing, variable extraction, analysis, or custom saving.</p> <p><pre><code>import pandas as pd\nfrom geoEpic.core import Workspace\nfrom geoEpic.io import DGN, ACY\n\nexp = Workspace('./config.yml')\nexp.output_dir = None  # Set to None if you don't want standard EPIC outputs saved\n\n@exp.routine\ndef save_lai(site):\n    # This function runs after each site simulation\n    lai = DGN(site.outputs['DGN']).get_var('LAI')\n    lai.to_csv(f'./outputs/{site.site_id}.csv') # Save LAI to a site-specific CSV\n</code></pre> In this example, the <code>save_lai</code> function is registered as a routine. It reads LAI data from the <code>DGN</code> output and saves it to a CSV file named after the site ID. Setting <code>exp.output_dir = None</code> prevents the workspace from saving the raw EPIC output files (<code>ACY</code>, <code>DGN</code>, etc.), which is useful if you only need the results from your custom routine.</p> </li> <li> <p>Run Multi-Site Simulations:     Execute the simulations for all selected sites defined in the workspace configuration. If custom routines are defined, they will run after each site simulation.</p> <pre><code># Execute simulations (and routines, if defined)\nexp.run()\n</code></pre> <p>Additionally, the <code>Workspace</code> provides <code>@exp.logger</code> and <code>@exp.objective</code> decorators for more advanced logging and objective function evaluation, which are detailed further in the package documentation.</p> </li> </ol>"},{"location":"getting_started/site/","title":"Site Module","text":"<p>In agricultural simulations using models like EPIC, the Site file (<code>.SIT</code>) contains essential information about the physical characteristics of the specific location being simulated. This includes parameters critical for hydrological and erosion calculations, such as geographic coordinates, elevation, slope steepness and length, and drainage area.</p>"},{"location":"getting_started/site/#1-managing-site-files-sit","title":"1. Managing Site Files (.SIT)","text":"<p>GeoEPIC provides tools, primarily through its Python API, to manage <code>.SIT</code> files. This process typically involves loading an existing site file (which could be a generic template or a file from a previous simulation), modifying its parameters to match the desired location, and then saving the updated information.</p>"},{"location":"getting_started/site/#11-requirements-and-configuration","title":"1.1 Requirements and Configuration","text":"<ul> <li>Base Site File: You need a starting <code>.SIT</code> file to modify. This could be:<ul> <li>A default template provided with EPIC or GeoEPIC.</li> <li>A <code>.SIT</code> file from a similar location or previous run.</li> </ul> </li> <li>Site Parameters: You need the specific values for the site you want to simulate, such as:<ul> <li>Latitude (<code>YLAT</code>) and Longitude (<code>XLON</code>)</li> <li>Elevation (<code>ELEV</code>)</li> <li>Site Area (<code>AR</code>) - usually in hectares</li> <li>Average Slope (<code>SLP</code>) - often as a percentage or fraction</li> <li>Slope Length (<code>SL</code>) - in meters</li> <li>Other parameters as required by the specific EPIC simulation setup (e.g., weather station details, drainage type). Refer to the EPIC manual for a full list and description of <code>.SIT</code> parameters.</li> </ul> </li> </ul>"},{"location":"getting_started/site/#12-fetching-elevation-and-slope","title":"1.2 Fetching Elevation and Slope","text":"<p>The <code>DEM</code> class in the <code>geoEpic.spatial</code> module retrieves elevation and slope data from digital elevation models:</p> <pre><code>from geoEpic.spatial import DEM\nfrom geoEpic.io import SIT\n\n# Defaults to GLO-30, can specify source=\"ASTER\" or \"SRTM\"\nelevation, slope = DEM.fetch(lat=35.9768, lon=-90.1399)\n\n# Create new site file with fetched data\nnew_site = SIT({'elevation': elevation, 'slope_steep': slope, 'lat': 35.9768, 'lon': -90.1399})\nnew_site.save('new_site.SIT')\n</code></pre> <p>Supported DEM sources: - GLO-30 (default): Copernicus Global DEM at 30m resolution - SRTM: NASA Shuttle Radar Topography Mission - ASTER: Advanced Spaceborne Thermal Emission and Reflection Radiometer</p>"},{"location":"getting_started/site/#13-using-python-api","title":"1.3 Using Python API","text":"<p>The <code>SIT</code> class within the <code>geoEpic.io</code> module allows for programmatic loading, modification, and saving of <code>.SIT</code> files.</p> <p>Key Components:</p> <ul> <li><code>SIT.load(filepath)</code>: Reads an existing <code>.SIT</code> file into a structured <code>SIT</code> object.</li> <li>Direct Attribute Access: Many common site parameters (like area, slope, coordinates, elevation) can be accessed and modified directly as attributes of the loaded <code>SIT</code> object (e.g., <code>sit_object.area</code>, <code>sit_object.slope</code>).</li> <li><code>SIT.entries</code>: Provides access to the underlying data structure (often a list of lists representing lines and values) for modifying parameters that don't have direct attribute access. This allows modification of any parameter in the file while maintaining the required format.</li> <li><code>SIT.save(filepath)</code>: Saves the current state of the <code>SIT</code> object (with any modifications) to a specified <code>.SIT</code> file, ensuring the correct EPIC format.</li> </ul> <p>Usage Example:</p> <p>The following example demonstrates loading and modifying a <code>.SIT</code> file:</p> <pre><code>from geoEpic.io import SIT\n\n# Load and modify existing file\nsit_file = SIT.load('./site1.SIT')\nsit_file.elevation = 335.0\nsit_file.slope = 0.02\nsit_file.save('./site1_modified.SIT')\n</code></pre> <p>This example illustrates how the <code>SIT</code> class simplifies managing site characteristics through direct property accessors.</p>"},{"location":"getting_started/soil/","title":"Soil Module","text":"<p>In agriculture simulations, such as those conducted using the EPIC model, soil data directly influences water availability, nutrient supply, and overall crop growth predictions. Soil input files to the model contain detailed information about the soil properties of a specific location, depth wise.</p> <p></p>"},{"location":"getting_started/soil/#1-data-sources","title":"1. Data Sources","text":"<p>GeoEPIC helps in generating soil files required by the EPIC Model from two primary soil data sources: </p> <ul> <li>USDA SSURGO: Contains detailed surveys of U.S. soils. </li> <li>ISRIC SoilGrids 250m: Offers global coverage in a grid format (Support adding soon).</li> </ul>"},{"location":"getting_started/soil/#11-usda-ssurgo","title":"1.1 USDA SSURGO","text":"<p>The USDA Soil Survey Geographic (SSURGO) database is a comprehensive resource for soil data collected by the Natural Resources Conservation Service (NRCS) across the United States and the Territories. This database provides detailed information on soil properties and classifications. The data is collected through extensive field surveys and laboratory analysis. For more detailed information, visit the USDA NRCS SSURGO page.</p>"},{"location":"getting_started/soil/#2-creating-soil-file","title":"2. Creating Soil File","text":"<p>This section describes how to fetch soil data using GeoEPIC tools.</p>"},{"location":"getting_started/soil/#21-requirements-and-configuration","title":"2.1 Requirements and Configuration","text":"<p>Depending on the method chosen to generate soil files, different inputs are required:</p> <ul> <li>Direct Fetching from USDA SSURGO (Online):     You need to provide the location(s) of interest using one of the following formats:         *   Specific latitude and longitude coordinates.         *   A CSV file containing columns for latitude (<code>lat</code>) and longitude (<code>lon</code>).         *   A vector boundary file (e.g., ESRI Shapefile <code>.shp</code>) defining the area(s) of interest. GeoEPIC will determine the representative soil(s) within the boundaries.</li> <li>Processing Local SSURGO GDB File (Offline):<ul> <li>Requires a pre-downloaded state Soil Survey Geographic (gSSURGO) database file (<code>.gdb</code>).</li> <li>These files contain all the necessary soil properties and spatial data for a specific state or territory.</li> <li>Downloading the GDB: State-specific gSSURGO databases can be downloaded from the USDA NRCS Geospatial Data Gateway or the specific gSSURGO Database page. You will typically download a <code>.zip</code> file containing the <code>.gdb</code> folder. Ensure the <code>.gdb</code> folder is extracted and accessible to the GeoEPIC command.</li> </ul> </li> </ul> <p>Ensure that the necessary input files (CSV, Shapefile, or GDB) are correctly formatted and accessible in your specified file paths when running the GeoEPIC commands or Python scripts.</p>"},{"location":"getting_started/soil/#22-using-command-line-cli","title":"2.2 Using Command Line (CLI)","text":""},{"location":"getting_started/soil/#fetching-from-usda-ssurgo","title":"Fetching from USDA SSURGO","text":"<p>To fetch and output soil files using the USDA SSURGO database via the command line:</p> <ul> <li>For a specific location: Specify the latitude and longitude coordinates to generate a soil file named {mukey}.SOL.     <pre><code># Fetch and output soil files for a specific latitude and longitude\ngeo_epic soil usda --fetch {lat} {lon} --out {out_path}\n</code></pre></li> <li>For a list of locations: Provide a CSV file containing latitude and longitude columns.     <pre><code># Fetch for a list of locations in a csv file with lat, lon\ngeo_epic soil usda --fetch {list.csv} --out {out_dir}\n</code></pre></li> <li>For crop sequence boundaries: Use a shapefile defining the area of interest.     <pre><code># Fetch for crop sequence boundaries shape file.\ngeo_epic soil usda --fetch {aoi_csb.shp} --out {out_dir}\n</code></pre></li> </ul> <p>Note: When using a CSV file or a shapefile, this command will write the corresponding Soil IDs (mukeys) as an attribute into the input file.</p>"},{"location":"getting_started/soil/#processing-local-ssurgo-gdb-file","title":"Processing Local SSURGO GDB File","text":"<p>To process a downloaded SSURGO GDB file and generate soil files for all unique soils contained within it:</p> <ol> <li>Download the state-specific gSSURGO data (e.g., 'gSSURGO_MD.zip' for Maryland) from the gSSURGO Database page.</li> <li>Extract the contents and place the <code>.gdb</code> folder in your workspace.</li> <li> <p>Use the following command:</p> <pre><code>geo_epic soil process_gdb -i {path/to/ssurgo.gdb} -o {out_dir}\n</code></pre> </li> </ol>"},{"location":"getting_started/soil/#23-using-python-api","title":"2.3 Using Python API","text":"<p>The <code>geoEpic.spatial</code> module provides classes for fetching soil data programmatically:</p> <ul> <li><code>SSURGO.fetch(lat, lon)</code>: Fetches soil properties from USDA SSURGO for a given location.</li> <li><code>SoilGrids.fetch(lat, lon)</code>: Fetches soil properties from the global ISRIC SoilGrids database (250m resolution).</li> </ul> <pre><code>from geoEpic.spatial import SSURGO, SoilGrids\nfrom geoEpic.io import SOL\n\n# Fetch soil data from USDA SSURGO or SoilGrids\nsoil_ssurgo = SSURGO.fetch(lat=35.9768, lon=-90.1399)\nsoil_grids = SoilGrids.fetch(lat=35.9768, lon=-90.1399)\nsoil_grids.save('soilgrids_1.SOL')\n\n# Load and modify existing soil file\nsoil = SOL.load('./existing_soil.SOL')\nsoil.albedo = 0.15\nsoil.layers_df.loc[0, 'Bulk_Density'] = 1.35\nsoil.save('modified_soil.SOL')\n</code></pre> <p>Both <code>SSURGO.fetch</code> and <code>SoilGrids.fetch</code> return a <code>SOL</code> object that can be directly saved or modified.</p>"},{"location":"getting_started/soil/#3-editing-soil-file","title":"3. Editing Soil File","text":"<p>Soil data files (<code>.SOL</code>) contain vital information on soil properties across different layers, such as bulk density and moisture capacity. The <code>SOL</code> class in the <code>io</code> module of GeoEPIC includes methods for loading, modifying, and saving these soil files to allow customization of soil profiles.</p> <p>The <code>geoEpic.io.SOL</code> class provides the primary interface for editing <code>.SOL</code> files.</p> <p>Key Components:</p> <ul> <li><code>SOL.load(filepath)</code>: Reads a <code>.SOL</code> file into a structured <code>SOL</code> object.</li> <li><code>SOL.save(filepath)</code>: Saves the current state of the <code>SOL</code> object to a new <code>.SOL</code> file.</li> <li><code>SOL.layers_df</code>: A Pandas DataFrame attribute within the <code>SOL</code> object representing the soil layers. Each row corresponds to a layer, allowing direct manipulation of layer properties (e.g., <code>Bulk_Density</code>, <code>Field_Capacity</code>).</li> <li>Other attributes like <code>SOL.albedo</code> allow modification of general soil properties.</li> </ul> <p>Example Usage:</p> <p>The following example demonstrates loading a soil file, modifying its albedo and the bulk density of the first layer, and then saving the changes to a new file.</p> <pre><code>from geoEpic.io import SOL\n\n# Load the existing .SOL file\nsoil = SOL.load('./umstead.SOL')\n\n# Modify a general soil property (albedo)\nsoil.albedo = 0.15\n\n# Modify a property in the first layer (index 0) using the layers_df DataFrame\nsoil.layers_df.loc[0, 'Bulk_Density'] = 1.35\n\n# Save the modified soil profile to a new file\nsoil.save('umstead_new.SOL')\n</code></pre> <p>In this example, the <code>load()</code> method reads the <code>.SOL</code> file into the <code>soil</code> object. Properties like <code>albedo</code> can be accessed and modified directly. The soil layers are accessed via the <code>layers_df</code> Pandas DataFrame, providing a convenient way to target and manipulate individual layer properties using standard DataFrame operations (like <code>.loc</code>). Finally, the <code>save()</code> method writes the updated soil profile to <code>umstead_new.SOL</code>.</p> <p>The ability to edit individual soil attributes enables users to fine-tune soil profiles based on specific knowledge or to simulate how different soil conditions influence model outputs.</p>"},{"location":"getting_started/weather/","title":"Weather Module","text":"<p>Weather data is fundamental to agricultural simulation, providing environmental inputs crucial for modeling crop growth and yield. Reliable weather data ensures realistic simulation outcomes. The EPIC model specifically requires weather data formatted into two key input files:</p> <ol> <li>Daily Weather Files (<code>.DLY</code>): Contain day-to-day meteorological data (solar radiation, temperature max/min, precipitation, relative humidity, wind speed). These drive the daily simulation processes.</li> <li>Monthly Weather Files (<code>.WP1</code>): Summarize climate statistics, typically monthly averages or totals derived from daily data.</li> </ol> <p></p>"},{"location":"getting_started/weather/#1-data-sources","title":"1. Data Sources","text":"<p>Weather files can be created from various sources. GeoEPIC provides support for google earth engine sources.</p> <ul> <li>Google Earth Engine (GEE): GEE is a cloud-based platform for planetary-scale geospatial analysis. It hosts a vast public data archive that includes numerous satellite imagery and climate datasets (like AgERA5, GRIDMET, CHIRPS, etc.), alongside capabilities to use your own private GEE assets. GeoEPIC leverages GEE to fetch data from these collections, allowing access to global or regional weather data often derived from satellite observations or climate reanalysis models.<ul> <li>Reference: Google Earth Engine dataset catalog</li> <li>Reference: GEE Community Catalog</li> </ul> </li> </ul>"},{"location":"getting_started/weather/#2-creating-weather-file","title":"2. Creating Weather File","text":"<p>This section outlines the process for generating EPIC-compatible weather files (.DLY) using GeoEPIC.</p>"},{"location":"getting_started/weather/#21-requirements-and-configuration","title":"2.1 Requirements and Configuration","text":"<p>When sourcing weather data from Google Earth Engine (GEE) collections (like AgERA5, GRIDMET, CHIRPS, or your private assets), a configuration file (typically <code>config.yml</code>) is essential. This file directs GeoEPIC on what data to retrieve, defining the time period, specific variables, target resolution, the GEE collection path, and how the source data bands should be mapped and potentially converted (e.g., units) to match EPIC's requirements.</p> <p>Example Configuration File (<code>config.yml</code>) using AgERA5:</p> <pre><code># Global parameters\nglobal_scope:\n# Define the time period for data fetching\ntime_range: ['2002-01-01', '2022-12-31']\n# List of standard EPIC weather variable names\nvariables: ['srad', 'tmax', 'tmin', 'prcp', 'rh', 'ws']\n# Target resolution in meters (AgERA5 native is ~9km or 9600m)\nresolution: 9600\n\n# Specify Earth Engine (EE) collections and map their bands to EPIC variables\ncollections:\nAgEra5:\n# Path to the EE ImageCollection Asset\ncollection: 'projects/climate-engine-pro/assets/ce-ag-era5/daily'\n# Define how EE bands map to EPIC variables\n# b() refers to the image band\n# Note: Unit conversions can be applied directly (e.g., Kelvin to Celsius)\nvariables:\nsrad: b('Solar_Radiation_Flux') # MJ/m^2/day (assuming source is daily total flux)\ntmax: b('Temperature_Air_2m_Max_24h') - 273.15 # Convert K to \u00b0C\ntmin: b('Temperature_Air_2m_Min_24h') - 273.15 # Convert K to \u00b0C\nprcp: b('Precipitation_Flux') # mm/day (assuming source is daily total)\nrh: b('Relative_Humidity_2m_06h') # % (using 6 AM value as representative)\nws: b('Wind_Speed_10m_Mean') # m/s\n</code></pre>"},{"location":"getting_started/weather/#22-command-line-cli","title":"2.2 Command Line (CLI)","text":"<p>Use the <code>geo_epic weather</code> command with your configuration file and specify the location(s).</p> <p>Fetch for a Single Location (Latitude, Longitude):</p> <pre><code># Provide lat, lon coordinates directly\ngeo_epic weather config.yml --fetch 40.71 -74.00 --out ./output/NewYorkCity.DLY\n</code></pre> <p>Fetch for Multiple Locations from a CSV File:</p> <pre><code># Specify input CSV and the column containing output file paths\n# CSV must contain 'lat', 'lon', and the output path column (e.g., 'OutputFileColumn')\n# GeoEPIC adds a 'wthgridid' column to the CSV\ngeo_epic weather config.yml --fetch locations.csv --out OutputFileColumn\n</code></pre> <p>Fetch for Regions from a Shapefile:</p> <pre><code># Specify input Shapefile and the output directory for .DLY files\n# GeoEPIC adds a 'wthgridid' attribute to the shapefile\ngeo_epic weather config.yml --fetch ./boundaries/fields.shp --out ./weather_output/\n</code></pre>"},{"location":"getting_started/weather/#23-python-api","title":"2.3 Python API","text":""},{"location":"getting_started/weather/#direct-fetching-with-geoepicspatial","title":"Direct Fetching with <code>geoEpic.spatial</code>","text":"<p>For simple single-location fetching, use the <code>Daymet</code> and <code>AgEra5</code> classes directly:</p> <pre><code>from geoEpic.spatial import Daymet, AgEra5\nfrom geoEpic.io import DLY\n\n# Fetch weather data from Daymet or AgERA5\ndly_daymet = Daymet.fetch(lat=35.9768, lon=-90.1399)\nprint(dly_daymet['srad'].head())\n\ndly_era5 = AgEra5.fetch(lat=35.9768, lon=-90.1399)\ndly_era5.save('era5_weather.DLY')\n\n# Load and manipulate existing weather file\ndly = DLY.load('./existing_weather.DLY')\ndly.to_monthly()  # Saves as WP1 format\n</code></pre>"},{"location":"getting_started/weather/#3-editing-weather-file","title":"3. Editing Weather File","text":"<p>Once <code>.DLY</code> files are created, the <code>geoEpic.io.DLY</code> class allows loading, manipulation, and further processing. It acts like a <code>pandas.DataFrame</code> with added EPIC-specific features.</p> <pre><code>from geoEpic.io import DLY\n\n# Load and manipulate existing weather file\ndly = DLY.load('./weather1.DLY')\nprint(dly['srad'].head())\n\n# Generate monthly weather file (.WP1)\ndly.to_monthly()\n</code></pre> <p>Key <code>DLY</code> Class Features:</p> <ul> <li>Loading: <code>DLY.load(filepath)</code> reads a <code>.DLY</code> file into a DLY object (DataFrame subclass).</li> <li>Data Access: Utilize standard <code>pandas.DataFrame</code> methods for analysis and modification.</li> <li>Monthly Aggregation: <code>dly.to_monthly()</code> calculates monthly statistics and saves as <code>.WP1</code> file.</li> </ul>"},{"location":"pages/","title":"Index","text":""},{"location":"pages/#python-package-for-geospatial-crop-simulations","title":"Python package for geospatial Crop Simulations","text":"<p>GeoEpic is an open source package that expands the capabilities of the EPIC crop simulation model, to simulate crop growth and development across large geographies, such as entire states or counties by leveraging openly availabe remote sensing products and geospatial databases. Additionally, the package features a unique calibration module that allows fine-tuning of model parameters to reflect specific local conditions or experimental results. This toolkit allows researchers to assess crop production potential, management scenarios and risks at broader scales, informing decision-making for sustainable agricultural practices.</p>"},{"location":"pages/#what-can-you-do-with-geoepic","title":"What can you do with GeoEPIC?Getting Input FilesSite SimulationModel CalibrationRemote Sensing","text":"<p>Access and download necessary input files for your simulations from various geospatial databases.</p> Learn More <p>Run simulations to model crop growth and development across different geographies.</p> Learn More <p>Calibrate model parameters to match local conditions or experimental data for more accurate simulations.</p> Learn More <p>Utilize remote sensing data to enhance the accuracy and scope of your crop simulations.</p> Learn More"},{"location":"pages/#contributors","title":"Contributors","text":"<ul> <li>Bharath Irigireddy, Varaprasad Bandaru, Sachin Velmurgan, SMaRS Group</li> <li>Contact: prasad.bandaru@usda.gov</li> </ul>"},{"location":"pages/Contributing/","title":"Contributing to GeoEPIC","text":"<p>This document outlines how to contribute to GeoEPIC through code changes, bug reports, feature requests, and other helpful contributions.</p>"},{"location":"pages/Contributing/#reporting-bugs","title":"Reporting Bugs","text":"<ul> <li>Please review the Issues page to check if a similar problem has already been reported.</li> <li>If you do not find an existing report, create a new issue with the following information:<ul> <li>A clear title that summarizes the problem</li> <li>A detailed description of the bug, outlining what went wrong</li> <li>Steps to reproduce the bug.</li> </ul> </li> </ul>"},{"location":"pages/Contributing/#feature-requests","title":"Feature Requests","text":"<ul> <li>To propose a new feature, submit your request via the Issues page.</li> <li>Make sure to include:<ul> <li>A comprehensive description of the feature you are suggesting</li> <li>The expected benefits of this feature will enhance the project</li> </ul> </li> </ul>"},{"location":"pages/Contributing/#code-contributions","title":"Code Contributions","text":"<ul> <li>Contact the maintainers directly for any contributions or potential collaborations.</li> </ul>"},{"location":"pages/Contributing/#questions-support","title":"Questions &amp; Support","text":"<ul> <li>Open an issue for general questions</li> <li>Or you can reach out to the maintainers directly</li> </ul> <p>Thank you for your interest in contributing to GeoEPIC!</p>"},{"location":"pages/LICENSE/","title":"BSD 3-Clause License","text":"<p>Copyright (c) 2024, smarsGroup</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this    list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,    this list of conditions and the following disclaimer in the documentation    and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"pages/OPC/","title":"OPC","text":"<p>The crop management file in the EPIC model contains essential information about various management practices carried out on an agricultural field. Each row in the management file (.OPC) represents a specific management operation. These operations include planting a specific crop,  fertilizer application, irrigation methods and timing, harvesting and tillage details. The interpretation of entries in certain columns changes depending on the operation being simulated. This detailed setup enables the EPIC model to accurately simulate the effects of management decisions on crop yield and environmental impact. For more information on the structure of management files, refer to the EPIC user manual.</p> <p></p>"},{"location":"pages/OPC/#1-generating-crop-management-file","title":"1. Generating Crop Management File","text":"<p>To generate an OPC file, the following command line utility can be used:</p> <pre><code>geo_epic generate_opc -c /path/to/cdl.csv -t /path/to/templates_folder -o /path/to/output.OPC\n</code></pre> <ul> <li> <p><code>-c</code>: Path to the crop data CSV file. This file should contain crop information for each year with the following columns (comma-separated):</p> <pre><code>year, crop_code, planting_date, harvest_date\n2006, 2, 2006-05-08, 2006-12-05\n2007, 1, 2007-03-18, 2007-12-18\n2008, 2, 2008-03-31, 2008-12-05\n2009, 1, 2009-04-09, 2009-10-25\n2010, 2, 2010-05-23, 2010-12-03\n</code></pre> </li> <li> <p><code>-t</code>: Path to the folder which contains operations template for each crop. This folder should contain:</p> <ul> <li> <p>{Crop}.OPC files for each required crop, detailing common management practices specific to that crop.</p> <pre><code>    Corn\n    3  0\n    1  4 22   30    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n    1  4 23   33    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n    1  4 24   71    0    2   52 160.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n    1  4 25    2    0    2    01700.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n    1  9 25  650    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n    1  9 26  740    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n    1  9 27   41    0    2    0   0.000    0.00    0.00   0.000    0.00    0.00    0.00    0.00\n</code></pre> </li> <li> <p>A MAPPING file that provides mapping from crop_code to template_name for each crop.</p> <pre><code>    crop_code,template_code\n    1,SOYB\n    2,CORN\n    3,GRSG\n    4,COTS\n    18,RICE\n</code></pre> </li> <li> <p>The management file is generated by adjusting all operations relative to the specified planting and harvest dates in the cdl.csv. If plating date and harvest are not provided, The output files will contain the dates used in the crop template file.</p> </li> </ul> <p>Note: If you create a workspace, you can see a sample template folder under the opc directory.</p> </li> <li> <p><code>-o</code>: Path where the output crop management file will be saved.</p> </li> </ul>"},{"location":"pages/Soil/","title":"Soil Module","text":"<p>In agriculture simulations, such as those conducted using the EPIC model, soil data directly influences water availability, nutrient supply, and overall crop growth predictions. Soil input files to the model contain detailed information about the soil properties of a specific location, depth wise. GeoEPIC helps in generating soil files required by the EPIC Model from two soil data sources: </p> <ul> <li>USDA SSURGO: which contains detailed surveys of U.S. soils. </li> <li>ISRIC SoilGrids 250m (adding soon): which offers global coverage in a grid format.</li> </ul> <p></p>"},{"location":"pages/Soil/#usage","title":"Usage","text":""},{"location":"pages/Soil/#usda-ssurgo","title":"USDA SSURGO","text":"<p>The USDA Soil Survey Geographic (SSURGO) database is a comprehensive resource for soil data collected by the Natural Resources Conservation Service (NRCS) across the United States and the Territories. This database provides detailed information on soil properties and classifications. The data is collected through extensive field surveys and laboratory analysis. For more detailed information, visit the USDA NRCS SSURGO page.</p> <p>To fetch and output soil files using the USDA SSURGO database, following commands could be used. For a specific location, specify the latitude and longitude coordinates to generate a soil file named {mukey}.SOL. </p> <p><pre><code># Fetch and output soil files for a specific latitude and longitude\ngeo_epic soil usda --fetch {lat} {lon} --out {out_path}\n</code></pre> <pre><code># Fetch for a list of locations in a csv file with lat, lon\ngeo_epic soil usda --fetch {list.csv} --out {out_dir}\n</code></pre> <pre><code># Fetch for crop sequence boundaries shape file.\ngeo_epic soil usda --fetch {aoi_csb.shp} --out {out_dir}\n</code></pre></p> <p>Note: This command will write Soil IDs (mukeys) corresponding to each location as an attribute into the input file, when used with a CSV file or crop sequence boundary shapefile.</p> <p>Processing ssurgo gdb file:</p> <p>To process a SSURGO GDB file and generate soil files for all unique soils contained in it, follow these steps. For instance, if you require soil files for Maryland, navigate to the 'State Database - Soils' section, and download the 'gSSURGO_MD.zip' file. Once the download is complete, extract the contents and place the GDB file in the 'soil' folder within your workspace. Use the following command to generate the soil files for all mukeys. </p> <p>Link: https://www.nrcs.usda.gov/resources/data-and-reports/gridded-soil-survey-geographic-gssurgo-database</p> <pre><code>geo_epic soil process_gdb -i {path/to/ssurgo.gdb} -o {out_dir} \n</code></pre>"},{"location":"pages/Weather/","title":"Weather Module","text":"<p>Weather data is vital in providing essential environmental inputs that significantly influence crop growth and development. Reliable weather inputs ensure that agricultural simulations reflect realistic responses to climatic conditions. The EPIC model requires weather input files that detail daily and monthly climatic variables. Daily files provide day-to-day weather data, while monthly files summarize the average or total values per month. These files are crucial for driving the daily simulation processes in EPIC.</p>"},{"location":"pages/Weather/#fetching-weather-data","title":"Fetching Weather Data","text":"<p>GeoEPIC streamlines the process of fetching and organizing weather data essential for EPIC simulations. It supports the creation of weather input files from daymet, and also from sophisticated datasets available through Google Earth Engine. One can define a composite collection in the config file, detailing the specific variables to select from each data source, tailoring the dataset to meet the needs of their simulations. </p>"},{"location":"pages/Weather/#using-gee-collections","title":"Using GEE collections","text":"<p>GeoEPIC allows the integration of various weather and climate data sources on GEE. To explore the available datasets, visit Google Earth Engine's dataset catalog and GEE Community Catalog. Private assets can also be uploaded to Earth Engine, to use them in combination with existing datasets. Below is an example of configuration file that can be used to create weather input files.</p> <p>Example config files:</p> <ul> <li>using AgERA5</li> </ul> <pre><code># Global parameters\nglobal_scope:\n  time_range: ['2002-01-01', '2022-12-31']\n  variables: ['srad', 'tmax', 'tmin', 'prcp', 'rh', 'ws']\n\n\n# Specify Earth Engine (EE) collections and their respective variables\ncollections:\n  AgEra5:\n    collection: 'projects/climate-engine-pro/assets/ce-ag-era5/daily'\n    variables:\n      srad: b('Solar_Radiation_Flux') \n      tmax: b('Temperature_Air_2m_Max_24h') - 273.15\n      tmin: b('Temperature_Air_2m_Min_24h') - 273.15\n      prcp: b('Precipitation_Flux') \n      rh: b('Relative_Humidity_2m_06h')\n      ws: b('Wind_Speed_10m_Mean')\n</code></pre> <p>Using the config file to get data:</p> <p><pre><code># Fetch and output weather input files for a specific latitude and longitude\ngeo_epic weather config.yml --fetch {lat} {lon} --out {out_path}\n</code></pre> <pre><code># Fetch for a list of locations in a csv file with lat, lon, out_path columns\ngeo_epic weather config.yml --fetch {list.csv} --out {column_name}\n</code></pre> <pre><code># Fetch for crop sequence boundaries shape file.\ngeo_epic weather config.yml --fetch {aoi_csb.shp} --out {out_dir}\n</code></pre></p> <p>Note: This command will write weather grid IDs corresponding to each location as an attribute into the input file, when used with a CSV file or crop sequence boundary shapefile.</p>"},{"location":"pages/Weather/#from-daymet-and-nldas","title":"From Daymet and NLDAS","text":"<p>To fetch weather data for a specific latitude and longitude using the Daymet dataset, start by downloading the windspeed data from the NLDAS dataset and saving it to the specified directory. Afterward, fetch the other required weather data from the Daymet dataset.</p> <p><pre><code># To download windspeed data, use the following command\ngeo_epic weather windspeed -bbox &lt;bounding_box&gt; -start &lt;start_date&gt; -end &lt;end_date&gt; -out './weather'\n</code></pre> <pre><code># To fetch and output weather input files for a specific latitude and longitude\ngeo_epic weather daymet --fetch {lat} {lon} -start &lt;start_date&gt; -end &lt;end_date&gt; --out './weather'\n</code></pre></p>"},{"location":"pages/csb_old/","title":"Csb old","text":"<p>For effective simulations using the EPIC model, it is essential to have land units that are characterized by uniform soil properties and consistent management practices. One way to achieve this for the USA is by utilizing the Crop Sequence Boundaries (CSB) dataset. The CSB dataset is derived by analyzing annual variations in the USDA's Cropland Data Layer (CDL). The CSB method involves identifying and demarcating the boundaries of agricultural fields that have maintained consistent crop rotation sequence over several years. </p> <p></p> <p>Crop Sequence Boundaries (CSB): Delineated Fields Using Remotely Sensed Crop Rotations.  USDA-NASS &amp; Global Conservation Institute. Hunt, K. A., Abernethy, J., Beeson, P., Bowman, M., Wallander, S., &amp; Williams, R.</p>"},{"location":"pages/csb_old/#obtaining-the-csb-dataset","title":"Obtaining the CSB dataset","text":"<p>Visit the following URL: https://www.nass.usda.gov/Research_and_Science/Crop-Sequence-Boundaries/index.php to download the 'Crop Sequence Boundaries 2016-2023' dataset. After downloading, extract the dataset, which includes crop-field shapes for the entire USA. You can then use crop_csb tool to clip it to fit your Area of Interest (AOI). If your study focuses on Maryland, clip the shapefile for Maryland and save it.</p>"},{"location":"pages/csb_old/#clipping-the-csb-file-to-your-area-of-interest","title":"Clipping the CSB file to your Area of Interest","text":"<p>GeoEPIC comes with a command-line tool to filter and clip the CSB file for your specific region of interest. The tool can be invoked with various options to specify the region: <pre><code># Help with the options provided by the tool\n&gt;&gt; geo_epic crop_csb -h\n\n# Cropping the csb to get the shapefile for Maryland:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --state_name \"Maryland\"\n\n# Using the state FIPS code (24) for Maryland:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --state_fips \"24\"\n\n# To get the shapefile for Montgomery County, Maryland:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --county_name \"Montgomery, Maryland\"\n\n# Cropping the CSB using a bounding box:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --bbox \"-77.5,38.0,-76.0,39.5\"\n\n# Cropping the csb around a center point with a specified extent in km:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --center \"39.0,-77.0\" --extent \"50x50\"\n</code></pre></p> <p>Note: For countries other than USA, a geo-tiff file covering the region of interest with one channel being the cultivated crop land mask of that region and another channel containing corresponding management info ID can be used to utilize GeoEPIC.</p>"},{"location":"pages/epic/","title":"Environmental Policy Integrated Climate Model","text":"<p>The Environmental Policy Integrated Climate (EPIC) model, originally known as the Erosion/Productivity Impact Calculator, is designed to simulate a field, farm or small watershed, that is homogenous in terms of climate, soil, land use, and topography. The model simulates biophysical and biogeochemical processes as influenced by climate, landscape, soil and management conditions.Processes simulated include plant growth and yield, water and wind erosion, and water, carbon and nutrient cycling. EPIC is capable of simulating around hundred crops including annual, perennial and woody cropping systems grown as monocultures or polycultures. Model has been validated for multiple crops and in over 30 countries and regions and it has been used to assess productivity, environmental impacts, sustainability and climate change impacts and mitigation. EPIC has regularly been used by federal programs to investigate potential impacts of agricultural policies.</p> <p>For more details on EPIC model, visit Texas A&amp;M AgriLife site. </p> <p> </p> <p></p> <p>Initially developed in the early 1980s, (J. R. Williams, Jones, &amp; Dyke, 1984). EPIC integrated components from previous models such as CREAMS (Knisel, 1980) and SWRRB (J. R. Williams, Nicks, &amp; Arnold, 1985). The model was later enhanced with a pesticide component from GLEAMS (Leonard, 1987) to assess pesticide fate, further developed by Sabbagh, Geleta, Elliott, Williams, &amp; Griggs (1991). EPIC has evolved to address broader issues such as global climate change impacts, biomass production for energy, and landfill design, making it a versatile tool in environmental and agricultural management.</p> <p></p> <p>Refer to the user manual for detailed guidance on using EPIC: </p>"},{"location":"pages/gee/","title":"Gee","text":"<p>Google Earth Engine (GEE) is a cloud-based platform for planetary-scale environmental data analysis. It hosts a vast collection of satellite imagery and other geospatial datasets, enabling users to perform large-scale data analysis. To explore the available datasets, visit Google Earth Engine's dataset catalog and GEE Community Catalog. Private assets can also be uploaded to Earth Engine, to use them in combination with existing datasets.</p> <p>This module can be used to combine various datasets and extract the required timeseries directly from Google Earth Engine.</p> <p></p>"},{"location":"pages/gee/#1-configuration-file-breakdown","title":"1. Configuration file breakdown","text":"<p>This module utilizes a configuration file in YAML format to extract data from Google Earth Engine. The configuration file defines global parameters, Earth Engine collections, and derived variables. Below is a detailed explanation of each section.</p> <pre><code># filename: config.yml\n\n# Global parameters\nglobal_scope:\n  time_range: ['2016-01-01', '2021-12-31']\n  variables: ['nir', 'red', 'green', 'ndvi']  \n  resolution: 10\n\n# Specify Earth Engine collections and their respective variables\ncollections:\n\n  le07:\n    collection: LANDSAT/LE07/C02/T1_L2\n    select:  (b('QA_PIXEL') &gt;&gt; 6) &amp; 1\n    variables:\n      nir: b('SR_B4')*0.0000275 - 0.2\n      red: b('SR_B3')*0.0000275 - 0.2\n      green: b('SR_B2')*0.0000275 - 0.2\n\n# Derived variables\nderived_variables:\n  ndvi: '(nir - red)/(nir + red)'\n</code></pre>"},{"location":"pages/gee/#a-global-parameters","title":"a) Global Parameters","text":"<p>The <code>global_scope</code> section contains general settings applicable to the entire data extraction process.</p> <ul> <li> <p><code>time_range</code>: Specifies the period for which the satellite data should be fetched.</p> </li> <li> <p><code>variables</code>: Lists the key variables to be extracted. These are typically satellite bands or derived products such as vegetation indices, or any other relevant parameters like tempurature etc.,</p> </li> <li> <p><code>resolution</code>: Defines the spatial resolution (in meters) for the output data.</p> </li> </ul>"},{"location":"pages/gee/#b-earth-engine-collections","title":"b) Earth Engine Collections","text":"<p>Each Earth Engine dataset is defined under its own subsection within collections. The key components are collection, select, and variables. Multiple datasets can be specified, and if data from multiple collections are present for the same day, the module will return the mean of the variables across all collections.</p> <ul> <li> <p><code>collection</code>: Refers to the specific Google Earth Engine dataset identifier.</p> </li> <li> <p><code>select</code>: Defines a masking or selection condition for the data extraction.</p> </li> <li> <p><code>variables</code>: Specifies the bands or parameters to extract, along with any scaling factors or transformations.</p> </li> </ul>"},{"location":"pages/gee/#c-derived-variables","title":"c) Derived Variables","text":"<p>Derived variables are calculated from the raw bands using mathematical expressions and functions available in numpy package.</p>"},{"location":"pages/gee/#2-fetching-the-data","title":"2. Fetching the Data","text":"<p>You can use the following command-line interface to fetch time-series of required variables from Google Earth Engine into a CSV file:</p> <pre><code>geo_epic gee &lt;config-file&gt; --fetch &lt;roi&gt; --out &lt;output-path&gt;\n</code></pre> <p>The region of interest for fetching data can be provided in three formats:</p> <ul> <li>Latitude and Longitude: Use direct coordinates [latitude, longitude] <pre><code>geo_epic gee ./landsat_ndvi.yml --fetch 40.5677 98.5505 --out ./out/sample.csv\n</code></pre></li> <li> <p>Shapefile (.shp): A shapefile containing the polygons of interest. It must contain a SiteID or FieldID column. The data will be stored in the name of FieldID or SiteID. Fetches one file for each polygon. <pre><code>geo_epic gee ./landsat_ndvi.yml --fetch ./input/region.shp --out ./out\n</code></pre></p> </li> <li> <p>CSV File: A CSV file must contain a SiteID or FieldID column. Additionally, if using a CSV, it must include lat, lon columns. The data will be stored in the name of FieldID or SiteID. <pre><code>geo_epic gee ./landsat_ndvi.yml --fetch ./input/region.csv --out ./out\n</code></pre></p> </li> </ul>"},{"location":"pages/installation/","title":"Installation","text":"<p>Before starting the setup, ensure you have <code>conda</code> installed.  Follow the links for corresponding installation guides.</p>"},{"location":"pages/installation/#geoepic-toolkit-installation-recommended","title":"GeoEPIC Toolkit Installation (Recommended)","text":"<ol> <li>Download <code>epic_setup.bat</code></li> <li>Install with the following command    <pre><code>call epic_setup.bat\n</code></pre></li> </ol>"},{"location":"pages/installation/#setting-up-geoepic-manually","title":"Setting up GeoEPIC manually","text":"<ol> <li>Create a virtual environment in conda <pre><code>conda create --name epic_env python=3.11\n</code></pre></li> <li> <p>Activate the environment <pre><code>conda activate epic_env\n</code></pre></p> </li> <li> <p>Install the GeoEPIC Toolkit    There are two options for installing the GeoEPIC Toolkit:</p> <p>Option 1: Install Directly from GitHub <pre><code>pip install git+https://github.com/smarsGroup/geo_epic_win.git\n</code></pre></p> <p>Option 2: Install locally  This option is advisable only for developers.  <pre><code>git clone https://github.com/smarsGroup/geo_epic_win.git\n</code></pre> <pre><code>cd geo_epic_win\n</code></pre> <pre><code>pip install .\n</code></pre></p> </li> </ol>"},{"location":"pages/installation/#verify-installation","title":"Verify installation","text":"<pre><code>conda activate epic_env\ngeo_epic init\n</code></pre> <p>All the commands and python API can be accessed via epic_env conda environment. Happy coding!</p>"},{"location":"pages/opt/","title":"Model Parameter Calibration","text":"<p>The Calibration Module in GeoEPIC is developed to assist in tuning desired parameters involved in the EPIC model based on observational data, such as Leaf Area Index (LAI), Net Ecosystem Exchange (NEE), crop yield, or biomass. This allows model parameters to be refined to better reflect specific local conditions or experimental results.</p> <p></p>"},{"location":"pages/opt/#getting-started","title":"Getting Started","text":"<ul> <li>If the package is installed in a conda environment, activate it in commond prompt with     <pre><code>conda activate epic_env\n</code></pre></li> <li>By this point, the workspace folder must be setup with all the required input files.</li> <li>Lets say, target_yields.csv has reported yield values with SiteID, Yield as columns, we would like to calibrate few parameters so that the simulated yields match with the target yields.</li> <li>Add files required for Calibration from geo_epic with    <pre><code>   cd [your_workspace folder]\n   geo_epic copy calibration_utils\n</code></pre></li> <li>Refer calibration_starter.ipynb, which has the following lines of code.</li> </ul>"},{"location":"pages/opt/#import-required-modules","title":"Import required Modules","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom geoEpic.io import Parm, CropCom\nfrom geoEpic.core import Workspace\n\nimport os\nimport pygmo as pg\n</code></pre>"},{"location":"pages/opt/#initiate-the-workspace-class","title":"Initiate the workspace class","text":"<ul> <li>Edit the required options in the config.yml file.</li> </ul> <pre><code>exp = Workspace('./config.yml')\n\n#clear the logs and output directory\nexp.clear_logs()\nexp.clear_outputs() \n</code></pre>"},{"location":"pages/opt/#load-parameter-files-and-set-sensitive-parameters","title":"load parameter files and set sensitive parameters","text":"<pre><code># Load default cropcom.DAT file from the calibration folder\ncropcom = CropCom('./calibration_files/defaults')\n\n# Set few parameters as sensitive for calibration.\ncropcom.set_sensitive(['WA', 'HI', 'WSYF'], [2]) # here 2 is the crop code for corn\n\n# save the loaded cropcom.DAT to model folder\ncropcom.save(f'./model')\n</code></pre>"},{"location":"pages/opt/#verify-sensitive-parameters","title":"Verify sensitive parameters","text":"<pre><code>cropcom.prms\n</code></pre> <pre><code>ieparm = Parm('./calibration_files/defaults')\nieparm.set_sensitive(['./calibration_files/sensitivity/parm_yld.csv'])\nieparm.save(f'./model')\n</code></pre> <pre><code>ieparm.prms\n</code></pre>"},{"location":"pages/opt/#load-target-yields-file","title":"Load target yields file","text":"<pre><code>target_yields = pd.read_csv('path/to/target.csv')\nexp.target_yields = target_yields.set_index('SiteID')['yields'].to_dict()\n</code></pre>"},{"location":"pages/opt/#define-an-objective-function","title":"Define an objective function","text":"<ul> <li>@exp.logger routine is called after every site simulation and site object is passed as input. logger outputs are saved for later use.</li> <li>@exp.objective routine is called after finishing simulation on all sites, It is useful to get the fitness for optimization </li> </ul> <pre><code>@exp.logger\ndef yield_error(site):\n  '''\n  Process EPIC outputs to extract data and carry out required computation.\n  '''\n  target_yield = exp.target_yields[site.site_id]\n  simulated_yields = ACY(site.outputs['ACY']).get_var('YLDG')\n  last_year_yield = simulated_yields['YLDG'].values[-1]\n  return {'error': np.abs(target_yield - last_year_yield)}\n\n@exp.objective\ndef aggregate():\n  '''\n  Aggregate all the logged error values to return as objective \n  '''\n  logged_data = exp.fetch_log('yield_error')\n  logged_data = logged_data.dropna()\n  return [logged_data['error'].mean()]\n</code></pre>"},{"location":"pages/opt/#run-calibration","title":"Run Calibration","text":"<ul> <li>Choose necessary settings and optimization algorithm</li> <li>Follow this link for pygmo docs</li> </ul> <pre><code>import pygmo as pg\nfrom geoEpic.core import PygmoProblem\n\n# define pygmo problem with workspace and parameter files\nproblem = PygmoProblem(exp, cropcom, parm)\nprint('Fitness before Optimzation:', exp.run())\n</code></pre> <pre><code># Choose an algorihm and settings\nalgo = pg.algorithm(pg.pso_gen(gen = 45, memory = True))\nalgo.set_verbosity(1) \n\nprint(\"Initial Population\")\npopulation = pg.population(problem, size = 50)\n\nprint(\"Optimizing...\")\npopulation = algo.evolve(population) \n</code></pre> <pre><code>print('Fitness After Optimzation:', exp.run())\n</code></pre>"},{"location":"pages/overview/","title":"GeoEPIC Overview","text":""},{"location":"pages/runepic_old/","title":"Running an EPIC Experiment","text":""},{"location":"pages/runepic_old/#1-create-new-workspace","title":"1. Create new workspace","text":"<p><pre><code>epic_pkg workspace new -w Test\ncd Test\n</code></pre> It willl create a new workspace in the new directory named 'Test'. This 'Test' folder will automatically create sub-folders for EPIC model like model, opc, sites, soil, weather and a config.yml doc in it. You need to to go to Test folder before simulation starts.</p>"},{"location":"pages/runepic_old/#2-edit-config-file-as-needed","title":"2. Edit config file as needed","text":"<p>This package is mainly designed for study regions in the USA. In the config.yml file, update the settings in the config file based on your area of interest (AOI) and preferences. For example, if you need to run the model for the state of Maryland, USA, modify these rows as follows:</p> <pre><code>EXPName: Nitrogen Assessment (Your Experiment Name) \nRegion: Maryland \ncode: MD \nFields_of_Interests: ./CropRotations/MDRotFilt.shp\n</code></pre> <p>Note: The MDRotFilt.shp file is the same one downloaded and placed in the CropRotations folder in the previous step. <pre><code>soil: \n   ssurgo_gdb: ./soil/gSSURGO_MD.gdb\n</code></pre> Guideline: The rule of thumb is to edit the configuration file using the region code specific to your study region, as done here using \"MD\" throughout the config file.</p>"},{"location":"pages/runepic_old/#3-prepare-opc-file","title":"3. Prepare OPC File","text":"<p>OPC refers to the agricultural management practice files which is yet to automated. For now, you need to prpare management files and keep it in a new folder named 'OPC' inside the 'Test' directory.</p>"},{"location":"pages/runepic_old/#4-prepare-the-workspace","title":"4. Prepare the workspace","text":"<p><pre><code>epic_pkg workspace prepare\n</code></pre> This command will automatically pre-process the input files before simulation.</p>"},{"location":"pages/runepic_old/#5-and-execute-the-simulations","title":"5. And execute the simulations","text":"<p><pre><code>epic_pkg workspace run\n</code></pre> This command will simulate the operation/s and automatically save the results in a new folder named 'Output'. </p> <p>This command will also create a post_process.pynb doc which will have an example code to visualize the required parameters from ACY and DGN files. </p> <p>You can edit this code as per your requirements. You just have to identify the parameters and edit accordingly.</p>"},{"location":"pages/runepic_old/#example-visualization","title":"Example Visualization","text":""},{"location":"pages/runepic_old/#6-post-process-the-output-visualization","title":"6. Post-process the output visualization","text":"<p>You need to post-process the output files according to your interests. Generally, as an agricultural reserachers you need to process the DGN and ACY files.</p>"},{"location":"pages/runepic_old/#for-post-processing","title":"For post-processing","text":"<p><pre><code>epic_pkg workspace post_process\n</code></pre> This will run the example code post_process.pynb which has been created in the Test folder. It will take a variable called 'YLDG' which denotes the yearly yield in t/ha/yr for all the sites and put it in a sepearte column corresponding to all the site ids with creatinh a yldg.csv file.</p>"},{"location":"pages/runepic_old/#for-visualization","title":"For visualization","text":"<p><pre><code>epic_pkg workspace visualize\n</code></pre> It will simply plot the 'YLDG' variable corresponding to the site ids and crate a map for study region. </p>"},{"location":"pages/runepic_old/#your-plot-will-look-like-this","title":"Your plot will look like this:","text":""},{"location":"pages/runexp/","title":"Running the Model","text":""},{"location":"pages/runexp/#todo","title":"&lt;ToDo&gt;","text":"<pre><code>from geoEpic import Site, EpicModel\n\n# initialise a site object\nsite = Site(opc = './continuous_corn.OPC',  # management file\n            dly = './1123455.DLY',          # daily weather file \n            sol = './Andisol.SOL',          # soil file\n            sit = './1.SIT')                # site file\n\n\n# initialise the EPIC model\nmodel = EpicModel(path = './model/EPIC2301dt20230820')\nmodel.setup(start_year = 2014, duration = 10)\nmodel.set_output_types(['ACY', 'DGN'])\n\n# run the simulation for the site\nmodel.run(site)\n\n# get the required outputs\nacy = model.outputs['ACY']\nmodel.close()\n</code></pre> <p>Loading from config files:</p> <pre><code>site = Site.from_config(lat = , lon = , config = './config.yml')\nmodel = EpicModel.from_config(config = './config.yml')\n\nmodel.run(site)\nmodel.close()\n</code></pre> <p>Example config file: <pre><code># Model details\nEPICModel: ./model/EPIC2301dt20230820\nstart_year: 1995\nduration: 25\noutput_types:\n  - ACY  # Annual Crop data file\n  - DGN  # Daily general output file\nlog_dir: ./log\noutput_dir: ./output\n</code></pre></p> <ul> <li>To edit the OPC, SOL or files in the epic model folder, you could use the epic_editor. the following command will copy the epiceditor in to your current folder.</li> </ul> <pre><code>&gt;&gt; GeoEPIC workspace add epic_editor\n</code></pre>"},{"location":"pages/starter_notebook/","title":"EPIC Model Simulation Tutorial","text":"<p>This notebook demonstrates how to use the EPIC model with the geoEpic package.</p> <ul> <li>If the package is installed in a conda environment, activate it in commond prompt with     <pre><code>conda activate epic_env\n</code></pre></li> <li>Set up a GeoEPIC workspace using     <pre><code>geo_epic workspace -n Test\n</code></pre></li> <li>Start a Jupyter notebook inside the workspace folder     <pre><code>cd Test\njupyter notebook\n</code></pre></li> </ul>"},{"location":"pages/starter_notebook/#follow-the-below-lines-of-code","title":"Follow the below lines of code","text":"<p>Import the required classes from geoEpic</p> <pre><code>from geoEpic.core import Site, EPICModel\nfrom geoEpic.io import ACY, DGN\n</code></pre> <p>First create a <code>Site</code> object with the necessary input files. </p> <p><pre><code>site = Site(opc = './opc/files/umstead.OPC',\n            dly = './weather/NCRDU.DLY',\n            sol = './soil/files/umstead.SOL',\n            sit = './sites/umstead.SIT')\nprint(site.site_id)\n</code></pre> umstead</p>"},{"location":"pages/starter_notebook/#define-the-epicmodel-class","title":"Define the EPICModel class","text":"<p>Now Let's create an <code>EPICModel</code> object and specify the start date, duration of the simulation.</p> <pre><code>model = EPICModel('./model/EPIC1102.exe')\nmodel.start_date = '2015-01-01'\nmodel.duration = 5\nmodel.output_types = ['ACY']\n</code></pre> <p>Run the model simulations at the required site</p> <pre><code>model.run(site)\n# Close the model instance\nmodel.close()\n# Path to output files is stored in the site.outputs dict\nsite.outputs\n</code></pre> <ul> <li>EPICModel instance can also be created using a configuration file.  Example config file: <pre><code># Model details\nEPICModel: ./model/EPIC1102.exe\nstart_year: '2015-01-01'\nduration: 5\noutput_types:\n  - ACY  # Annual Crop data file\n  - DGN  # Daily general output file\nlog_dir: ./log\noutput_dir: ./output\n</code></pre></li> <li>This method allows for easier management of model parameters.</li> </ul>"},{"location":"pages/starter_notebook/#using-epicmodel-class-with-configuration-file","title":"Using EPICModel class with Configuration File","text":"<pre><code>model = EPICModel.from_config('./config.yml')\nmodel.run(site)\nmodel.close()\n\n#using with context\nwith EPICModel.from_config('./config.yml') as model:\n    model.run(site)\n</code></pre>"},{"location":"pages/starter_notebook/#examine-the-outputs","title":"Examine the outputs","text":"<p>Finally, examine the outputs generated by the model run.</p> <pre><code>yields = ACY(site.outputs['ACY']).get_var('YLDG')\nyields\n</code></pre> index YR CPNM YLDG 0 0 2015 CORN 7.175 1 1 2016 CORN 4.735 2 2 2017 CORN 9.072 3 3 2018 CORN 7.829 4 4 2019 CORN 5.434 <p>Plot the simulated Leaf Area Index</p> <pre><code>import matplotlib.pyplot as plt\n\nlai = DGN(site.outputs['DGN']).get_var('LAI')\n\nplt.figure(figsize=(12, 6))\nplt.plot(lai['Date'], lai['LAI'])\nplt.title('Leaf Area Index (LAI) Over Time')\nplt.xlabel('Date')\nplt.ylabel('LAI')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"pages/api/api/","title":"Api","text":"<p>GeoEPIC provides a command-line interface to execute various tasks.   The general command structure is as follows:</p> <pre><code>geo_epic {module} {func} [options]\n</code></pre> <p>Example usage: <pre><code>geo_epic workspace new -n Test\n</code></pre></p>"},{"location":"pages/api/api/#modules-and-functions","title":"Modules and Functions","text":""},{"location":"pages/api/api/#workspace","title":"workspace","text":"<ul> <li>new: Create a new workspace with a predefined template structure.</li> <li>copy: Copy files between different folders.</li> <li>run: Execute simulations within the workspace.</li> </ul>"},{"location":"pages/api/api/#utility","title":"utility","text":"<ul> <li>gee: Download required time-series data from Google Earth Engine.</li> </ul>"},{"location":"pages/api/api/#weather","title":"weather","text":"<ul> <li>ee: Retrieve weather data from Earth Engine.</li> <li>download_daily: Download daily weather data from Daymet.</li> </ul>"},{"location":"pages/api/api/#soil","title":"soil","text":"<ul> <li>usda: Fetch soil data from USDA SSURGO.</li> <li>process_gdb: Process SSURGO geodatabase (GDB) files.</li> </ul>"},{"location":"pages/api/api/#sites","title":"sites","text":"<ul> <li>generate: Generate site files from processed data.</li> </ul> <p>For more details on each command and its available options, use the following command:</p> <pre><code>geo_epic {module} {func} --help\n</code></pre>"},{"location":"pages/api/core/","title":"Core Module","text":""},{"location":"pages/api/core/#core","title":"<code>core</code>","text":"<p>Classes:</p> Name Description <code>EPICModel</code> <p>This class handles the setup and execution of the EPIC model executable.</p> <code>Problem_Wrapper</code> <p>A wrapper class that provides a simplified interface for optimization and sensitivity analysis.</p> <code>PygmoProblem</code> <p>A class designed to define an optimization problem for use with the PyGMO library, </p> <code>Site</code> <p>Represents a site (ex: agricultural field) with paths to it's corresponding EPIC input files.</p> <code>Workspace</code> <p>This class organises the workspace for executing simulations, saving required results.</p>"},{"location":"pages/api/core/#core.EPICModel","title":"<code>EPICModel</code>","text":"<p>This class handles the setup and execution of the EPIC model executable.</p> <p>Attributes:</p> Name Type Description <code>base_dir</code> <code>str</code> <p>The base directory for model runs.</p> <code>executable</code> <code>str</code> <p>Path to the executable model file.</p> <code>output_dir</code> <code>str</code> <p>Directory to store model outputs.</p> <code>log_dir</code> <code>str</code> <p>Directory to store logs.</p> <code>start_date</code> <code>date</code> <p>The start date of the EPIC model simulation.</p> <code>duration</code> <code>int</code> <p>The duration of the EPIC model simulation in years.</p> <code>output_types</code> <code>list</code> <p>A list of enabled output types for the EPIC model.</p> <code>model_dir</code> <code>str</code> <p>Directory path where the executable is located.</p> <code>executable_name</code> <code>str</code> <p>Name of the executable file.</p> <p>Methods:</p> Name Description <code>auto_Nfertilization</code> <p>Update the nitrogen settings in the EPICCONT.DAT file.</p> <code>auto_irrigation</code> <p>Update the irrigation settings in the EPICCONT.DAT file.</p> <code>close</code> <p>Release the lock on the model's directory by deleting the lock file.</p> <code>from_config</code> <p>Create an EPICModel instance from a configuration path.</p> <code>run</code> <p>Execute the model for the given site and handle output files.</p> <code>set_output_types</code> <p>Set the model output types and update the model's print file to enable specified outputs.</p> <code>setup</code> <p>Set up the model run configurations based on provided settings.</p>"},{"location":"pages/api/core/#core.EPICModel.duration","title":"<code>duration</code>  <code>property</code> <code>writable</code>","text":"<p>Get the duration of the EPIC model simulation.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The duration of the simulation in years.</p>"},{"location":"pages/api/core/#core.EPICModel.output_types","title":"<code>output_types</code>  <code>property</code> <code>writable</code>","text":"<p>Get the current output types of the EPIC model.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of enabled output types.</p>"},{"location":"pages/api/core/#core.EPICModel.start_date","title":"<code>start_date</code>  <code>property</code> <code>writable</code>","text":"<p>Get the start date of the EPIC model simulation.</p> <p>Returns:</p> Type Description <p>datetime.date: The start date of the simulation.</p>"},{"location":"pages/api/core/#core.EPICModel.auto_Nfertilization","title":"<code>auto_Nfertilization(bft0, fnp=None, fmx=None)</code>","text":"<p>Update the nitrogen settings in the EPICCONT.DAT file. Only BFT0 is required. Other parameters will be updated only if they are not None.</p> <p>:param file_path: Path to the EPICCONT.DAT file :param bft0: Nitrogen stress factor to trigger auto fertilization (BFT0) - required :param fnp: Fertilizer application variable (FNP) - optional :param fmx: Maximum annual N fertilizer applied for a crop (FMX) - optional</p>"},{"location":"pages/api/core/#core.EPICModel.auto_irrigation","title":"<code>auto_irrigation(bir, efi=None, vimx=None, armn=None, armx=None)</code>","text":"<p>Update the irrigation settings in the EPICCONT.DAT file. Only BIR is required. Other parameters will be updated only if they are not None.</p> <p>:param file_path: Path to the EPICCONT.DAT file :param bir: Water stress factor to trigger automatic irrigation (BIR) - required :param efi: Runoff volume/Volume irrigation water applied (EFI) - optional :param vimx: Maximum annual irrigation volume (VIMX) in mm - optional :param armn: Minimum single application volume (ARMN) in mm - optional :param armx: Maximum single application volume (ARMX) in mm - optional</p>"},{"location":"pages/api/core/#core.EPICModel.close","title":"<code>close()</code>","text":"<p>Release the lock on the model's directory by deleting the lock file.</p>"},{"location":"pages/api/core/#core.EPICModel.from_config","title":"<code>from_config(config_path)</code>  <code>classmethod</code>","text":"<p>Create an EPICModel instance from a configuration path.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <p>Returns:</p> Name Type Description <code>EPICModel</code> <p>A configured instance of the EPICModel.</p>"},{"location":"pages/api/core/#core.EPICModel.run","title":"<code>run(site, verbose=False, dest=None)</code>","text":"<p>Execute the model for the given site and handle output files.</p> <p>Parameters:</p> Name Type Description Default <code>site</code> <code>Site</code> <p>A site instance containing site-specific configuration.</p> required <code>dest</code> <code>str</code> <p>Destination directory for the run. If None, a temporary directory is used.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If any output file is not generated or is empty.</p>"},{"location":"pages/api/core/#core.EPICModel.set_output_types","title":"<code>set_output_types(output_types)</code>","text":"<p>Set the model output types and update the model's print file to enable specified outputs.</p> <p>Parameters:</p> Name Type Description Default <code>output_types</code> <code>list of str</code> <p>List of output types to be enabled.</p> required"},{"location":"pages/api/core/#core.EPICModel.setup","title":"<code>setup(config)</code>","text":"<p>Set up the model run configurations based on provided settings.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing model settings.</p> required"},{"location":"pages/api/core/#core.Problem_Wrapper","title":"<code>Problem_Wrapper</code>","text":"<p>A wrapper class that provides a simplified interface for optimization and sensitivity analysis.</p> <p>Attributes:</p> Name Type Description <code>problem</code> <code>PygmoProblem</code> <p>The PygmoProblem instance.</p> <code>pg_problem</code> <code>problem</code> <p>The wrapped PyGMO problem instance.</p> <code>algorithm</code> <p>The PyGMO algorithm instance for optimization.</p> <code>population</code> <p>The PyGMO population instance.</p> <code>population_size</code> <code>int</code> <p>Size of the population for optimization.</p> <p>Methods:</p> Name Description <code>init</code> <p>Initialize the optimization algorithm and population.</p> <code>optimize</code> <p>Run the optimization process.</p> <code>sensitivity_analysis</code> <p>Perform sensitivity analysis using SALib with status updates.</p>"},{"location":"pages/api/core/#core.Problem_Wrapper.init","title":"<code>init(algorithm, **kwargs)</code>","text":"<p>Initialize the optimization algorithm and population.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <p>PyGMO algorithm class (e.g., pg.pso_gen)</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the algorithm</p> <code>{}</code>"},{"location":"pages/api/core/#core.Problem_Wrapper.optimize","title":"<code>optimize(population_size, generations)</code>","text":"<p>Run the optimization process.</p> <p>Parameters:</p> Name Type Description Default <code>population_size</code> <code>int</code> <p>Size of the population for optimization</p> required <code>generations</code> <code>int</code> <p>Number of generations to run</p> required <p>Returns:</p> Type Description <p>The evolved population after optimization</p>"},{"location":"pages/api/core/#core.Problem_Wrapper.sensitivity_analysis","title":"<code>sensitivity_analysis(base_no_of_samples, method)</code>","text":"<p>Perform sensitivity analysis using SALib with status updates.</p> <p>Parameters: - base_no_of_samples (int): Base number of samples to generate. - method (str): Sensitivity analysis method ('sobol', 'efast', 'morris').</p> <p>Returns: - dict: Results of the sensitivity analysis.</p>"},{"location":"pages/api/core/#core.PygmoProblem","title":"<code>PygmoProblem</code>","text":"<p>A class designed to define an optimization problem for use with the PyGMO library, </p> <p>Attributes:</p> Name Type Description <code>workspace</code> <code>Workspace</code> <p>The workspace object managing the environment in which the model runs.</p> <code>dfs</code> <code>tuple</code> <p>A tuple of DataFrame-like objects that hold constraints and parameters for the problem.</p> <code>bounds</code> <code>ndarray</code> <p>An array of parameter bounds, each specified as (min, max).</p> <code>lens</code> <code>ndarray</code> <p>An array of cumulative lengths that help in splitting parameters for each DataFrame.</p> <p>Methods:</p> Name Description <code>apply_solution</code> <p>Apply a solution vector to update parameters in all dataframes.</p> <code>fitness</code> <p>Evaluate the fitness of a solution vector 'x'.</p> <code>get_bounds</code> <p>Get the bounds for parameters as tuples of (min, max) values for each parameter across all data frames.</p>"},{"location":"pages/api/core/#core.PygmoProblem.current","title":"<code>current</code>  <code>property</code>","text":"<p>Retrieve the current parameter values from all data frames.</p> <p>Returns:</p> Type Description <p>np.array: A concatenated array of current parameter values from all data frames.</p>"},{"location":"pages/api/core/#core.PygmoProblem.var_names","title":"<code>var_names</code>  <code>property</code>","text":"<p>Get the variable names from all data frames.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of variable names concatenated from all data frames. Each data frame's   var_names() method is called and the results are combined into a single list.</p>"},{"location":"pages/api/core/#core.PygmoProblem.apply_solution","title":"<code>apply_solution(x)</code>","text":"<p>Apply a solution vector to update parameters in all dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>A solution vector containing parameter values for all data frames.</p> required"},{"location":"pages/api/core/#core.PygmoProblem.fitness","title":"<code>fitness(x)</code>","text":"<p>Evaluate the fitness of a solution vector 'x'.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>A solution vector containing parameter values for all data frames.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The fitness value as determined by the workspace's fitness function.</p>"},{"location":"pages/api/core/#core.PygmoProblem.get_bounds","title":"<code>get_bounds()</code>","text":"<p>Get the bounds for parameters as tuples of (min, max) values for each parameter across all data frames.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Two numpy arrays representing the lower and upper bounds of the parameters.</p>"},{"location":"pages/api/core/#core.Site","title":"<code>Site</code>","text":"<p>Represents a site (ex: agricultural field) with paths to it's corresponding EPIC input files.</p> <p>Attributes:</p> Name Type Description <code>opc_path</code> <code>str</code> <p>Path to the operational practice code file.</p> <code>dly_path</code> <code>str</code> <p>Path to the daily weather data file.</p> <code>sol_path</code> <code>str</code> <p>Path to the soil data file.</p> <code>sit_path</code> <code>str</code> <p>Path to the site information file.</p> <code>site_id</code> <code>str</code> <p>Identifier for the site, derived from the sit file name if not provided.</p> <code>outputs</code> <code>dict</code> <p>Dictionary to store output file paths.</p> <p>Methods:</p> Name Description <code>copy</code> <p>Copy or symlink site files to a destination folder.</p> <code>fetch_usa</code> <p>Fetch all required EPIC input files for a location in the conterminous USA.</p> <code>from_config</code> <p>Factory method to create a Site instance from a configuration dictionary and additional site information.</p> <code>get_dly</code> <p>Retrieve daily weather data from a DLY file.</p> <code>get_opc</code> <p>Retrieve operation schedule data from an OPC file.</p> <code>get_sit</code> <p>Retrieve site data from a SIT file.</p> <code>get_sol</code> <p>Retrieve soil data from a SOL file.</p>"},{"location":"pages/api/core/#core.Site.elevation","title":"<code>elevation</code>  <code>property</code>","text":"<p>Elevation of the site.</p>"},{"location":"pages/api/core/#core.Site.latitude","title":"<code>latitude</code>  <code>property</code>","text":"<p>Latitude of the site.</p>"},{"location":"pages/api/core/#core.Site.longitude","title":"<code>longitude</code>  <code>property</code>","text":"<p>Longitude of the site.</p>"},{"location":"pages/api/core/#core.Site.copy","title":"<code>copy(dest_folder, use_symlink=False)</code>","text":"<p>Copy or symlink site files to a destination folder.</p> <p>Parameters:</p> Name Type Description Default <code>dest_folder</code> <code>str</code> <p>Destination folder path</p> required <code>use_symlinks</code> <code>bool</code> <p>If True, create symbolic links instead of copying files. Defaults to False.</p> required <p>Returns:</p> Name Type Description <code>Site</code> <p>A new Site instance pointing to the copied/linked files</p>"},{"location":"pages/api/core/#core.Site.fetch_usa","title":"<code>fetch_usa(lat, lon, opc, site_id=None, start_date=None, end_date=None)</code>  <code>classmethod</code>","text":"<p>Fetch all required EPIC input files for a location in the conterminous USA.</p> <p>This method automatically retrieves: - Soil data from SSURGO via Soil Data Access - Weather data from Daymet - Elevation and slope from DEM (GLO-30)</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of the site</p> required <code>lon</code> <code>float</code> <p>Longitude of the site</p> required <code>opc</code> <code>str</code> <p>Path to the OPC (operation schedule) file</p> required <code>site_id</code> <code>str</code> <p>Site identifier. Auto-generated if None.</p> <code>None</code> <code>start_date</code> <code>str</code> <p>Weather data start date (YYYY-MM-DD format)</p> <code>None</code> <code>end_date</code> <code>str</code> <p>Weather data end date (YYYY-MM-DD format)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Site</code> <p>A Site object with all input files fetched and ready for simulation</p> Example <p>site = Site.fetch_usa(41.1686, -96.4736, opc='./irrigated_corn.OPC') print(site)</p>"},{"location":"pages/api/core/#core.Site.from_config","title":"<code>from_config(config, **site_info)</code>  <code>classmethod</code>","text":"<p>Factory method to create a Site instance from a configuration dictionary and additional site information.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with paths and settings.</p> required <code>**site_info</code> <code>dict</code> <p>Keyword arguments containing site-specific information such as 'opc', 'dly', 'soil', 'SiteID', 'lat', 'lon', and 'ele'.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Site</code> <p>An instance of the Site class configured according to the provided settings.</p>"},{"location":"pages/api/core/#core.Site.get_dly","title":"<code>get_dly()</code>","text":"<p>Retrieve daily weather data from a DLY file.</p> <p>Returns:</p> Name Type Description <code>DailyWeather</code> <p>An instance of the DailyWeather class containing weather data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the DLY file does not exist at the specified path.</p>"},{"location":"pages/api/core/#core.Site.get_opc","title":"<code>get_opc()</code>","text":"<p>Retrieve operation schedule data from an OPC file.</p> <p>Returns:</p> Name Type Description <code>Operation</code> <p>An instance of the Operation class containing operation schedule data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the OPC file does not exist at the specified path.</p>"},{"location":"pages/api/core/#core.Site.get_sit","title":"<code>get_sit()</code>","text":"<p>Retrieve site data from a SIT file.</p> <p>Returns:</p> Name Type Description <code>Site</code> <p>An instance of the Site class containing site data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the SIT file does not exist at the specified path.</p>"},{"location":"pages/api/core/#core.Site.get_sol","title":"<code>get_sol()</code>","text":"<p>Retrieve soil data from a SOL file.</p> <p>Returns:</p> Name Type Description <code>Soil</code> <p>An instance of the Soil class containing soil data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the SOL file does not exist at the specified path.</p>"},{"location":"pages/api/core/#core.Workspace","title":"<code>Workspace</code>","text":"<p>This class organises the workspace for executing simulations, saving required results.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>str</code> <p>Unique ID assigned to each workspace instance</p> <code>config</code> <code>dict</code> <p>Configuration data loaded from a config file.</p> <code>base_dir</code> <code>str</code> <p>Base directory for the workspace.</p> <code>routines</code> <code>dict</code> <p>Dictionary to store functions as routines.</p> <code>objective_function</code> <code>callable</code> <p>Function to be executed as the objective.</p> <code>dataframes</code> <code>dict</code> <p>Cache for dataframes.</p> <code>delete_after_use</code> <code>bool</code> <p>Whether to delete temporary files after use.</p> <code>model</code> <code>EPICModel</code> <p>Instance of the EPIC model.</p> <code>data_logger</code> <code>DataLogger</code> <p>Instance of the DataLogger for logging data.</p> <p>Methods:</p> Name Description <code>clear_logs</code> <p>Clear all log files and temporary run directories.</p> <code>clear_outputs</code> <p>Clear all output files.</p> <code>close</code> <p>Explicit cleanup (use this in notebooks).</p> <code>fetch_log</code> <p>Retrieve the logs for a specific function.</p> <code>logger</code> <p>Decorator to log the results of a function.</p> <code>make_problem</code> <p>Create a PygmoProblem instance after validating inputs.</p> <code>objective</code> <p>Set the objective function to be executed after simulations.</p> <code>routine</code> <p>Decorator to add a function as a routine without logging or returning values.</p> <code>run</code> <p>Run simulations for all sites or filtered by a selection string.</p> <code>run_simulation</code> <p>Run simulation for a given site or site information.</p>"},{"location":"pages/api/core/#core.Workspace.clear_logs","title":"<code>clear_logs()</code>","text":"<p>Clear all log files and temporary run directories.</p>"},{"location":"pages/api/core/#core.Workspace.clear_outputs","title":"<code>clear_outputs()</code>","text":"<p>Clear all output files.</p>"},{"location":"pages/api/core/#core.Workspace.close","title":"<code>close()</code>","text":"<p>Explicit cleanup (use this in notebooks).</p>"},{"location":"pages/api/core/#core.Workspace.fetch_log","title":"<code>fetch_log(func, keep=False)</code>","text":"<p>Retrieve the logs for a specific function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>str</code> <p>The name of the function whose logs are to be retrieved.</p> required <code>keep</code> <code>bool</code> <p>If True, preserve the logs after retrieval. If False, logs are deleted after reading. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: DataFrame containing the logs for the specified function.</p>"},{"location":"pages/api/core/#core.Workspace.logger","title":"<code>logger(func)</code>","text":"<p>Decorator to log the results of a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function that logs its output.</p>"},{"location":"pages/api/core/#core.Workspace.make_problem","title":"<code>make_problem(*dfs)</code>","text":"<p>Create a PygmoProblem instance after validating inputs.</p> <p>Parameters:</p> Name Type Description Default <code>*dfs</code> <p>Variable number of dataframes to pass to PygmoProblem</p> <code>()</code> <p>Returns:</p> Name Type Description <code>PygmoProblem</code> <p>A configured optimization problem instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no dataframes provided or if any dataframe lacks constraints</p>"},{"location":"pages/api/core/#core.Workspace.objective","title":"<code>objective(func)</code>","text":"<p>Set the objective function to be executed after simulations.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The objective function to be set.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorator function that sets the objective function.</p>"},{"location":"pages/api/core/#core.Workspace.routine","title":"<code>routine(func)</code>","text":"<p>Decorator to add a function as a routine without logging or returning values.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function that executes without logging.</p>"},{"location":"pages/api/core/#core.Workspace.run","title":"<code>run(select_str=None, progress_bar=True)</code>","text":"<p>Run simulations for all sites or filtered by a selection string.</p> <p>Parameters:</p> Name Type Description Default <code>select_str</code> <code>str</code> <p>String to filter sites. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the objective function if set, otherwise None.</p>"},{"location":"pages/api/core/#core.Workspace.run_simulation","title":"<code>run_simulation(site_or_info)</code>","text":"<p>Run simulation for a given site or site information.</p> <p>Parameters:</p> Name Type Description Default <code>site_or_info</code> <code>Site or dict</code> <p>A Site object or a dictionary containing site information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The results from the post-processing routines. Output files are saved based on the options selected</p>"},{"location":"pages/api/gee/","title":"GEE Module","text":""},{"location":"pages/api/gee/#gee","title":"<code>gee</code>","text":"<p>Classes:</p> Name Description <code>CompositeCollection</code> <p>A class to handle composite collection of Earth Engine data.</p>"},{"location":"pages/api/gee/#gee.CompositeCollection","title":"<code>CompositeCollection</code>","text":"<p>A class to handle composite collection of Earth Engine data.</p> <p>This class initializes collections of Earth Engine based on a provided YAML configuration file, applies specified formulas and selections, and allows for the extraction of temporal data for a given Area of Interest (AOI).</p> <p>Methods:</p> Name Description <code>extract</code> <p>Extracts temporal data for a given AOI and returns it as a pandas DataFrame.</p>"},{"location":"pages/api/gee/#gee.CompositeCollection.extract","title":"<code>extract(aoi_coords)</code>","text":"<p>Extracts temporal data for a given Area of Interest (AOI).</p> <p>Parameters:</p> Name Type Description Default <code>aoi_coords</code> <code>tuple / list</code> <p>Coordinates representing the AOI, either as a Point or as vertices of a Polygon.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A pandas DataFrame containing the extracted data.</p>"},{"location":"pages/api/gee/#gee.CompositeCollection.merged","title":"<code>merged()</code>","text":"<p>Merges all collections in self.collections into a single ImageCollection.</p> <p>Returns:</p> Type Description <p>ee.ImageCollection: A merged collection containing all images from all collections.</p>"},{"location":"pages/api/io/","title":"IO Module","text":""},{"location":"pages/api/io/#io","title":"<code>io</code>","text":"<p>Classes:</p> Name Description <code>ACY</code> <code>DGN</code> <code>DLY</code> <code>DSL</code> <code>DWC</code> <code>OPC</code> <code>SIT</code> <code>SOL</code>"},{"location":"pages/api/io/#io.ACY","title":"<code>ACY</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the ACY data.</p>"},{"location":"pages/api/io/#io.ACY.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the ACY data.</p>"},{"location":"pages/api/io/#io.DGN","title":"<code>DGN</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DGN data.</p>"},{"location":"pages/api/io/#io.DGN.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DGN data.</p>"},{"location":"pages/api/io/#io.DLY","title":"<code>DLY</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>load</code> <p>Load data from a DLY file into DataFrame.</p> <code>save</code> <p>Save DataFrame into a DLY file.</p> <code>to_monthly</code> <p>Save as monthly file</p> <code>validate</code> <p>Validate the DataFrame to ensure it contains a continuous range of dates </p>"},{"location":"pages/api/io/#io.DLY.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load data from a DLY file into DataFrame.</p>"},{"location":"pages/api/io/#io.DLY.save","title":"<code>save(path=None)</code>","text":"<p>Save DataFrame into a DLY file.</p>"},{"location":"pages/api/io/#io.DLY.to_monthly","title":"<code>to_monthly(path=None)</code>","text":"<p>Save as monthly file</p>"},{"location":"pages/api/io/#io.DLY.validate","title":"<code>validate(start_date, end_date)</code>","text":"<p>Validate the DataFrame to ensure it contains a continuous range of dates  between start_date and end_date, without duplicates.</p>"},{"location":"pages/api/io/#io.DSL","title":"<code>DSL</code>","text":"<p>Methods:</p> Name Description <code>get_data</code> <p>Return stored water data.</p>"},{"location":"pages/api/io/#io.DSL.get_data","title":"<code>get_data()</code>","text":"<p>Return stored water data.</p>"},{"location":"pages/api/io/#io.DWC","title":"<code>DWC</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DWC data.</p>"},{"location":"pages/api/io/#io.DWC.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DWC data.</p>"},{"location":"pages/api/io/#io.OPC","title":"<code>OPC</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>append</code> <p>Append another OPC or DataFrame to the current OPC instance.</p> <code>edit_crop_season</code> <p>Edit the planting and/or harvest dates for a given year and crop.</p> <code>edit_fertilizer_rate</code> <p>Edit the fertilizer rate for a given year.</p> <code>edit_harvest_date</code> <p>Edit the harvest date for a given year and crop.</p> <code>edit_operation_date</code> <p>Edit the operation date for a given year.</p> <code>edit_operation_value</code> <p>Edit the operation value for a given year.</p> <code>edit_plantation_date</code> <p>Edit the plantation date for a given year and crop.</p> <code>get_harvest_date</code> <p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <code>get_plantation_date</code> <p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <code>iter_seasons</code> <p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <code>load</code> <p>Load data from an OPC file into DataFrame.</p> <code>new</code> <p>Create a new OPC instance with an empty DataFrame with preset columns, a default header,</p> <code>remove</code> <p>Remove operation(s) from the OPC file that match all provided criteria.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>update</code> <p>Add or update an operation in the OPC file.</p> <code>update_phu</code> <p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <code>validate</code> <p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p>"},{"location":"pages/api/io/#io.OPC.IAUI","title":"<code>IAUI</code>  <code>property</code> <code>writable</code>","text":"<p>Get the auto-irrigation implement ID from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if auto-irrigation is enabled (72), False if disabled (0)</p>"},{"location":"pages/api/io/#io.OPC.LUN","title":"<code>LUN</code>  <code>property</code> <code>writable</code>","text":"<p>Get the land use number from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The land use number from the first 4 characters of header line 2</p>"},{"location":"pages/api/io/#io.OPC.append","title":"<code>append(second_opc)</code>","text":"<p>Append another OPC or DataFrame to the current OPC instance. Args:     second_opc (pd.DataFrame or OPC): The data to append. Returns:     OPC: A new OPC instance with combined data. Raises:     ValueError: If second_opc is not a pandas DataFrame or OPC instance.</p>"},{"location":"pages/api/io/#io.OPC.edit_crop_season","title":"<code>edit_crop_season(new_planting_date=None, new_harvest_date=None, crop_code=None)</code>","text":"<p>Edit the planting and/or harvest dates for a given year and crop.</p> <p>Parameters: year (int): Year. new_planting_date (datetime, optional): New planting date. If not provided, only harvest date will be updated. new_harvest_date (datetime, optional): New harvest date. If not provided, only planting date will be updated. crop_code (int, optional): Crop code. If not provided, changes the first crop found.</p>"},{"location":"pages/api/io/#io.OPC.edit_fertilizer_rate","title":"<code>edit_fertilizer_rate(rate, year=2020, month=None, day=None)</code>","text":"<p>Edit the fertilizer rate for a given year.</p> <p>Parameters: rate (float): Fertilizer rate to be set. year (int, optional): Year for the fertilizer rate application. Defaults to 2020. month (int, optional): Month for the fertilizer rate application. If not provided, the first instance is changed. day (int, optional): Day for the fertilizer rate application. Defaults to None.</p>"},{"location":"pages/api/io/#io.OPC.edit_harvest_date","title":"<code>edit_harvest_date(date, crop_code)</code>","text":"<p>Edit the harvest date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of harvest in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"pages/api/io/#io.OPC.edit_operation_date","title":"<code>edit_operation_date(code, year, month, day, crop_code=None)</code>","text":"<p>Edit the operation date for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. month (int): Month of operation. day (int): Day of operation. crop_code (int, optional): Crop code.</p>"},{"location":"pages/api/io/#io.OPC.edit_operation_value","title":"<code>edit_operation_value(code, year, value, crop_code=None)</code>","text":"<p>Edit the operation value for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. value (float): New operation value. crop_code (int, optional): Crop code.</p>"},{"location":"pages/api/io/#io.OPC.edit_plantation_date","title":"<code>edit_plantation_date(date, crop_code)</code>","text":"<p>Edit the plantation date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of plantation in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"pages/api/io/#io.OPC.get_harvest_date","title":"<code>get_harvest_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their harvest dates with row indices.</p>"},{"location":"pages/api/io/#io.OPC.get_plantation_date","title":"<code>get_plantation_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their plantation dates with row indices.</p>"},{"location":"pages/api/io/#io.OPC.iter_seasons","title":"<code>iter_seasons(start_year=None, end_year=None)</code>","text":"<p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <p>Parameters: start_year (int, optional): The starting year to consider. Defaults to None. end_year (int, optional): The ending year to consider. Defaults to None.</p> <p>dict: A dictionary containing:     - plantation_date: The date of plantation     - harvest_date: The date of harvest     - crop_code: The crop code     - operations: A subset of OPC rows for this season     - plantation_index: The index of the plantation row</p>"},{"location":"pages/api/io/#io.OPC.load","title":"<code>load(path, start_year=None)</code>  <code>classmethod</code>","text":"<p>Load data from an OPC file into DataFrame.</p> <p>Parameters: path (str): Path to the OPC file. start_year (int, optional): Start year for the OPC file. If not provided, it will be read from the file header.</p> <p>Returns: OPC: An instance of the OPC class containing the loaded data.</p>"},{"location":"pages/api/io/#io.OPC.new","title":"<code>new(name, start_year)</code>  <code>classmethod</code>","text":"<p>Create a new OPC instance with an empty DataFrame with preset columns, a default header, and provided name and start year.</p> <p>Parameters: name (str): The name for this OPC file/instance. start_year (int): The start year to assign in the OPC instance.</p> <p>Returns: OPC: An OPC instance with no data but with the required metadata set.</p>"},{"location":"pages/api/io/#io.OPC.remove","title":"<code>remove(opID=None, date=None, cropID=None, XMTU=None, fertID=None, year=None)</code>","text":"<p>Remove operation(s) from the OPC file that match all provided criteria.</p> <p>Parameters:</p> Name Type Description Default <code>opID</code> <code>int</code> <p>Operation ID to match</p> <code>None</code> <code>date</code> <code>str</code> <p>Date to match in format 'YYYY-MM-DD'</p> <code>None</code> <code>cropID</code> <code>int</code> <p>Crop ID to match</p> <code>None</code> <code>XMTU/LYR/pestID/fertID</code> <code>int</code> <p>Machine type/layer/pesticide ID/fertilizer ID to match</p> required <code>year</code> <code>int</code> <p>Year to match</p> <code>None</code>"},{"location":"pages/api/io/#io.OPC.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p> <p>Parameters: path (str): Path to save the OPC file.</p>"},{"location":"pages/api/io/#io.OPC.update","title":"<code>update(operation)</code>","text":"<p>Add or update an operation in the OPC file.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Dictionary containing operation details with keys: - opID: Operation ID (required) - cropID: Crop ID (required) - date: Operation date as string 'YYYY-MM-DD' (required) - XMTU/LYR/pestID/fertID: Machine type/years/pesticide ID/fertilizer ID (optional, default 0) - OPV1-OPV8: Additional operation values (optional, default 0)</p> required"},{"location":"pages/api/io/#io.OPC.update_phu","title":"<code>update_phu(dly, cropcom)</code>","text":"<p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <p>Parameters: dly (DLY): DLY object containing weather data. cropcom (DataFrame): DataFrame containing crop code and TBS values.</p>"},{"location":"pages/api/io/#io.OPC.validate","title":"<code>validate(duration=None)</code>","text":"<p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p> <p>Parameters: duration (int, optional): Duration of the simulation in years. If None, uses the maximum Yid.</p> <p>Returns: bool: True if the data is valid, False otherwise.</p>"},{"location":"pages/api/io/#io.SIT","title":"<code>SIT</code>","text":"<p>Methods:</p> Name Description <code>load</code> <p>Class method to load the .sit file and return a SiteFile instance.</p> <code>save</code> <p>Save the current site information to a .SIT file.</p>"},{"location":"pages/api/io/#io.SIT.elevation","title":"<code>elevation</code>  <code>property</code> <code>writable</code>","text":"<p>Get elevation value.</p>"},{"location":"pages/api/io/#io.SIT.lat","title":"<code>lat</code>  <code>property</code> <code>writable</code>","text":"<p>Get latitude value.</p>"},{"location":"pages/api/io/#io.SIT.lon","title":"<code>lon</code>  <code>property</code> <code>writable</code>","text":"<p>Get longitude value.</p>"},{"location":"pages/api/io/#io.SIT.slope","title":"<code>slope</code>  <code>property</code> <code>writable</code>","text":"<p>Get slope steep value.</p>"},{"location":"pages/api/io/#io.SIT.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Class method to load the .sit file and return a SiteFile instance.</p> <p>Parameters: file_path (str): Path to the .sit file.</p> <p>Returns: SiteFile: An instance of the SiteFile class with loaded data.</p>"},{"location":"pages/api/io/#io.SIT.save","title":"<code>save(output_dir)</code>","text":"<p>Save the current site information to a .SIT file.</p>"},{"location":"pages/api/io/#io.SOL","title":"<code>SOL</code>","text":"<p>Methods:</p> Name Description <code>from_sda</code> <p>Create a Soil object from Soil Data Access using a query.</p> <code>load</code> <p>Load soil data from a file and return a Soil object.</p> <code>save</code> <p>Save the soil data to a file using a template.</p>"},{"location":"pages/api/io/#io.SOL.from_sda","title":"<code>from_sda(query)</code>  <code>classmethod</code>","text":"<p>Create a Soil object from Soil Data Access using a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>int or str</code> <p>Query string for SoilDataAccess. (mukey or WKT str) ( \"POINT(-123.4567 45.6789)\" )</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from SDA.</p>"},{"location":"pages/api/io/#io.SOL.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Load soil data from a file and return a Soil object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the soil file.</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from the file.</p>"},{"location":"pages/api/io/#io.SOL.save","title":"<code>save(filepath, template=None)</code>","text":"<p>Save the soil data to a file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the soil file.</p> required <code>template</code> <code>list</code> <p>Optional list of template lines.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If soil properties DataFrame is empty.</p>"},{"location":"pages/api/io/#io.config_parser","title":"<code>config_parser</code>","text":"<p>Classes:</p> Name Description <code>ConfigParser</code>"},{"location":"pages/api/io/#io.config_parser.ConfigParser","title":"<code>ConfigParser</code>","text":"<p>Methods:</p> Name Description <code>get</code> <p>Retrieve a value from the configuration.</p> <code>load</code> <p>Load data from the YAML file.</p> <code>save</code> <p>Save data to the YAML file.</p> <code>update</code> <p>Update the current config with new values.</p>"},{"location":"pages/api/io/#io.config_parser.ConfigParser.get","title":"<code>get(key, default=None)</code>","text":"<p>Retrieve a value from the configuration.</p>"},{"location":"pages/api/io/#io.config_parser.ConfigParser.load","title":"<code>load()</code>","text":"<p>Load data from the YAML file.</p>"},{"location":"pages/api/io/#io.config_parser.ConfigParser.save","title":"<code>save()</code>","text":"<p>Save data to the YAML file.</p>"},{"location":"pages/api/io/#io.config_parser.ConfigParser.update","title":"<code>update(updates)</code>","text":"<p>Update the current config with new values.</p>"},{"location":"pages/api/io/#io.cropcom","title":"<code>cropcom</code>","text":"<p>Classes:</p> Name Description <code>CropCom</code> <p>Class for handling CROPCOM.DAT file.</p>"},{"location":"pages/api/io/#io.cropcom.CropCom","title":"<code>CropCom</code>","text":"<p>Class for handling CROPCOM.DAT file.</p> <p>Methods:</p> Name Description <code>constraints</code> <p>Returns the constraints (min, max ranges) for the parameters.</p> <code>edit</code> <p>Updates the parameters in the DataFrame with new values.</p> <code>get_vars</code> <p>Returns the vars DataFrame with an additional column containing current values.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>set_sensitive</code> <p>Sets sensitive parameters based on a CSV path or list of parameter names.</p>"},{"location":"pages/api/io/#io.cropcom.CropCom.current","title":"<code>current</code>  <code>property</code>","text":"<p>Returns the current values of parameters in the DataFrame.</p>"},{"location":"pages/api/io/#io.cropcom.CropCom.constraints","title":"<code>constraints()</code>","text":"<p>Returns the constraints (min, max ranges) for the parameters.</p>"},{"location":"pages/api/io/#io.cropcom.CropCom.edit","title":"<code>edit(values)</code>","text":"<p>Updates the parameters in the DataFrame with new values.</p>"},{"location":"pages/api/io/#io.cropcom.CropCom.get_vars","title":"<code>get_vars()</code>","text":"<p>Returns the vars DataFrame with an additional column containing current values.</p>"},{"location":"pages/api/io/#io.cropcom.CropCom.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p>"},{"location":"pages/api/io/#io.cropcom.CropCom.set_sensitive","title":"<code>set_sensitive(parms_input, crop_codes, all=False)</code>","text":"<p>Sets sensitive parameters based on a CSV path or list of parameter names. If <code>all</code> is True, all parameters are considered sensitive.</p> <p>Parameters:</p> Name Type Description Default <code>parms_input</code> <code>str or list</code> <p>Either a CSV file path or list of parameter names to select</p> required <code>crop_codes</code> <code>list</code> <p>List of crop codes to apply parameters to</p> required <code>all</code> <code>bool</code> <p>If True, all parameters are considered sensitive regardless of input</p> <code>False</code>"},{"location":"pages/api/io/#io.data_logger","title":"<code>data_logger</code>","text":"<p>Classes:</p> Name Description <code>DataLogger</code> <p>A class to handle logging of data using different backends: Redis, SQL, or LMDB.</p> <code>LMDBTableWriter</code> <p>Minimal, efficient row store with auto-increment row_id.</p> <code>RedisWriter</code>"},{"location":"pages/api/io/#io.data_logger.DataLogger","title":"<code>DataLogger</code>","text":"<p>A class to handle logging of data using different backends: Redis, SQL, or LMDB. It supports logging dictionaries and retrieving logged data.</p> <p>Attributes:</p> Name Type Description <code>output_folder</code> <code>str</code> <p>Directory where files are stored (if applicable).</p> <code>delete_on_read</code> <code>bool</code> <p>Whether to delete the data after retrieving it.</p> <code>backend</code> <code>str</code> <p>The backend to use ('redis', 'sql', 'lmdb').</p> <p>Methods:</p> Name Description <code>get</code> <p>Retrieve logged data using the specified backend.</p> <code>get_writer</code> <p>Get the appropriate writer based on the backend.</p> <code>log_dict</code> <p>Log a dictionary of results using the specified backend.</p>"},{"location":"pages/api/io/#io.data_logger.DataLogger.get","title":"<code>get(func_name, keep=False)</code>","text":"<p>Retrieve logged data using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function whose data needs to be retrieved.</p> required <code>keep</code> <code>bool</code> <p>If True, do not delete the table even if delete_on_read is True.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: The DataFrame containing the logged data.</p>"},{"location":"pages/api/io/#io.data_logger.DataLogger.get_writer","title":"<code>get_writer(func_name)</code>","text":"<p>Get the appropriate writer based on the backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to create a writer for.</p> required <p>Returns:</p> Name Type Description <code>Writer</code> <p>An instance of the appropriate writer class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported backend is specified.</p>"},{"location":"pages/api/io/#io.data_logger.DataLogger.log_dict","title":"<code>log_dict(func_name, result)</code>","text":"<p>Log a dictionary of results using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to log the data for.</p> required <code>result</code> <code>dict</code> <p>Dictionary of results to log.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result is not a dictionary.</p>"},{"location":"pages/api/io/#io.data_logger.LMDBTableWriter","title":"<code>LMDBTableWriter</code>","text":"<p>Minimal, efficient row store with auto-increment row_id. - write_row(row_id=None, **kwargs) -&gt; returns the row_id (string) - read_row(row_id) -&gt; dict or None - query_rows() -&gt; DataFrame with row_id index - delete_table(), open(), close(), context manager</p> <p>Methods:</p> Name Description <code>query_rows</code> <p>Return all rows as a DataFrame with numeric-sorted row_id.</p> <code>write_row</code> <p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"pages/api/io/#io.data_logger.LMDBTableWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Return all rows as a DataFrame with numeric-sorted row_id.</p>"},{"location":"pages/api/io/#io.data_logger.LMDBTableWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"pages/api/io/#io.data_logger.RedisWriter","title":"<code>RedisWriter</code>","text":"<p>Methods:</p> Name Description <code>close</code> <p>Close the connection to Redis (flag only; redis client is pooled).</p> <code>delete_table</code> <p>Delete all entries associated with the table name, including the counter.</p> <code>open</code> <p>Establish connection to Redis and initialize counter if needed.</p> <code>query_rows</code> <p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p> <code>read_row</code> <p>Read a row from Redis hash.</p> <code>write_row</code> <p>Write a row to Redis hash under the specified table name.</p>"},{"location":"pages/api/io/#io.data_logger.RedisWriter.close","title":"<code>close()</code>","text":"<p>Close the connection to Redis (flag only; redis client is pooled).</p>"},{"location":"pages/api/io/#io.data_logger.RedisWriter.delete_table","title":"<code>delete_table()</code>","text":"<p>Delete all entries associated with the table name, including the counter.</p>"},{"location":"pages/api/io/#io.data_logger.RedisWriter.open","title":"<code>open()</code>","text":"<p>Establish connection to Redis and initialize counter if needed.</p>"},{"location":"pages/api/io/#io.data_logger.RedisWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p>"},{"location":"pages/api/io/#io.data_logger.RedisWriter.read_row","title":"<code>read_row(row_id)</code>","text":"<p>Read a row from Redis hash.</p>"},{"location":"pages/api/io/#io.data_logger.RedisWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Write a row to Redis hash under the specified table name.</p>"},{"location":"pages/api/io/#io.data_logger.lmdb_writer","title":"<code>lmdb_writer</code>","text":"<p>Classes:</p> Name Description <code>LMDBTableWriter</code> <p>Minimal, efficient row store with auto-increment row_id.</p>"},{"location":"pages/api/io/#io.data_logger.lmdb_writer.LMDBTableWriter","title":"<code>LMDBTableWriter</code>","text":"<p>Minimal, efficient row store with auto-increment row_id. - write_row(row_id=None, **kwargs) -&gt; returns the row_id (string) - read_row(row_id) -&gt; dict or None - query_rows() -&gt; DataFrame with row_id index - delete_table(), open(), close(), context manager</p> <p>Methods:</p> Name Description <code>query_rows</code> <p>Return all rows as a DataFrame with numeric-sorted row_id.</p> <code>write_row</code> <p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"pages/api/io/#io.data_logger.lmdb_writer.LMDBTableWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Return all rows as a DataFrame with numeric-sorted row_id.</p>"},{"location":"pages/api/io/#io.data_logger.lmdb_writer.LMDBTableWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"pages/api/io/#io.data_logger.main","title":"<code>main</code>","text":"<p>Classes:</p> Name Description <code>DataLogger</code> <p>A class to handle logging of data using different backends: Redis, SQL, or LMDB.</p>"},{"location":"pages/api/io/#io.data_logger.main.DataLogger","title":"<code>DataLogger</code>","text":"<p>A class to handle logging of data using different backends: Redis, SQL, or LMDB. It supports logging dictionaries and retrieving logged data.</p> <p>Attributes:</p> Name Type Description <code>output_folder</code> <code>str</code> <p>Directory where files are stored (if applicable).</p> <code>delete_on_read</code> <code>bool</code> <p>Whether to delete the data after retrieving it.</p> <code>backend</code> <code>str</code> <p>The backend to use ('redis', 'sql', 'lmdb').</p> <p>Methods:</p> Name Description <code>get</code> <p>Retrieve logged data using the specified backend.</p> <code>get_writer</code> <p>Get the appropriate writer based on the backend.</p> <code>log_dict</code> <p>Log a dictionary of results using the specified backend.</p>"},{"location":"pages/api/io/#io.data_logger.main.DataLogger.get","title":"<code>get(func_name, keep=False)</code>","text":"<p>Retrieve logged data using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function whose data needs to be retrieved.</p> required <code>keep</code> <code>bool</code> <p>If True, do not delete the table even if delete_on_read is True.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: The DataFrame containing the logged data.</p>"},{"location":"pages/api/io/#io.data_logger.main.DataLogger.get_writer","title":"<code>get_writer(func_name)</code>","text":"<p>Get the appropriate writer based on the backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to create a writer for.</p> required <p>Returns:</p> Name Type Description <code>Writer</code> <p>An instance of the appropriate writer class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported backend is specified.</p>"},{"location":"pages/api/io/#io.data_logger.main.DataLogger.log_dict","title":"<code>log_dict(func_name, result)</code>","text":"<p>Log a dictionary of results using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to log the data for.</p> required <code>result</code> <code>dict</code> <p>Dictionary of results to log.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result is not a dictionary.</p>"},{"location":"pages/api/io/#io.data_logger.redis_writer","title":"<code>redis_writer</code>","text":"<p>Classes:</p> Name Description <code>RedisWriter</code>"},{"location":"pages/api/io/#io.data_logger.redis_writer.RedisWriter","title":"<code>RedisWriter</code>","text":"<p>Methods:</p> Name Description <code>close</code> <p>Close the connection to Redis (flag only; redis client is pooled).</p> <code>delete_table</code> <p>Delete all entries associated with the table name, including the counter.</p> <code>open</code> <p>Establish connection to Redis and initialize counter if needed.</p> <code>query_rows</code> <p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p> <code>read_row</code> <p>Read a row from Redis hash.</p> <code>write_row</code> <p>Write a row to Redis hash under the specified table name.</p>"},{"location":"pages/api/io/#io.data_logger.redis_writer.RedisWriter.close","title":"<code>close()</code>","text":"<p>Close the connection to Redis (flag only; redis client is pooled).</p>"},{"location":"pages/api/io/#io.data_logger.redis_writer.RedisWriter.delete_table","title":"<code>delete_table()</code>","text":"<p>Delete all entries associated with the table name, including the counter.</p>"},{"location":"pages/api/io/#io.data_logger.redis_writer.RedisWriter.open","title":"<code>open()</code>","text":"<p>Establish connection to Redis and initialize counter if needed.</p>"},{"location":"pages/api/io/#io.data_logger.redis_writer.RedisWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p>"},{"location":"pages/api/io/#io.data_logger.redis_writer.RedisWriter.read_row","title":"<code>read_row(row_id)</code>","text":"<p>Read a row from Redis hash.</p>"},{"location":"pages/api/io/#io.data_logger.redis_writer.RedisWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Write a row to Redis hash under the specified table name.</p>"},{"location":"pages/api/io/#io.inputs","title":"<code>inputs</code>","text":"<p>Classes:</p> Name Description <code>DLY</code> <code>OPC</code> <code>SIT</code> <code>SOL</code>"},{"location":"pages/api/io/#io.inputs.DLY","title":"<code>DLY</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>load</code> <p>Load data from a DLY file into DataFrame.</p> <code>save</code> <p>Save DataFrame into a DLY file.</p> <code>to_monthly</code> <p>Save as monthly file</p> <code>validate</code> <p>Validate the DataFrame to ensure it contains a continuous range of dates </p>"},{"location":"pages/api/io/#io.inputs.DLY.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load data from a DLY file into DataFrame.</p>"},{"location":"pages/api/io/#io.inputs.DLY.save","title":"<code>save(path=None)</code>","text":"<p>Save DataFrame into a DLY file.</p>"},{"location":"pages/api/io/#io.inputs.DLY.to_monthly","title":"<code>to_monthly(path=None)</code>","text":"<p>Save as monthly file</p>"},{"location":"pages/api/io/#io.inputs.DLY.validate","title":"<code>validate(start_date, end_date)</code>","text":"<p>Validate the DataFrame to ensure it contains a continuous range of dates  between start_date and end_date, without duplicates.</p>"},{"location":"pages/api/io/#io.inputs.OPC","title":"<code>OPC</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>append</code> <p>Append another OPC or DataFrame to the current OPC instance.</p> <code>edit_crop_season</code> <p>Edit the planting and/or harvest dates for a given year and crop.</p> <code>edit_fertilizer_rate</code> <p>Edit the fertilizer rate for a given year.</p> <code>edit_harvest_date</code> <p>Edit the harvest date for a given year and crop.</p> <code>edit_operation_date</code> <p>Edit the operation date for a given year.</p> <code>edit_operation_value</code> <p>Edit the operation value for a given year.</p> <code>edit_plantation_date</code> <p>Edit the plantation date for a given year and crop.</p> <code>get_harvest_date</code> <p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <code>get_plantation_date</code> <p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <code>iter_seasons</code> <p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <code>load</code> <p>Load data from an OPC file into DataFrame.</p> <code>new</code> <p>Create a new OPC instance with an empty DataFrame with preset columns, a default header,</p> <code>remove</code> <p>Remove operation(s) from the OPC file that match all provided criteria.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>update</code> <p>Add or update an operation in the OPC file.</p> <code>update_phu</code> <p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <code>validate</code> <p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p>"},{"location":"pages/api/io/#io.inputs.OPC.IAUI","title":"<code>IAUI</code>  <code>property</code> <code>writable</code>","text":"<p>Get the auto-irrigation implement ID from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if auto-irrigation is enabled (72), False if disabled (0)</p>"},{"location":"pages/api/io/#io.inputs.OPC.LUN","title":"<code>LUN</code>  <code>property</code> <code>writable</code>","text":"<p>Get the land use number from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The land use number from the first 4 characters of header line 2</p>"},{"location":"pages/api/io/#io.inputs.OPC.append","title":"<code>append(second_opc)</code>","text":"<p>Append another OPC or DataFrame to the current OPC instance. Args:     second_opc (pd.DataFrame or OPC): The data to append. Returns:     OPC: A new OPC instance with combined data. Raises:     ValueError: If second_opc is not a pandas DataFrame or OPC instance.</p>"},{"location":"pages/api/io/#io.inputs.OPC.edit_crop_season","title":"<code>edit_crop_season(new_planting_date=None, new_harvest_date=None, crop_code=None)</code>","text":"<p>Edit the planting and/or harvest dates for a given year and crop.</p> <p>Parameters: year (int): Year. new_planting_date (datetime, optional): New planting date. If not provided, only harvest date will be updated. new_harvest_date (datetime, optional): New harvest date. If not provided, only planting date will be updated. crop_code (int, optional): Crop code. If not provided, changes the first crop found.</p>"},{"location":"pages/api/io/#io.inputs.OPC.edit_fertilizer_rate","title":"<code>edit_fertilizer_rate(rate, year=2020, month=None, day=None)</code>","text":"<p>Edit the fertilizer rate for a given year.</p> <p>Parameters: rate (float): Fertilizer rate to be set. year (int, optional): Year for the fertilizer rate application. Defaults to 2020. month (int, optional): Month for the fertilizer rate application. If not provided, the first instance is changed. day (int, optional): Day for the fertilizer rate application. Defaults to None.</p>"},{"location":"pages/api/io/#io.inputs.OPC.edit_harvest_date","title":"<code>edit_harvest_date(date, crop_code)</code>","text":"<p>Edit the harvest date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of harvest in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"pages/api/io/#io.inputs.OPC.edit_operation_date","title":"<code>edit_operation_date(code, year, month, day, crop_code=None)</code>","text":"<p>Edit the operation date for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. month (int): Month of operation. day (int): Day of operation. crop_code (int, optional): Crop code.</p>"},{"location":"pages/api/io/#io.inputs.OPC.edit_operation_value","title":"<code>edit_operation_value(code, year, value, crop_code=None)</code>","text":"<p>Edit the operation value for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. value (float): New operation value. crop_code (int, optional): Crop code.</p>"},{"location":"pages/api/io/#io.inputs.OPC.edit_plantation_date","title":"<code>edit_plantation_date(date, crop_code)</code>","text":"<p>Edit the plantation date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of plantation in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"pages/api/io/#io.inputs.OPC.get_harvest_date","title":"<code>get_harvest_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their harvest dates with row indices.</p>"},{"location":"pages/api/io/#io.inputs.OPC.get_plantation_date","title":"<code>get_plantation_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their plantation dates with row indices.</p>"},{"location":"pages/api/io/#io.inputs.OPC.iter_seasons","title":"<code>iter_seasons(start_year=None, end_year=None)</code>","text":"<p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <p>Parameters: start_year (int, optional): The starting year to consider. Defaults to None. end_year (int, optional): The ending year to consider. Defaults to None.</p> <p>dict: A dictionary containing:     - plantation_date: The date of plantation     - harvest_date: The date of harvest     - crop_code: The crop code     - operations: A subset of OPC rows for this season     - plantation_index: The index of the plantation row</p>"},{"location":"pages/api/io/#io.inputs.OPC.load","title":"<code>load(path, start_year=None)</code>  <code>classmethod</code>","text":"<p>Load data from an OPC file into DataFrame.</p> <p>Parameters: path (str): Path to the OPC file. start_year (int, optional): Start year for the OPC file. If not provided, it will be read from the file header.</p> <p>Returns: OPC: An instance of the OPC class containing the loaded data.</p>"},{"location":"pages/api/io/#io.inputs.OPC.new","title":"<code>new(name, start_year)</code>  <code>classmethod</code>","text":"<p>Create a new OPC instance with an empty DataFrame with preset columns, a default header, and provided name and start year.</p> <p>Parameters: name (str): The name for this OPC file/instance. start_year (int): The start year to assign in the OPC instance.</p> <p>Returns: OPC: An OPC instance with no data but with the required metadata set.</p>"},{"location":"pages/api/io/#io.inputs.OPC.remove","title":"<code>remove(opID=None, date=None, cropID=None, XMTU=None, fertID=None, year=None)</code>","text":"<p>Remove operation(s) from the OPC file that match all provided criteria.</p> <p>Parameters:</p> Name Type Description Default <code>opID</code> <code>int</code> <p>Operation ID to match</p> <code>None</code> <code>date</code> <code>str</code> <p>Date to match in format 'YYYY-MM-DD'</p> <code>None</code> <code>cropID</code> <code>int</code> <p>Crop ID to match</p> <code>None</code> <code>XMTU/LYR/pestID/fertID</code> <code>int</code> <p>Machine type/layer/pesticide ID/fertilizer ID to match</p> required <code>year</code> <code>int</code> <p>Year to match</p> <code>None</code>"},{"location":"pages/api/io/#io.inputs.OPC.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p> <p>Parameters: path (str): Path to save the OPC file.</p>"},{"location":"pages/api/io/#io.inputs.OPC.update","title":"<code>update(operation)</code>","text":"<p>Add or update an operation in the OPC file.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Dictionary containing operation details with keys: - opID: Operation ID (required) - cropID: Crop ID (required) - date: Operation date as string 'YYYY-MM-DD' (required) - XMTU/LYR/pestID/fertID: Machine type/years/pesticide ID/fertilizer ID (optional, default 0) - OPV1-OPV8: Additional operation values (optional, default 0)</p> required"},{"location":"pages/api/io/#io.inputs.OPC.update_phu","title":"<code>update_phu(dly, cropcom)</code>","text":"<p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <p>Parameters: dly (DLY): DLY object containing weather data. cropcom (DataFrame): DataFrame containing crop code and TBS values.</p>"},{"location":"pages/api/io/#io.inputs.OPC.validate","title":"<code>validate(duration=None)</code>","text":"<p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p> <p>Parameters: duration (int, optional): Duration of the simulation in years. If None, uses the maximum Yid.</p> <p>Returns: bool: True if the data is valid, False otherwise.</p>"},{"location":"pages/api/io/#io.inputs.SIT","title":"<code>SIT</code>","text":"<p>Methods:</p> Name Description <code>load</code> <p>Class method to load the .sit file and return a SiteFile instance.</p> <code>save</code> <p>Save the current site information to a .SIT file.</p>"},{"location":"pages/api/io/#io.inputs.SIT.elevation","title":"<code>elevation</code>  <code>property</code> <code>writable</code>","text":"<p>Get elevation value.</p>"},{"location":"pages/api/io/#io.inputs.SIT.lat","title":"<code>lat</code>  <code>property</code> <code>writable</code>","text":"<p>Get latitude value.</p>"},{"location":"pages/api/io/#io.inputs.SIT.lon","title":"<code>lon</code>  <code>property</code> <code>writable</code>","text":"<p>Get longitude value.</p>"},{"location":"pages/api/io/#io.inputs.SIT.slope","title":"<code>slope</code>  <code>property</code> <code>writable</code>","text":"<p>Get slope steep value.</p>"},{"location":"pages/api/io/#io.inputs.SIT.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Class method to load the .sit file and return a SiteFile instance.</p> <p>Parameters: file_path (str): Path to the .sit file.</p> <p>Returns: SiteFile: An instance of the SiteFile class with loaded data.</p>"},{"location":"pages/api/io/#io.inputs.SIT.save","title":"<code>save(output_dir)</code>","text":"<p>Save the current site information to a .SIT file.</p>"},{"location":"pages/api/io/#io.inputs.SOL","title":"<code>SOL</code>","text":"<p>Methods:</p> Name Description <code>from_sda</code> <p>Create a Soil object from Soil Data Access using a query.</p> <code>load</code> <p>Load soil data from a file and return a Soil object.</p> <code>save</code> <p>Save the soil data to a file using a template.</p>"},{"location":"pages/api/io/#io.inputs.SOL.from_sda","title":"<code>from_sda(query)</code>  <code>classmethod</code>","text":"<p>Create a Soil object from Soil Data Access using a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>int or str</code> <p>Query string for SoilDataAccess. (mukey or WKT str) ( \"POINT(-123.4567 45.6789)\" )</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from SDA.</p>"},{"location":"pages/api/io/#io.inputs.SOL.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Load soil data from a file and return a Soil object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the soil file.</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from the file.</p>"},{"location":"pages/api/io/#io.inputs.SOL.save","title":"<code>save(filepath, template=None)</code>","text":"<p>Save the soil data to a file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the soil file.</p> required <code>template</code> <code>list</code> <p>Optional list of template lines.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If soil properties DataFrame is empty.</p>"},{"location":"pages/api/io/#io.inputs.dly","title":"<code>dly</code>","text":"<p>Classes:</p> Name Description <code>DLY</code>"},{"location":"pages/api/io/#io.inputs.dly.DLY","title":"<code>DLY</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>load</code> <p>Load data from a DLY file into DataFrame.</p> <code>save</code> <p>Save DataFrame into a DLY file.</p> <code>to_monthly</code> <p>Save as monthly file</p> <code>validate</code> <p>Validate the DataFrame to ensure it contains a continuous range of dates </p>"},{"location":"pages/api/io/#io.inputs.dly.DLY.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load data from a DLY file into DataFrame.</p>"},{"location":"pages/api/io/#io.inputs.dly.DLY.save","title":"<code>save(path=None)</code>","text":"<p>Save DataFrame into a DLY file.</p>"},{"location":"pages/api/io/#io.inputs.dly.DLY.to_monthly","title":"<code>to_monthly(path=None)</code>","text":"<p>Save as monthly file</p>"},{"location":"pages/api/io/#io.inputs.dly.DLY.validate","title":"<code>validate(start_date, end_date)</code>","text":"<p>Validate the DataFrame to ensure it contains a continuous range of dates  between start_date and end_date, without duplicates.</p>"},{"location":"pages/api/io/#io.inputs.opc","title":"<code>opc</code>","text":"<p>Classes:</p> Name Description <code>OPC</code>"},{"location":"pages/api/io/#io.inputs.opc.OPC","title":"<code>OPC</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>append</code> <p>Append another OPC or DataFrame to the current OPC instance.</p> <code>edit_crop_season</code> <p>Edit the planting and/or harvest dates for a given year and crop.</p> <code>edit_fertilizer_rate</code> <p>Edit the fertilizer rate for a given year.</p> <code>edit_harvest_date</code> <p>Edit the harvest date for a given year and crop.</p> <code>edit_operation_date</code> <p>Edit the operation date for a given year.</p> <code>edit_operation_value</code> <p>Edit the operation value for a given year.</p> <code>edit_plantation_date</code> <p>Edit the plantation date for a given year and crop.</p> <code>get_harvest_date</code> <p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <code>get_plantation_date</code> <p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <code>iter_seasons</code> <p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <code>load</code> <p>Load data from an OPC file into DataFrame.</p> <code>new</code> <p>Create a new OPC instance with an empty DataFrame with preset columns, a default header,</p> <code>remove</code> <p>Remove operation(s) from the OPC file that match all provided criteria.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>update</code> <p>Add or update an operation in the OPC file.</p> <code>update_phu</code> <p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <code>validate</code> <p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.IAUI","title":"<code>IAUI</code>  <code>property</code> <code>writable</code>","text":"<p>Get the auto-irrigation implement ID from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if auto-irrigation is enabled (72), False if disabled (0)</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.LUN","title":"<code>LUN</code>  <code>property</code> <code>writable</code>","text":"<p>Get the land use number from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The land use number from the first 4 characters of header line 2</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.append","title":"<code>append(second_opc)</code>","text":"<p>Append another OPC or DataFrame to the current OPC instance. Args:     second_opc (pd.DataFrame or OPC): The data to append. Returns:     OPC: A new OPC instance with combined data. Raises:     ValueError: If second_opc is not a pandas DataFrame or OPC instance.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.edit_crop_season","title":"<code>edit_crop_season(new_planting_date=None, new_harvest_date=None, crop_code=None)</code>","text":"<p>Edit the planting and/or harvest dates for a given year and crop.</p> <p>Parameters: year (int): Year. new_planting_date (datetime, optional): New planting date. If not provided, only harvest date will be updated. new_harvest_date (datetime, optional): New harvest date. If not provided, only planting date will be updated. crop_code (int, optional): Crop code. If not provided, changes the first crop found.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.edit_fertilizer_rate","title":"<code>edit_fertilizer_rate(rate, year=2020, month=None, day=None)</code>","text":"<p>Edit the fertilizer rate for a given year.</p> <p>Parameters: rate (float): Fertilizer rate to be set. year (int, optional): Year for the fertilizer rate application. Defaults to 2020. month (int, optional): Month for the fertilizer rate application. If not provided, the first instance is changed. day (int, optional): Day for the fertilizer rate application. Defaults to None.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.edit_harvest_date","title":"<code>edit_harvest_date(date, crop_code)</code>","text":"<p>Edit the harvest date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of harvest in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.edit_operation_date","title":"<code>edit_operation_date(code, year, month, day, crop_code=None)</code>","text":"<p>Edit the operation date for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. month (int): Month of operation. day (int): Day of operation. crop_code (int, optional): Crop code.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.edit_operation_value","title":"<code>edit_operation_value(code, year, value, crop_code=None)</code>","text":"<p>Edit the operation value for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. value (float): New operation value. crop_code (int, optional): Crop code.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.edit_plantation_date","title":"<code>edit_plantation_date(date, crop_code)</code>","text":"<p>Edit the plantation date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of plantation in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.get_harvest_date","title":"<code>get_harvest_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their harvest dates with row indices.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.get_plantation_date","title":"<code>get_plantation_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their plantation dates with row indices.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.iter_seasons","title":"<code>iter_seasons(start_year=None, end_year=None)</code>","text":"<p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <p>Parameters: start_year (int, optional): The starting year to consider. Defaults to None. end_year (int, optional): The ending year to consider. Defaults to None.</p> <p>dict: A dictionary containing:     - plantation_date: The date of plantation     - harvest_date: The date of harvest     - crop_code: The crop code     - operations: A subset of OPC rows for this season     - plantation_index: The index of the plantation row</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.load","title":"<code>load(path, start_year=None)</code>  <code>classmethod</code>","text":"<p>Load data from an OPC file into DataFrame.</p> <p>Parameters: path (str): Path to the OPC file. start_year (int, optional): Start year for the OPC file. If not provided, it will be read from the file header.</p> <p>Returns: OPC: An instance of the OPC class containing the loaded data.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.new","title":"<code>new(name, start_year)</code>  <code>classmethod</code>","text":"<p>Create a new OPC instance with an empty DataFrame with preset columns, a default header, and provided name and start year.</p> <p>Parameters: name (str): The name for this OPC file/instance. start_year (int): The start year to assign in the OPC instance.</p> <p>Returns: OPC: An OPC instance with no data but with the required metadata set.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.remove","title":"<code>remove(opID=None, date=None, cropID=None, XMTU=None, fertID=None, year=None)</code>","text":"<p>Remove operation(s) from the OPC file that match all provided criteria.</p> <p>Parameters:</p> Name Type Description Default <code>opID</code> <code>int</code> <p>Operation ID to match</p> <code>None</code> <code>date</code> <code>str</code> <p>Date to match in format 'YYYY-MM-DD'</p> <code>None</code> <code>cropID</code> <code>int</code> <p>Crop ID to match</p> <code>None</code> <code>XMTU/LYR/pestID/fertID</code> <code>int</code> <p>Machine type/layer/pesticide ID/fertilizer ID to match</p> required <code>year</code> <code>int</code> <p>Year to match</p> <code>None</code>"},{"location":"pages/api/io/#io.inputs.opc.OPC.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p> <p>Parameters: path (str): Path to save the OPC file.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.update","title":"<code>update(operation)</code>","text":"<p>Add or update an operation in the OPC file.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Dictionary containing operation details with keys: - opID: Operation ID (required) - cropID: Crop ID (required) - date: Operation date as string 'YYYY-MM-DD' (required) - XMTU/LYR/pestID/fertID: Machine type/years/pesticide ID/fertilizer ID (optional, default 0) - OPV1-OPV8: Additional operation values (optional, default 0)</p> required"},{"location":"pages/api/io/#io.inputs.opc.OPC.update_phu","title":"<code>update_phu(dly, cropcom)</code>","text":"<p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <p>Parameters: dly (DLY): DLY object containing weather data. cropcom (DataFrame): DataFrame containing crop code and TBS values.</p>"},{"location":"pages/api/io/#io.inputs.opc.OPC.validate","title":"<code>validate(duration=None)</code>","text":"<p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p> <p>Parameters: duration (int, optional): Duration of the simulation in years. If None, uses the maximum Yid.</p> <p>Returns: bool: True if the data is valid, False otherwise.</p>"},{"location":"pages/api/io/#io.inputs.sit","title":"<code>sit</code>","text":"<p>Classes:</p> Name Description <code>SIT</code>"},{"location":"pages/api/io/#io.inputs.sit.SIT","title":"<code>SIT</code>","text":"<p>Methods:</p> Name Description <code>load</code> <p>Class method to load the .sit file and return a SiteFile instance.</p> <code>save</code> <p>Save the current site information to a .SIT file.</p>"},{"location":"pages/api/io/#io.inputs.sit.SIT.elevation","title":"<code>elevation</code>  <code>property</code> <code>writable</code>","text":"<p>Get elevation value.</p>"},{"location":"pages/api/io/#io.inputs.sit.SIT.lat","title":"<code>lat</code>  <code>property</code> <code>writable</code>","text":"<p>Get latitude value.</p>"},{"location":"pages/api/io/#io.inputs.sit.SIT.lon","title":"<code>lon</code>  <code>property</code> <code>writable</code>","text":"<p>Get longitude value.</p>"},{"location":"pages/api/io/#io.inputs.sit.SIT.slope","title":"<code>slope</code>  <code>property</code> <code>writable</code>","text":"<p>Get slope steep value.</p>"},{"location":"pages/api/io/#io.inputs.sit.SIT.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Class method to load the .sit file and return a SiteFile instance.</p> <p>Parameters: file_path (str): Path to the .sit file.</p> <p>Returns: SiteFile: An instance of the SiteFile class with loaded data.</p>"},{"location":"pages/api/io/#io.inputs.sit.SIT.save","title":"<code>save(output_dir)</code>","text":"<p>Save the current site information to a .SIT file.</p>"},{"location":"pages/api/io/#io.inputs.sol","title":"<code>sol</code>","text":"<p>Classes:</p> Name Description <code>SOL</code>"},{"location":"pages/api/io/#io.inputs.sol.SOL","title":"<code>SOL</code>","text":"<p>Methods:</p> Name Description <code>from_sda</code> <p>Create a Soil object from Soil Data Access using a query.</p> <code>load</code> <p>Load soil data from a file and return a Soil object.</p> <code>save</code> <p>Save the soil data to a file using a template.</p>"},{"location":"pages/api/io/#io.inputs.sol.SOL.from_sda","title":"<code>from_sda(query)</code>  <code>classmethod</code>","text":"<p>Create a Soil object from Soil Data Access using a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>int or str</code> <p>Query string for SoilDataAccess. (mukey or WKT str) ( \"POINT(-123.4567 45.6789)\" )</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from SDA.</p>"},{"location":"pages/api/io/#io.inputs.sol.SOL.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Load soil data from a file and return a Soil object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the soil file.</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from the file.</p>"},{"location":"pages/api/io/#io.inputs.sol.SOL.save","title":"<code>save(filepath, template=None)</code>","text":"<p>Save the soil data to a file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the soil file.</p> required <code>template</code> <code>list</code> <p>Optional list of template lines.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If soil properties DataFrame is empty.</p>"},{"location":"pages/api/io/#io.outputs","title":"<code>outputs</code>","text":"<p>Classes:</p> Name Description <code>ACY</code> <code>DGN</code> <code>DSL</code> <code>DWC</code>"},{"location":"pages/api/io/#io.outputs.ACY","title":"<code>ACY</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the ACY data.</p>"},{"location":"pages/api/io/#io.outputs.ACY.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the ACY data.</p>"},{"location":"pages/api/io/#io.outputs.DGN","title":"<code>DGN</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DGN data.</p>"},{"location":"pages/api/io/#io.outputs.DGN.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DGN data.</p>"},{"location":"pages/api/io/#io.outputs.DSL","title":"<code>DSL</code>","text":"<p>Methods:</p> Name Description <code>get_data</code> <p>Return stored water data.</p>"},{"location":"pages/api/io/#io.outputs.DSL.get_data","title":"<code>get_data()</code>","text":"<p>Return stored water data.</p>"},{"location":"pages/api/io/#io.outputs.DWC","title":"<code>DWC</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DWC data.</p>"},{"location":"pages/api/io/#io.outputs.DWC.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DWC data.</p>"},{"location":"pages/api/io/#io.outputs.all","title":"<code>all</code>","text":"<p>Classes:</p> Name Description <code>ACY</code> <code>DGN</code> <code>DWC</code>"},{"location":"pages/api/io/#io.outputs.all.ACY","title":"<code>ACY</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the ACY data.</p>"},{"location":"pages/api/io/#io.outputs.all.ACY.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the ACY data.</p>"},{"location":"pages/api/io/#io.outputs.all.DGN","title":"<code>DGN</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DGN data.</p>"},{"location":"pages/api/io/#io.outputs.all.DGN.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DGN data.</p>"},{"location":"pages/api/io/#io.outputs.all.DWC","title":"<code>DWC</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DWC data.</p>"},{"location":"pages/api/io/#io.outputs.all.DWC.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DWC data.</p>"},{"location":"pages/api/io/#io.outputs.sw","title":"<code>sw</code>","text":"<p>Classes:</p> Name Description <code>DSL</code>"},{"location":"pages/api/io/#io.outputs.sw.DSL","title":"<code>DSL</code>","text":"<p>Methods:</p> Name Description <code>get_data</code> <p>Return stored water data.</p>"},{"location":"pages/api/io/#io.outputs.sw.DSL.get_data","title":"<code>get_data()</code>","text":"<p>Return stored water data.</p>"},{"location":"pages/api/io/#io.parm","title":"<code>parm</code>","text":"<p>Classes:</p> Name Description <code>Parm</code>"},{"location":"pages/api/io/#io.parm.Parm","title":"<code>Parm</code>","text":"<p>Methods:</p> Name Description <code>constraints</code> <p>Returns the constraints (min, max ranges) for the parameters.</p> <code>edit</code> <p>Updates the parameters in the DataFrame with new values.</p> <code>get_vars</code> <p>Returns the vars DataFrame with an additional column containing current values.</p> <code>read_parm</code> <p>Reads and constructs a DataFrame from a .DAT file.</p> <code>save</code> <p>Saves the current DataFrame to a .DAT file.</p> <code>set_sensitive</code> <p>Sets sensitive parameters based on a CSV path or list of parameter names.</p>"},{"location":"pages/api/io/#io.parm.Parm.current","title":"<code>current</code>  <code>property</code>","text":"<p>Returns the current values of parameters in the DataFrame.</p>"},{"location":"pages/api/io/#io.parm.Parm.constraints","title":"<code>constraints()</code>","text":"<p>Returns the constraints (min, max ranges) for the parameters.</p>"},{"location":"pages/api/io/#io.parm.Parm.edit","title":"<code>edit(values)</code>","text":"<p>Updates the parameters in the DataFrame with new values.</p>"},{"location":"pages/api/io/#io.parm.Parm.get_vars","title":"<code>get_vars()</code>","text":"<p>Returns the vars DataFrame with an additional column containing current values.</p>"},{"location":"pages/api/io/#io.parm.Parm.read_parm","title":"<code>read_parm(file_name)</code>","text":"<p>Reads and constructs a DataFrame from a .DAT file.</p>"},{"location":"pages/api/io/#io.parm.Parm.save","title":"<code>save(path)</code>","text":"<p>Saves the current DataFrame to a .DAT file.</p>"},{"location":"pages/api/io/#io.parm.Parm.set_sensitive","title":"<code>set_sensitive(parms_input, all=False)</code>","text":"<p>Sets sensitive parameters based on a CSV path or list of parameter names. If <code>all</code> is True, all parameters are considered sensitive.</p> <p>Parameters:</p> Name Type Description Default <code>parms_input</code> <code>str or list</code> <p>Either a CSV file path or list of parameter names to select</p> required <code>all</code> <code>bool</code> <p>If True, all parameters are considered sensitive regardless of input</p> <code>False</code>"},{"location":"pages/api/soil/","title":"Soil Module","text":""},{"location":"pages/api/soil/#soil","title":"<code>soil</code>","text":"<p>Classes:</p> Name Description <code>SoilDataAccess</code>"},{"location":"pages/api/soil/#soil.SoilDataAccess","title":"<code>SoilDataAccess</code>","text":"<p>Methods:</p> Name Description <code>fetch_properties</code> <p>Fetches soil data based on the input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <code>fetch_slope_length</code> <p>Fetches the slope length for a given input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <code>fetch_value</code> <p>Fetches specific values from a given table for a given input value.</p> <code>get_cokey_from_wkt</code> <p>Fetches a list of cokey (Component Key) values based on a WKT spatial location.</p> <code>get_mukey</code> <p>Fetches the mukey for a given WKT location.</p> <code>get_mukey_list</code> <p>Fetches the mukey for a given WKT location.</p> <code>query</code> <p>Performs a query to the NRCS Soil Data Access service.</p>"},{"location":"pages/api/soil/#soil.SoilDataAccess.fetch_properties","title":"<code>fetch_properties(input_value)</code>  <code>staticmethod</code>","text":"<p>Fetches soil data based on the input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <p>Args: input (int or str): The input value representing either a mukey (int) or a WKT location (str).</p> <p>Returns: pd.DataFrame: A DataFrame containing the soil data for the specified input.</p>"},{"location":"pages/api/soil/#soil.SoilDataAccess.fetch_slope_length","title":"<code>fetch_slope_length(input_value)</code>  <code>staticmethod</code>","text":"<p>Fetches the slope length for a given input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <p>Args: input (int or str): The input value representing either a mukey (int) or a WKT location (str).</p> <p>Returns: float: The slope length for the specified input value.</p>"},{"location":"pages/api/soil/#soil.SoilDataAccess.fetch_value","title":"<code>fetch_value(input_value, values, table)</code>  <code>staticmethod</code>","text":"<p>Fetches specific values from a given table for a given input value.</p> <p>Args: input_value (int or str): The input value representing either a mukey (int) or a WKT location (str). values (List[str]): A list of column names for the values to fetch. table (str): The name of the table to query.</p> <p>Returns: List[Dict[str, Any]]: A list of dictionaries containing the key (mukey, cokey, etc.) and fetched values.</p>"},{"location":"pages/api/soil/#soil.SoilDataAccess.get_cokey_from_wkt","title":"<code>get_cokey_from_wkt(wkt_string)</code>  <code>staticmethod</code>","text":"<p>Fetches a list of cokey (Component Key) values based on a WKT spatial location.</p> <p>Args: wkt_string (str): A WKT string representing the spatial location (e.g., POLYGON, POINT).</p> <p>Returns: List[int]: A list of cokey values corresponding to the input WKT location.</p>"},{"location":"pages/api/soil/#soil.SoilDataAccess.get_mukey","title":"<code>get_mukey(wkt)</code>  <code>staticmethod</code>","text":"<p>Fetches the mukey for a given WKT location.</p> <p>Args: wkt (str): The WKT location.</p> <p>Returns: int: The mukey for the specified location.</p>"},{"location":"pages/api/soil/#soil.SoilDataAccess.get_mukey_list","title":"<code>get_mukey_list(wkt)</code>  <code>staticmethod</code>","text":"<p>Fetches the mukey for a given WKT location.</p> <p>Args: wkt (str): The WKT location.</p> <p>Returns: int: The mukey for the specified location.</p>"},{"location":"pages/api/soil/#soil.SoilDataAccess.query","title":"<code>query(query)</code>  <code>staticmethod</code>","text":"<p>Performs a query to the NRCS Soil Data Access service.</p> <p>Args: query (str): The SQL query to execute.</p> <p>Returns: pd.DataFrame: A DataFrame containing the results of the query.</p> <p>Raises: ValueError: If no data is found for the provided query. requests.RequestException: If there is an issue with the network request.</p>"},{"location":"pages/api/utils/","title":"Utils Module","text":""},{"location":"pages/api/utils/#utils","title":"<code>utils</code>","text":"<p>Classes:</p> Name Description <code>GeoInterface</code> <p>A class that provides a unified interface for working with geospatial data from various sources.</p> <p>Functions:</p> Name Description <code>sample_raster_nearest</code> <p>Sample a raster file at specific coordinates, taking the nearest pixel.</p> <code>reproject_crop_raster</code> <p>Reproject and crop a raster file.</p> <code>copy_file</code> <p>Copy a file from source to destination, optionally creating a symbolic link instead.</p> <code>parallel_executor</code> <p>Pebble-only parallel executor.</p> <code>read_gdb_layer</code> <p>Reads selected columns from a GDB layer and returns them in a pandas DataFrame.</p>"},{"location":"pages/api/utils/#utils.GeoInterface","title":"<code>GeoInterface</code>","text":"<p>A class that provides a unified interface for working with geospatial data from various sources.</p> <p>This class can load and process data from raster files (.tif/.tiff), CSV files, shapefiles, or pandas DataFrames. It provides methods for finding nearest neighbors using haversine distances between geographic coordinates.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The loaded data, containing at minimum 'lat' and 'lon' columns.</p> <code>points_rad</code> <code>ndarray</code> <p>The latitude/longitude points converted to radians.</p> <code>tree</code> <code>BallTree</code> <p>A BallTree structure for efficient nearest neighbor queries.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>str or DataFrame</code> <p>The input data source. Can be: - Path to a raster file (.tif/.tiff) - Path to a CSV file (.csv) - Path to a shapefile (.shp) - A pandas DataFrame with 'lat' and 'lon' columns</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data source format is unsupported or required columns are missing.</p> <p>Methods:</p> Name Description <code>find_nearest</code> <p>Find the nearest 'k' data points for each latitude and longitude provided separately.</p> <code>lookup</code> <p>Find the nearest data point to a single latitude and longitude.</p>"},{"location":"pages/api/utils/#utils.GeoInterface.find_nearest","title":"<code>find_nearest(lats, lons, k=1)</code>","text":"<p>Find the nearest 'k' data points for each latitude and longitude provided separately.</p> <p>Parameters:</p> Name Type Description Default <code>lats</code> <code>list of float</code> <p>A list of latitudes.</p> required <code>lons</code> <code>list of float</code> <p>A list of longitudes.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors to find.</p> <code>1</code> <p>Returns:</p> Type Description <p>list or pandas.DataFrame: Depending on 'k', returns a DataFrame or a list of DataFrames with the nearest points.</p>"},{"location":"pages/api/utils/#utils.GeoInterface.lookup","title":"<code>lookup(lat, lon)</code>","text":"<p>Find the nearest data point to a single latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of the query point.</p> required <code>lon</code> <code>float</code> <p>Longitude of the query point.</p> required <p>Returns:</p> Type Description <p>pandas.Series: The row from the DataFrame corresponding to the nearest point.</p>"},{"location":"pages/api/utils/#utils.sample_raster_nearest","title":"<code>sample_raster_nearest(raster_file, coords, crs='EPSG:4326')</code>","text":"<p>Sample a raster file at specific coordinates, taking the nearest pixel.</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>str</code> <p>Path to the raster file.</p> required <code>coords</code> <code>list of tuples</code> <p>List of (x, y)/(lon, lat) tuples.</p> required <code>crs</code> <code>str</code> <p>The CRS the coords are in.</p> <code>'EPSG:4326'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with band names as keys and lists of pixel values at the given coordinates as values.</p>"},{"location":"pages/api/utils/#utils.reproject_crop_raster","title":"<code>reproject_crop_raster(src, dst, out_epsg, min_coords, max_coords)</code>","text":"<p>Reproject and crop a raster file. src_filename: Source file path. dst_filename: Destination file path. out_epsg: Output coordinate system as EPSG code. min_lon, min_lat, max_lon, max_lat: Bounding box coordinates.</p>"},{"location":"pages/api/utils/#utils.copy_file","title":"<code>copy_file(src, dest, symlink=False)</code>","text":"<p>Copy a file from source to destination, optionally creating a symbolic link instead.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>Path to the source file</p> required <code>dest</code> <code>str</code> <p>Path to the destination file/link</p> required <code>symlink</code> <code>bool</code> <p>Whether to create a symbolic link instead of copying.  Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>str | None: Path to the destination file if successful, None if source doesn't exist</p> Note <p>If symlink is True and the destination already exists, it will be removed first.</p>"},{"location":"pages/api/utils/#utils.parallel_executor","title":"<code>parallel_executor(func, args, method='Process', max_workers=10, return_value=False, bar=True, timeout=None, verbose=True)</code>","text":"<p>Pebble-only parallel executor. - method: 'Process' or 'Thread' - args: iterable of items; each item can be a single arg or a tuple for *args - timeout: per-task timeout (sec). For ProcessPool it is enforced at schedule(),            for ThreadPool it's enforced at result(). Returns: (results, failed_indices)</p>"},{"location":"pages/api/utils/#utils.read_gdb_layer","title":"<code>read_gdb_layer(gdb_data, layer_name, columns=None, names=None)</code>","text":"<p>Reads selected columns from a GDB layer and returns them in a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>gdb</code> <code>gdb</code> <p>The GDB file opened by ogr.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer to read.</p> required <code>columns</code> <code>list</code> <p>List of column indices to read. If None, all columns are read.</p> <code>None</code> <code>names</code> <code>list</code> <p>List of column names corresponding to the indices in <code>columns</code>. If None, all column names are inferred from the layer definition.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: The resulting dataframe.</p>"},{"location":"pages/api/weather/","title":"Weather Module","text":""},{"location":"pages/api/weather/#weather","title":"<code>weather</code>","text":"<p>Functions:</p> Name Description <code>fetch_list</code> <p>Fetches weather data based on the input type which could be coordinates, a CSV file, or a shapefile.</p>"},{"location":"pages/api/weather/#weather.fetch_list","title":"<code>fetch_list(config_file, input_data, output_dir, raw=False)</code>","text":"<p>Fetches weather data based on the input type which could be coordinates, a CSV file, or a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>str</code> <p>Could be latitude and longitude as a string, path to a CSV file, or path to a shapefile.</p> required <code>output_dir</code> <code>str</code> <p>Directory or file path where the output should be saved.</p> required <code>raw</code> <code>bool</code> <p>Whether to save as raw CSV (True) or DLY format (False). Defaults to False.</p> <code>False</code>"},{"location":"pages/getting_started/Weather/","title":"Weather Module","text":"<p>Weather data is vital in providing essential environmental inputs that significantly influence crop growth and development. Reliable weather inputs ensure that agricultural simulations reflect realistic responses to climatic conditions. The EPIC model requires weather input files that detail daily and monthly climatic variables. Daily files provide day-to-day weather data, while monthly files summarize the average or total values per month. These files are crucial for driving the daily simulation processes in EPIC.</p>"},{"location":"pages/getting_started/Weather/#fetching-weather-data","title":"Fetching Weather Data","text":"<p>GeoEPIC streamlines the process of fetching and organizing weather data essential for EPIC simulations. It supports the creation of weather input files from daymet, and also from sophisticated datasets available through Google Earth Engine. One can define a composite collection in the config file, detailing the specific variables to select from each data source, tailoring the dataset to meet the needs of their simulations. </p>"},{"location":"pages/getting_started/Weather/#using-gee-collections","title":"Using GEE collections","text":"<p>GeoEPIC allows the integration of various weather and climate data sources on GEE. To explore the available datasets, visit Google Earth Engine's dataset catalog and GEE Community Catalog. Private assets can also be uploaded to Earth Engine, to use them in combination with existing datasets. Below is an example of configuration file that can be used to create weather input files.</p> <p>Example config files:</p> <ul> <li>using AgERA5</li> </ul> <pre><code># Global parameters\nglobal_scope:\n  time_range: ['2002-01-01', '2022-12-31']\n  variables: ['srad', 'tmax', 'tmin', 'prcp', 'rh', 'ws']  \n  resolution: 9600\n\n\n# Specify Earth Engine (EE) collections and their respective variables\ncollections:\n  AgEra5:\n    collection: 'projects/climate-engine-pro/assets/ce-ag-era5/daily'\n    variables:\n      srad: b('Solar_Radiation_Flux') \n      tmax: b('Temperature_Air_2m_Max_24h') - 273.15\n      tmin: b('Temperature_Air_2m_Min_24h') - 273.15\n      prcp: b('Precipitation_Flux') \n      rh: b('Relative_Humidity_2m_06h')\n      ws: b('Wind_Speed_10m_Mean')\n</code></pre> <p>Using the config file to get data:</p> <p><pre><code># Fetch and output weather input files for a specific latitude and longitude\ngeo_epic weather config.yml --fetch {lat} {lon} --out {out_path}\n</code></pre> <pre><code># Fetch for a list of locations in a csv file with lat, lon, out_path columns\ngeo_epic weather config.yml --fetch {list.csv} --out {column_name}\n</code></pre> <pre><code># Fetch for crop sequence boundaries shape file.\ngeo_epic weather config.yml --fetch {aoi_csb.shp} --out {out_dir}\n</code></pre></p> <p>Note: This command will write weather grid IDs corresponding to each location as an attribute into the input file, when used with a CSV file or crop sequence boundary shapefile.</p>"},{"location":"pages/getting_started/Weather/#from-daymet-and-nldas","title":"From Daymet and NLDAS","text":"<p>To fetch weather data for a specific latitude and longitude using the Daymet dataset, start by downloading the windspeed data from the NLDAS dataset and saving it to the specified directory. Afterward, fetch the other required weather data from the Daymet dataset.</p> <p><pre><code># To download windspeed data, use the following command\ngeo_epic weather windspeed -bbox &lt;bounding_box&gt; -start &lt;start_date&gt; -end &lt;end_date&gt; -out './weather'\n</code></pre> <pre><code># To fetch and output weather input files for a specific latitude and longitude\ngeo_epic weather daymet --fetch {lat} {lon} -start &lt;start_date&gt; -end &lt;end_date&gt; --out './weather'\n</code></pre></p>"},{"location":"pages/getting_started/getting_started/","title":"Getting started","text":"<p>This is the overview markdown file for getting started. This section contains all the theoretical parts with links to the respective pages.</p> <ul> <li>Overview</li> <li>Installation</li> <li>EPIC Model</li> <li>Input Files:<ul> <li>Weather Module</li> <li>Soil Module</li> <li>Crop Management</li> </ul> </li> <li>EPIC Python Interface</li> <li>Calibration</li> <li>Earth Engine Utility</li> </ul>"},{"location":"reference/api/api/","title":"Command Line Interface","text":"<p>GeoEPIC provides a command-line interface to execute various tasks.   The general command structure is as follows:</p> <pre><code>geo_epic {module} {func} [options]\n</code></pre> <p>Example usage: <pre><code>geo_epic workspace new -n Test\n</code></pre></p>"},{"location":"reference/api/api/#modules-and-functions","title":"Modules and Functions","text":""},{"location":"reference/api/api/#workspace","title":"workspace","text":"<ul> <li>new: Create a new workspace with a predefined template structure.</li> <li>copy: Copy files between different folders.</li> <li>run: Execute simulations within the workspace.</li> </ul>"},{"location":"reference/api/api/#utility","title":"utility","text":"<ul> <li>gee: Download required time-series data from Google Earth Engine.</li> </ul>"},{"location":"reference/api/api/#weather","title":"weather","text":"<ul> <li>ee: Retrieve weather data from Earth Engine.</li> <li>download_daily: Download daily weather data from Daymet.</li> </ul>"},{"location":"reference/api/api/#soil","title":"soil","text":"<ul> <li>usda: Fetch soil data from USDA SSURGO.</li> <li>process_gdb: Process SSURGO geodatabase (GDB) files.</li> </ul>"},{"location":"reference/api/api/#sites","title":"sites","text":"<ul> <li>generate: Generate site files from processed data.</li> </ul> <p>For more details on each command and its available options, use the following command:</p> <pre><code>geo_epic {module} {func} --help\n</code></pre>"},{"location":"reference/api/core/","title":"Core Module","text":""},{"location":"reference/api/core/#core","title":"<code>core</code>","text":"<p>Classes:</p> Name Description <code>EPICModel</code> <p>This class handles the setup and execution of the EPIC model executable.</p> <code>Problem_Wrapper</code> <p>A wrapper class that provides a simplified interface for optimization and sensitivity analysis.</p> <code>PygmoProblem</code> <p>A class designed to define an optimization problem for use with the PyGMO library, </p> <code>Site</code> <p>Represents a site (ex: agricultural field) with paths to it's corresponding EPIC input files.</p> <code>Workspace</code> <p>This class organises the workspace for executing simulations, saving required results.</p>"},{"location":"reference/api/core/#core.EPICModel","title":"<code>EPICModel</code>","text":"<p>This class handles the setup and execution of the EPIC model executable.</p> <p>Attributes:</p> Name Type Description <code>base_dir</code> <code>str</code> <p>The base directory for model runs.</p> <code>executable</code> <code>str</code> <p>Path to the executable model file.</p> <code>output_dir</code> <code>str</code> <p>Directory to store model outputs.</p> <code>log_dir</code> <code>str</code> <p>Directory to store logs.</p> <code>start_date</code> <code>date</code> <p>The start date of the EPIC model simulation.</p> <code>duration</code> <code>int</code> <p>The duration of the EPIC model simulation in years.</p> <code>output_types</code> <code>list</code> <p>A list of enabled output types for the EPIC model.</p> <code>model_dir</code> <code>str</code> <p>Directory path where the executable is located.</p> <code>executable_name</code> <code>str</code> <p>Name of the executable file.</p> <p>Methods:</p> Name Description <code>auto_Nfertilization</code> <p>Update the nitrogen settings in the EPICCONT.DAT file.</p> <code>auto_irrigation</code> <p>Update the irrigation settings in the EPICCONT.DAT file.</p> <code>close</code> <p>Release the lock on the model's directory by deleting the lock file.</p> <code>from_config</code> <p>Create an EPICModel instance from a configuration path.</p> <code>run</code> <p>Execute the model for the given site and handle output files.</p> <code>set_output_types</code> <p>Set the model output types and update the model's print file to enable specified outputs.</p> <code>setup</code> <p>Set up the model run configurations based on provided settings.</p>"},{"location":"reference/api/core/#core.EPICModel.duration","title":"<code>duration</code>  <code>property</code> <code>writable</code>","text":"<p>Get the duration of the EPIC model simulation.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The duration of the simulation in years.</p>"},{"location":"reference/api/core/#core.EPICModel.output_types","title":"<code>output_types</code>  <code>property</code> <code>writable</code>","text":"<p>Get the current output types of the EPIC model.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of enabled output types.</p>"},{"location":"reference/api/core/#core.EPICModel.start_date","title":"<code>start_date</code>  <code>property</code> <code>writable</code>","text":"<p>Get the start date of the EPIC model simulation.</p> <p>Returns:</p> Type Description <p>datetime.date: The start date of the simulation.</p>"},{"location":"reference/api/core/#core.EPICModel.auto_Nfertilization","title":"<code>auto_Nfertilization(bft0, fnp=None, fmx=None)</code>","text":"<p>Update the nitrogen settings in the EPICCONT.DAT file. Only BFT0 is required. Other parameters will be updated only if they are not None.</p> <p>:param file_path: Path to the EPICCONT.DAT file :param bft0: Nitrogen stress factor to trigger auto fertilization (BFT0) - required :param fnp: Fertilizer application variable (FNP) - optional :param fmx: Maximum annual N fertilizer applied for a crop (FMX) - optional</p>"},{"location":"reference/api/core/#core.EPICModel.auto_irrigation","title":"<code>auto_irrigation(bir, efi=None, vimx=None, armn=None, armx=None)</code>","text":"<p>Update the irrigation settings in the EPICCONT.DAT file. Only BIR is required. Other parameters will be updated only if they are not None.</p> <p>:param file_path: Path to the EPICCONT.DAT file :param bir: Water stress factor to trigger automatic irrigation (BIR) - required :param efi: Runoff volume/Volume irrigation water applied (EFI) - optional :param vimx: Maximum annual irrigation volume (VIMX) in mm - optional :param armn: Minimum single application volume (ARMN) in mm - optional :param armx: Maximum single application volume (ARMX) in mm - optional</p>"},{"location":"reference/api/core/#core.EPICModel.close","title":"<code>close()</code>","text":"<p>Release the lock on the model's directory by deleting the lock file.</p>"},{"location":"reference/api/core/#core.EPICModel.from_config","title":"<code>from_config(config_path)</code>  <code>classmethod</code>","text":"<p>Create an EPICModel instance from a configuration path.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> required <p>Returns:</p> Name Type Description <code>EPICModel</code> <p>A configured instance of the EPICModel.</p>"},{"location":"reference/api/core/#core.EPICModel.run","title":"<code>run(site, verbose=False, dest=None)</code>","text":"<p>Execute the model for the given site and handle output files.</p> <p>Parameters:</p> Name Type Description Default <code>site</code> <code>Site</code> <p>A site instance containing site-specific configuration.</p> required <code>dest</code> <code>str</code> <p>Destination directory for the run. If None, a temporary directory is used.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If any output file is not generated or is empty.</p>"},{"location":"reference/api/core/#core.EPICModel.set_output_types","title":"<code>set_output_types(output_types)</code>","text":"<p>Set the model output types and update the model's print file to enable specified outputs.</p> <p>Parameters:</p> Name Type Description Default <code>output_types</code> <code>list of str</code> <p>List of output types to be enabled.</p> required"},{"location":"reference/api/core/#core.EPICModel.setup","title":"<code>setup(config)</code>","text":"<p>Set up the model run configurations based on provided settings.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing model settings.</p> required"},{"location":"reference/api/core/#core.Problem_Wrapper","title":"<code>Problem_Wrapper</code>","text":"<p>A wrapper class that provides a simplified interface for optimization and sensitivity analysis.</p> <p>Attributes:</p> Name Type Description <code>problem</code> <code>PygmoProblem</code> <p>The PygmoProblem instance.</p> <code>pg_problem</code> <code>problem</code> <p>The wrapped PyGMO problem instance.</p> <code>algorithm</code> <p>The PyGMO algorithm instance for optimization.</p> <code>population</code> <p>The PyGMO population instance.</p> <code>population_size</code> <code>int</code> <p>Size of the population for optimization.</p> <p>Methods:</p> Name Description <code>init</code> <p>Initialize the optimization algorithm and population.</p> <code>optimize</code> <p>Run the optimization process.</p> <code>sensitivity_analysis</code> <p>Perform sensitivity analysis using SALib with status updates.</p>"},{"location":"reference/api/core/#core.Problem_Wrapper.init","title":"<code>init(algorithm, **kwargs)</code>","text":"<p>Initialize the optimization algorithm and population.</p> <p>Parameters:</p> Name Type Description Default <code>algorithm</code> <p>PyGMO algorithm class (e.g., pg.pso_gen)</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to the algorithm</p> <code>{}</code>"},{"location":"reference/api/core/#core.Problem_Wrapper.optimize","title":"<code>optimize(population_size, generations)</code>","text":"<p>Run the optimization process.</p> <p>Parameters:</p> Name Type Description Default <code>population_size</code> <code>int</code> <p>Size of the population for optimization</p> required <code>generations</code> <code>int</code> <p>Number of generations to run</p> required <p>Returns:</p> Type Description <p>The evolved population after optimization</p>"},{"location":"reference/api/core/#core.Problem_Wrapper.sensitivity_analysis","title":"<code>sensitivity_analysis(base_no_of_samples, method)</code>","text":"<p>Perform sensitivity analysis using SALib with status updates.</p> <p>Parameters: - base_no_of_samples (int): Base number of samples to generate. - method (str): Sensitivity analysis method ('sobol', 'efast', 'morris').</p> <p>Returns: - dict: Results of the sensitivity analysis.</p>"},{"location":"reference/api/core/#core.PygmoProblem","title":"<code>PygmoProblem</code>","text":"<p>A class designed to define an optimization problem for use with the PyGMO library, </p> <p>Attributes:</p> Name Type Description <code>workspace</code> <code>Workspace</code> <p>The workspace object managing the environment in which the model runs.</p> <code>dfs</code> <code>tuple</code> <p>A tuple of DataFrame-like objects that hold constraints and parameters for the problem.</p> <code>bounds</code> <code>ndarray</code> <p>An array of parameter bounds, each specified as (min, max).</p> <code>lens</code> <code>ndarray</code> <p>An array of cumulative lengths that help in splitting parameters for each DataFrame.</p> <p>Methods:</p> Name Description <code>apply_solution</code> <p>Apply a solution vector to update parameters in all dataframes.</p> <code>fitness</code> <p>Evaluate the fitness of a solution vector 'x'.</p> <code>get_bounds</code> <p>Get the bounds for parameters as tuples of (min, max) values for each parameter across all data frames.</p>"},{"location":"reference/api/core/#core.PygmoProblem.current","title":"<code>current</code>  <code>property</code>","text":"<p>Retrieve the current parameter values from all data frames.</p> <p>Returns:</p> Type Description <p>np.array: A concatenated array of current parameter values from all data frames.</p>"},{"location":"reference/api/core/#core.PygmoProblem.var_names","title":"<code>var_names</code>  <code>property</code>","text":"<p>Get the variable names from all data frames.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>A list of variable names concatenated from all data frames. Each data frame's   var_names() method is called and the results are combined into a single list.</p>"},{"location":"reference/api/core/#core.PygmoProblem.apply_solution","title":"<code>apply_solution(x)</code>","text":"<p>Apply a solution vector to update parameters in all dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>A solution vector containing parameter values for all data frames.</p> required"},{"location":"reference/api/core/#core.PygmoProblem.fitness","title":"<code>fitness(x)</code>","text":"<p>Evaluate the fitness of a solution vector 'x'.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>A solution vector containing parameter values for all data frames.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The fitness value as determined by the workspace's fitness function.</p>"},{"location":"reference/api/core/#core.PygmoProblem.get_bounds","title":"<code>get_bounds()</code>","text":"<p>Get the bounds for parameters as tuples of (min, max) values for each parameter across all data frames.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Two numpy arrays representing the lower and upper bounds of the parameters.</p>"},{"location":"reference/api/core/#core.Site","title":"<code>Site</code>","text":"<p>Represents a site (ex: agricultural field) with paths to it's corresponding EPIC input files.</p> <p>Attributes:</p> Name Type Description <code>opc_path</code> <code>str</code> <p>Path to the operational practice code file.</p> <code>dly_path</code> <code>str</code> <p>Path to the daily weather data file.</p> <code>sol_path</code> <code>str</code> <p>Path to the soil data file.</p> <code>sit_path</code> <code>str</code> <p>Path to the site information file.</p> <code>site_id</code> <code>str</code> <p>Identifier for the site, derived from the sit file name if not provided.</p> <code>outputs</code> <code>dict</code> <p>Dictionary to store output file paths.</p> <p>Methods:</p> Name Description <code>copy</code> <p>Copy or symlink site files to a destination folder.</p> <code>fetch_usa</code> <p>Fetch all required EPIC input files for a location in the conterminous USA.</p> <code>from_config</code> <p>Factory method to create a Site instance from a configuration dictionary and additional site information.</p> <code>get_dly</code> <p>Retrieve daily weather data from a DLY file.</p> <code>get_opc</code> <p>Retrieve operation schedule data from an OPC file.</p> <code>get_sit</code> <p>Retrieve site data from a SIT file.</p> <code>get_sol</code> <p>Retrieve soil data from a SOL file.</p>"},{"location":"reference/api/core/#core.Site.elevation","title":"<code>elevation</code>  <code>property</code>","text":"<p>Elevation of the site.</p>"},{"location":"reference/api/core/#core.Site.latitude","title":"<code>latitude</code>  <code>property</code>","text":"<p>Latitude of the site.</p>"},{"location":"reference/api/core/#core.Site.longitude","title":"<code>longitude</code>  <code>property</code>","text":"<p>Longitude of the site.</p>"},{"location":"reference/api/core/#core.Site.copy","title":"<code>copy(dest_folder, use_symlink=False)</code>","text":"<p>Copy or symlink site files to a destination folder.</p> <p>Parameters:</p> Name Type Description Default <code>dest_folder</code> <code>str</code> <p>Destination folder path</p> required <code>use_symlinks</code> <code>bool</code> <p>If True, create symbolic links instead of copying files. Defaults to False.</p> required <p>Returns:</p> Name Type Description <code>Site</code> <p>A new Site instance pointing to the copied/linked files</p>"},{"location":"reference/api/core/#core.Site.fetch_usa","title":"<code>fetch_usa(lat, lon, opc, site_id=None, start_date=None, end_date=None)</code>  <code>classmethod</code>","text":"<p>Fetch all required EPIC input files for a location in the conterminous USA.</p> <p>This method automatically retrieves: - Soil data from SSURGO via Soil Data Access - Weather data from Daymet - Elevation and slope from DEM (GLO-30)</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of the site</p> required <code>lon</code> <code>float</code> <p>Longitude of the site</p> required <code>opc</code> <code>str</code> <p>Path to the OPC (operation schedule) file</p> required <code>site_id</code> <code>str</code> <p>Site identifier. Auto-generated if None.</p> <code>None</code> <code>start_date</code> <code>str</code> <p>Weather data start date (YYYY-MM-DD format)</p> <code>None</code> <code>end_date</code> <code>str</code> <p>Weather data end date (YYYY-MM-DD format)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Site</code> <p>A Site object with all input files fetched and ready for simulation</p> Example <p>site = Site.fetch_usa(41.1686, -96.4736, opc='./irrigated_corn.OPC') print(site)</p>"},{"location":"reference/api/core/#core.Site.from_config","title":"<code>from_config(config, **site_info)</code>  <code>classmethod</code>","text":"<p>Factory method to create a Site instance from a configuration dictionary and additional site information.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with paths and settings.</p> required <code>**site_info</code> <code>dict</code> <p>Keyword arguments containing site-specific information such as 'opc', 'dly', 'soil', 'SiteID', 'lat', 'lon', and 'ele'.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Site</code> <p>An instance of the Site class configured according to the provided settings.</p>"},{"location":"reference/api/core/#core.Site.get_dly","title":"<code>get_dly()</code>","text":"<p>Retrieve daily weather data from a DLY file.</p> <p>Returns:</p> Name Type Description <code>DailyWeather</code> <p>An instance of the DailyWeather class containing weather data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the DLY file does not exist at the specified path.</p>"},{"location":"reference/api/core/#core.Site.get_opc","title":"<code>get_opc()</code>","text":"<p>Retrieve operation schedule data from an OPC file.</p> <p>Returns:</p> Name Type Description <code>Operation</code> <p>An instance of the Operation class containing operation schedule data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the OPC file does not exist at the specified path.</p>"},{"location":"reference/api/core/#core.Site.get_sit","title":"<code>get_sit()</code>","text":"<p>Retrieve site data from a SIT file.</p> <p>Returns:</p> Name Type Description <code>Site</code> <p>An instance of the Site class containing site data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the SIT file does not exist at the specified path.</p>"},{"location":"reference/api/core/#core.Site.get_sol","title":"<code>get_sol()</code>","text":"<p>Retrieve soil data from a SOL file.</p> <p>Returns:</p> Name Type Description <code>Soil</code> <p>An instance of the Soil class containing soil data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the SOL file does not exist at the specified path.</p>"},{"location":"reference/api/core/#core.Workspace","title":"<code>Workspace</code>","text":"<p>This class organises the workspace for executing simulations, saving required results.</p> <p>Attributes:</p> Name Type Description <code>uuid</code> <code>str</code> <p>Unique ID assigned to each workspace instance</p> <code>config</code> <code>dict</code> <p>Configuration data loaded from a config file.</p> <code>base_dir</code> <code>str</code> <p>Base directory for the workspace.</p> <code>routines</code> <code>dict</code> <p>Dictionary to store functions as routines.</p> <code>objective_function</code> <code>callable</code> <p>Function to be executed as the objective.</p> <code>dataframes</code> <code>dict</code> <p>Cache for dataframes.</p> <code>delete_after_use</code> <code>bool</code> <p>Whether to delete temporary files after use.</p> <code>model</code> <code>EPICModel</code> <p>Instance of the EPIC model.</p> <code>data_logger</code> <code>DataLogger</code> <p>Instance of the DataLogger for logging data.</p> <p>Methods:</p> Name Description <code>clear_logs</code> <p>Clear all log files and temporary run directories.</p> <code>clear_outputs</code> <p>Clear all output files.</p> <code>close</code> <p>Explicit cleanup (use this in notebooks).</p> <code>fetch_log</code> <p>Retrieve the logs for a specific function.</p> <code>logger</code> <p>Decorator to log the results of a function.</p> <code>make_problem</code> <p>Create a PygmoProblem instance after validating inputs.</p> <code>objective</code> <p>Set the objective function to be executed after simulations.</p> <code>routine</code> <p>Decorator to add a function as a routine without logging or returning values.</p> <code>run</code> <p>Run simulations for all sites or filtered by a selection string.</p> <code>run_simulation</code> <p>Run simulation for a given site or site information.</p>"},{"location":"reference/api/core/#core.Workspace.clear_logs","title":"<code>clear_logs()</code>","text":"<p>Clear all log files and temporary run directories.</p>"},{"location":"reference/api/core/#core.Workspace.clear_outputs","title":"<code>clear_outputs()</code>","text":"<p>Clear all output files.</p>"},{"location":"reference/api/core/#core.Workspace.close","title":"<code>close()</code>","text":"<p>Explicit cleanup (use this in notebooks).</p>"},{"location":"reference/api/core/#core.Workspace.fetch_log","title":"<code>fetch_log(func, keep=False)</code>","text":"<p>Retrieve the logs for a specific function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>str</code> <p>The name of the function whose logs are to be retrieved.</p> required <code>keep</code> <code>bool</code> <p>If True, preserve the logs after retrieval. If False, logs are deleted after reading. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: DataFrame containing the logs for the specified function.</p>"},{"location":"reference/api/core/#core.Workspace.logger","title":"<code>logger(func)</code>","text":"<p>Decorator to log the results of a function.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function that logs its output.</p>"},{"location":"reference/api/core/#core.Workspace.make_problem","title":"<code>make_problem(*dfs)</code>","text":"<p>Create a PygmoProblem instance after validating inputs.</p> <p>Parameters:</p> Name Type Description Default <code>*dfs</code> <p>Variable number of dataframes to pass to PygmoProblem</p> <code>()</code> <p>Returns:</p> Name Type Description <code>PygmoProblem</code> <p>A configured optimization problem instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no dataframes provided or if any dataframe lacks constraints</p>"},{"location":"reference/api/core/#core.Workspace.objective","title":"<code>objective(func)</code>","text":"<p>Set the objective function to be executed after simulations.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The objective function to be set.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorator function that sets the objective function.</p>"},{"location":"reference/api/core/#core.Workspace.routine","title":"<code>routine(func)</code>","text":"<p>Decorator to add a function as a routine without logging or returning values.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>callable</code> <p>The function to be decorated.</p> required <p>Returns:</p> Name Type Description <code>callable</code> <p>The decorated function that executes without logging.</p>"},{"location":"reference/api/core/#core.Workspace.run","title":"<code>run(select_str=None, progress_bar=True)</code>","text":"<p>Run simulations for all sites or filtered by a selection string.</p> <p>Parameters:</p> Name Type Description Default <code>select_str</code> <code>str</code> <p>String to filter sites. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Any</code> <p>The result of the objective function if set, otherwise None.</p>"},{"location":"reference/api/core/#core.Workspace.run_simulation","title":"<code>run_simulation(site_or_info)</code>","text":"<p>Run simulation for a given site or site information.</p> <p>Parameters:</p> Name Type Description Default <code>site_or_info</code> <code>Site or dict</code> <p>A Site object or a dictionary containing site information.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>The results from the post-processing routines. Output files are saved based on the options selected</p>"},{"location":"reference/api/gee/","title":"GEE Module","text":""},{"location":"reference/api/gee/#gee","title":"<code>gee</code>","text":"<p>Classes:</p> Name Description <code>CompositeCollection</code> <p>A class to handle composite collection of Earth Engine data.</p>"},{"location":"reference/api/gee/#gee.CompositeCollection","title":"<code>CompositeCollection</code>","text":"<p>A class to handle composite collection of Earth Engine data.</p> <p>This class initializes collections of Earth Engine based on a provided YAML configuration file, applies specified formulas and selections, and allows for the extraction of temporal data for a given Area of Interest (AOI).</p> <p>Methods:</p> Name Description <code>extract</code> <p>Extracts temporal data for a given AOI and returns it as a pandas DataFrame.</p>"},{"location":"reference/api/gee/#gee.CompositeCollection.extract","title":"<code>extract(aoi_coords)</code>","text":"<p>Extracts temporal data for a given Area of Interest (AOI).</p> <p>Parameters:</p> Name Type Description Default <code>aoi_coords</code> <code>tuple / list</code> <p>Coordinates representing the AOI, either as a Point or as vertices of a Polygon.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A pandas DataFrame containing the extracted data.</p>"},{"location":"reference/api/gee/#gee.CompositeCollection.merged","title":"<code>merged()</code>","text":"<p>Merges all collections in self.collections into a single ImageCollection.</p> <p>Returns:</p> Type Description <p>ee.ImageCollection: A merged collection containing all images from all collections.</p>"},{"location":"reference/api/io/","title":"IO Module","text":""},{"location":"reference/api/io/#io","title":"<code>io</code>","text":"<p>Classes:</p> Name Description <code>ACY</code> <code>DGN</code> <code>DLY</code> <code>DSL</code> <code>DWC</code> <code>OPC</code> <code>SIT</code> <code>SOL</code>"},{"location":"reference/api/io/#io.ACY","title":"<code>ACY</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the ACY data.</p>"},{"location":"reference/api/io/#io.ACY.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the ACY data.</p>"},{"location":"reference/api/io/#io.DGN","title":"<code>DGN</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DGN data.</p>"},{"location":"reference/api/io/#io.DGN.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DGN data.</p>"},{"location":"reference/api/io/#io.DLY","title":"<code>DLY</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>load</code> <p>Load data from a DLY file into DataFrame.</p> <code>save</code> <p>Save DataFrame into a DLY file.</p> <code>to_monthly</code> <p>Save as monthly file</p> <code>validate</code> <p>Validate the DataFrame to ensure it contains a continuous range of dates </p>"},{"location":"reference/api/io/#io.DLY.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load data from a DLY file into DataFrame.</p>"},{"location":"reference/api/io/#io.DLY.save","title":"<code>save(path=None)</code>","text":"<p>Save DataFrame into a DLY file.</p>"},{"location":"reference/api/io/#io.DLY.to_monthly","title":"<code>to_monthly(path=None)</code>","text":"<p>Save as monthly file</p>"},{"location":"reference/api/io/#io.DLY.validate","title":"<code>validate(start_date, end_date)</code>","text":"<p>Validate the DataFrame to ensure it contains a continuous range of dates  between start_date and end_date, without duplicates.</p>"},{"location":"reference/api/io/#io.DSL","title":"<code>DSL</code>","text":"<p>Methods:</p> Name Description <code>get_data</code> <p>Return stored water data.</p>"},{"location":"reference/api/io/#io.DSL.get_data","title":"<code>get_data()</code>","text":"<p>Return stored water data.</p>"},{"location":"reference/api/io/#io.DWC","title":"<code>DWC</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DWC data.</p>"},{"location":"reference/api/io/#io.DWC.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DWC data.</p>"},{"location":"reference/api/io/#io.OPC","title":"<code>OPC</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>append</code> <p>Append another OPC or DataFrame to the current OPC instance.</p> <code>edit_crop_season</code> <p>Edit the planting and/or harvest dates for a given year and crop.</p> <code>edit_fertilizer_rate</code> <p>Edit the fertilizer rate for a given year.</p> <code>edit_harvest_date</code> <p>Edit the harvest date for a given year and crop.</p> <code>edit_operation_date</code> <p>Edit the operation date for a given year.</p> <code>edit_operation_value</code> <p>Edit the operation value for a given year.</p> <code>edit_plantation_date</code> <p>Edit the plantation date for a given year and crop.</p> <code>get_harvest_date</code> <p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <code>get_plantation_date</code> <p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <code>iter_seasons</code> <p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <code>load</code> <p>Load data from an OPC file into DataFrame.</p> <code>new</code> <p>Create a new OPC instance with an empty DataFrame with preset columns, a default header,</p> <code>remove</code> <p>Remove operation(s) from the OPC file that match all provided criteria.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>update</code> <p>Add or update an operation in the OPC file.</p> <code>update_phu</code> <p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <code>validate</code> <p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p>"},{"location":"reference/api/io/#io.OPC.IAUI","title":"<code>IAUI</code>  <code>property</code> <code>writable</code>","text":"<p>Get the auto-irrigation implement ID from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if auto-irrigation is enabled (72), False if disabled (0)</p>"},{"location":"reference/api/io/#io.OPC.LUN","title":"<code>LUN</code>  <code>property</code> <code>writable</code>","text":"<p>Get the land use number from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The land use number from the first 4 characters of header line 2</p>"},{"location":"reference/api/io/#io.OPC.append","title":"<code>append(second_opc)</code>","text":"<p>Append another OPC or DataFrame to the current OPC instance. Args:     second_opc (pd.DataFrame or OPC): The data to append. Returns:     OPC: A new OPC instance with combined data. Raises:     ValueError: If second_opc is not a pandas DataFrame or OPC instance.</p>"},{"location":"reference/api/io/#io.OPC.edit_crop_season","title":"<code>edit_crop_season(new_planting_date=None, new_harvest_date=None, crop_code=None)</code>","text":"<p>Edit the planting and/or harvest dates for a given year and crop.</p> <p>Parameters: year (int): Year. new_planting_date (datetime, optional): New planting date. If not provided, only harvest date will be updated. new_harvest_date (datetime, optional): New harvest date. If not provided, only planting date will be updated. crop_code (int, optional): Crop code. If not provided, changes the first crop found.</p>"},{"location":"reference/api/io/#io.OPC.edit_fertilizer_rate","title":"<code>edit_fertilizer_rate(rate, year=2020, month=None, day=None)</code>","text":"<p>Edit the fertilizer rate for a given year.</p> <p>Parameters: rate (float): Fertilizer rate to be set. year (int, optional): Year for the fertilizer rate application. Defaults to 2020. month (int, optional): Month for the fertilizer rate application. If not provided, the first instance is changed. day (int, optional): Day for the fertilizer rate application. Defaults to None.</p>"},{"location":"reference/api/io/#io.OPC.edit_harvest_date","title":"<code>edit_harvest_date(date, crop_code)</code>","text":"<p>Edit the harvest date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of harvest in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"reference/api/io/#io.OPC.edit_operation_date","title":"<code>edit_operation_date(code, year, month, day, crop_code=None)</code>","text":"<p>Edit the operation date for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. month (int): Month of operation. day (int): Day of operation. crop_code (int, optional): Crop code.</p>"},{"location":"reference/api/io/#io.OPC.edit_operation_value","title":"<code>edit_operation_value(code, year, value, crop_code=None)</code>","text":"<p>Edit the operation value for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. value (float): New operation value. crop_code (int, optional): Crop code.</p>"},{"location":"reference/api/io/#io.OPC.edit_plantation_date","title":"<code>edit_plantation_date(date, crop_code)</code>","text":"<p>Edit the plantation date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of plantation in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"reference/api/io/#io.OPC.get_harvest_date","title":"<code>get_harvest_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their harvest dates with row indices.</p>"},{"location":"reference/api/io/#io.OPC.get_plantation_date","title":"<code>get_plantation_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their plantation dates with row indices.</p>"},{"location":"reference/api/io/#io.OPC.iter_seasons","title":"<code>iter_seasons(start_year=None, end_year=None)</code>","text":"<p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <p>Parameters: start_year (int, optional): The starting year to consider. Defaults to None. end_year (int, optional): The ending year to consider. Defaults to None.</p> <p>dict: A dictionary containing:     - plantation_date: The date of plantation     - harvest_date: The date of harvest     - crop_code: The crop code     - operations: A subset of OPC rows for this season     - plantation_index: The index of the plantation row</p>"},{"location":"reference/api/io/#io.OPC.load","title":"<code>load(path, start_year=None)</code>  <code>classmethod</code>","text":"<p>Load data from an OPC file into DataFrame.</p> <p>Parameters: path (str): Path to the OPC file. start_year (int, optional): Start year for the OPC file. If not provided, it will be read from the file header.</p> <p>Returns: OPC: An instance of the OPC class containing the loaded data.</p>"},{"location":"reference/api/io/#io.OPC.new","title":"<code>new(name, start_year)</code>  <code>classmethod</code>","text":"<p>Create a new OPC instance with an empty DataFrame with preset columns, a default header, and provided name and start year.</p> <p>Parameters: name (str): The name for this OPC file/instance. start_year (int): The start year to assign in the OPC instance.</p> <p>Returns: OPC: An OPC instance with no data but with the required metadata set.</p>"},{"location":"reference/api/io/#io.OPC.remove","title":"<code>remove(opID=None, date=None, cropID=None, XMTU=None, fertID=None, year=None)</code>","text":"<p>Remove operation(s) from the OPC file that match all provided criteria.</p> <p>Parameters:</p> Name Type Description Default <code>opID</code> <code>int</code> <p>Operation ID to match</p> <code>None</code> <code>date</code> <code>str</code> <p>Date to match in format 'YYYY-MM-DD'</p> <code>None</code> <code>cropID</code> <code>int</code> <p>Crop ID to match</p> <code>None</code> <code>XMTU/LYR/pestID/fertID</code> <code>int</code> <p>Machine type/layer/pesticide ID/fertilizer ID to match</p> required <code>year</code> <code>int</code> <p>Year to match</p> <code>None</code>"},{"location":"reference/api/io/#io.OPC.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p> <p>Parameters: path (str): Path to save the OPC file.</p>"},{"location":"reference/api/io/#io.OPC.update","title":"<code>update(operation)</code>","text":"<p>Add or update an operation in the OPC file.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Dictionary containing operation details with keys: - opID: Operation ID (required) - cropID: Crop ID (required) - date: Operation date as string 'YYYY-MM-DD' (required) - XMTU/LYR/pestID/fertID: Machine type/years/pesticide ID/fertilizer ID (optional, default 0) - OPV1-OPV8: Additional operation values (optional, default 0)</p> required"},{"location":"reference/api/io/#io.OPC.update_phu","title":"<code>update_phu(dly, cropcom)</code>","text":"<p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <p>Parameters: dly (DLY): DLY object containing weather data. cropcom (DataFrame): DataFrame containing crop code and TBS values.</p>"},{"location":"reference/api/io/#io.OPC.validate","title":"<code>validate(duration=None)</code>","text":"<p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p> <p>Parameters: duration (int, optional): Duration of the simulation in years. If None, uses the maximum Yid.</p> <p>Returns: bool: True if the data is valid, False otherwise.</p>"},{"location":"reference/api/io/#io.SIT","title":"<code>SIT</code>","text":"<p>Methods:</p> Name Description <code>load</code> <p>Class method to load the .sit file and return a SiteFile instance.</p> <code>save</code> <p>Save the current site information to a .SIT file.</p>"},{"location":"reference/api/io/#io.SIT.elevation","title":"<code>elevation</code>  <code>property</code> <code>writable</code>","text":"<p>Get elevation value.</p>"},{"location":"reference/api/io/#io.SIT.lat","title":"<code>lat</code>  <code>property</code> <code>writable</code>","text":"<p>Get latitude value.</p>"},{"location":"reference/api/io/#io.SIT.lon","title":"<code>lon</code>  <code>property</code> <code>writable</code>","text":"<p>Get longitude value.</p>"},{"location":"reference/api/io/#io.SIT.slope","title":"<code>slope</code>  <code>property</code> <code>writable</code>","text":"<p>Get slope steep value.</p>"},{"location":"reference/api/io/#io.SIT.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Class method to load the .sit file and return a SiteFile instance.</p> <p>Parameters: file_path (str): Path to the .sit file.</p> <p>Returns: SiteFile: An instance of the SiteFile class with loaded data.</p>"},{"location":"reference/api/io/#io.SIT.save","title":"<code>save(output_dir)</code>","text":"<p>Save the current site information to a .SIT file.</p>"},{"location":"reference/api/io/#io.SOL","title":"<code>SOL</code>","text":"<p>Methods:</p> Name Description <code>from_sda</code> <p>Create a Soil object from Soil Data Access using a query.</p> <code>load</code> <p>Load soil data from a file and return a Soil object.</p> <code>save</code> <p>Save the soil data to a file using a template.</p>"},{"location":"reference/api/io/#io.SOL.from_sda","title":"<code>from_sda(query)</code>  <code>classmethod</code>","text":"<p>Create a Soil object from Soil Data Access using a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>int or str</code> <p>Query string for SoilDataAccess. (mukey or WKT str) ( \"POINT(-123.4567 45.6789)\" )</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from SDA.</p>"},{"location":"reference/api/io/#io.SOL.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Load soil data from a file and return a Soil object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the soil file.</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from the file.</p>"},{"location":"reference/api/io/#io.SOL.save","title":"<code>save(filepath, template=None)</code>","text":"<p>Save the soil data to a file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the soil file.</p> required <code>template</code> <code>list</code> <p>Optional list of template lines.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If soil properties DataFrame is empty.</p>"},{"location":"reference/api/io/#io.config_parser","title":"<code>config_parser</code>","text":"<p>Classes:</p> Name Description <code>ConfigParser</code>"},{"location":"reference/api/io/#io.config_parser.ConfigParser","title":"<code>ConfigParser</code>","text":"<p>Methods:</p> Name Description <code>get</code> <p>Retrieve a value from the configuration.</p> <code>load</code> <p>Load data from the YAML file.</p> <code>save</code> <p>Save data to the YAML file.</p> <code>update</code> <p>Update the current config with new values.</p>"},{"location":"reference/api/io/#io.config_parser.ConfigParser.get","title":"<code>get(key, default=None)</code>","text":"<p>Retrieve a value from the configuration.</p>"},{"location":"reference/api/io/#io.config_parser.ConfigParser.load","title":"<code>load()</code>","text":"<p>Load data from the YAML file.</p>"},{"location":"reference/api/io/#io.config_parser.ConfigParser.save","title":"<code>save()</code>","text":"<p>Save data to the YAML file.</p>"},{"location":"reference/api/io/#io.config_parser.ConfigParser.update","title":"<code>update(updates)</code>","text":"<p>Update the current config with new values.</p>"},{"location":"reference/api/io/#io.cropcom","title":"<code>cropcom</code>","text":"<p>Classes:</p> Name Description <code>CropCom</code> <p>Class for handling CROPCOM.DAT file.</p>"},{"location":"reference/api/io/#io.cropcom.CropCom","title":"<code>CropCom</code>","text":"<p>Class for handling CROPCOM.DAT file.</p> <p>Methods:</p> Name Description <code>constraints</code> <p>Returns the constraints (min, max ranges) for the parameters.</p> <code>edit</code> <p>Updates the parameters in the DataFrame with new values.</p> <code>get_vars</code> <p>Returns the vars DataFrame with an additional column containing current values.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>set_sensitive</code> <p>Sets sensitive parameters based on a CSV path or list of parameter names.</p>"},{"location":"reference/api/io/#io.cropcom.CropCom.current","title":"<code>current</code>  <code>property</code>","text":"<p>Returns the current values of parameters in the DataFrame.</p>"},{"location":"reference/api/io/#io.cropcom.CropCom.constraints","title":"<code>constraints()</code>","text":"<p>Returns the constraints (min, max ranges) for the parameters.</p>"},{"location":"reference/api/io/#io.cropcom.CropCom.edit","title":"<code>edit(values)</code>","text":"<p>Updates the parameters in the DataFrame with new values.</p>"},{"location":"reference/api/io/#io.cropcom.CropCom.get_vars","title":"<code>get_vars()</code>","text":"<p>Returns the vars DataFrame with an additional column containing current values.</p>"},{"location":"reference/api/io/#io.cropcom.CropCom.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p>"},{"location":"reference/api/io/#io.cropcom.CropCom.set_sensitive","title":"<code>set_sensitive(parms_input, crop_codes, all=False)</code>","text":"<p>Sets sensitive parameters based on a CSV path or list of parameter names. If <code>all</code> is True, all parameters are considered sensitive.</p> <p>Parameters:</p> Name Type Description Default <code>parms_input</code> <code>str or list</code> <p>Either a CSV file path or list of parameter names to select</p> required <code>crop_codes</code> <code>list</code> <p>List of crop codes to apply parameters to</p> required <code>all</code> <code>bool</code> <p>If True, all parameters are considered sensitive regardless of input</p> <code>False</code>"},{"location":"reference/api/io/#io.data_logger","title":"<code>data_logger</code>","text":"<p>Classes:</p> Name Description <code>DataLogger</code> <p>A class to handle logging of data using different backends: Redis, SQL, or LMDB.</p> <code>LMDBTableWriter</code> <p>Minimal, efficient row store with auto-increment row_id.</p> <code>RedisWriter</code>"},{"location":"reference/api/io/#io.data_logger.DataLogger","title":"<code>DataLogger</code>","text":"<p>A class to handle logging of data using different backends: Redis, SQL, or LMDB. It supports logging dictionaries and retrieving logged data.</p> <p>Attributes:</p> Name Type Description <code>output_folder</code> <code>str</code> <p>Directory where files are stored (if applicable).</p> <code>delete_on_read</code> <code>bool</code> <p>Whether to delete the data after retrieving it.</p> <code>backend</code> <code>str</code> <p>The backend to use ('redis', 'sql', 'lmdb').</p> <p>Methods:</p> Name Description <code>get</code> <p>Retrieve logged data using the specified backend.</p> <code>get_writer</code> <p>Get the appropriate writer based on the backend.</p> <code>log_dict</code> <p>Log a dictionary of results using the specified backend.</p>"},{"location":"reference/api/io/#io.data_logger.DataLogger.get","title":"<code>get(func_name, keep=False)</code>","text":"<p>Retrieve logged data using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function whose data needs to be retrieved.</p> required <code>keep</code> <code>bool</code> <p>If True, do not delete the table even if delete_on_read is True.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: The DataFrame containing the logged data.</p>"},{"location":"reference/api/io/#io.data_logger.DataLogger.get_writer","title":"<code>get_writer(func_name)</code>","text":"<p>Get the appropriate writer based on the backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to create a writer for.</p> required <p>Returns:</p> Name Type Description <code>Writer</code> <p>An instance of the appropriate writer class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported backend is specified.</p>"},{"location":"reference/api/io/#io.data_logger.DataLogger.log_dict","title":"<code>log_dict(func_name, result)</code>","text":"<p>Log a dictionary of results using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to log the data for.</p> required <code>result</code> <code>dict</code> <p>Dictionary of results to log.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result is not a dictionary.</p>"},{"location":"reference/api/io/#io.data_logger.LMDBTableWriter","title":"<code>LMDBTableWriter</code>","text":"<p>Minimal, efficient row store with auto-increment row_id. - write_row(row_id=None, **kwargs) -&gt; returns the row_id (string) - read_row(row_id) -&gt; dict or None - query_rows() -&gt; DataFrame with row_id index - delete_table(), open(), close(), context manager</p> <p>Methods:</p> Name Description <code>query_rows</code> <p>Return all rows as a DataFrame with numeric-sorted row_id.</p> <code>write_row</code> <p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"reference/api/io/#io.data_logger.LMDBTableWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Return all rows as a DataFrame with numeric-sorted row_id.</p>"},{"location":"reference/api/io/#io.data_logger.LMDBTableWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"reference/api/io/#io.data_logger.RedisWriter","title":"<code>RedisWriter</code>","text":"<p>Methods:</p> Name Description <code>close</code> <p>Close the connection to Redis (flag only; redis client is pooled).</p> <code>delete_table</code> <p>Delete all entries associated with the table name, including the counter.</p> <code>open</code> <p>Establish connection to Redis and initialize counter if needed.</p> <code>query_rows</code> <p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p> <code>read_row</code> <p>Read a row from Redis hash.</p> <code>write_row</code> <p>Write a row to Redis hash under the specified table name.</p>"},{"location":"reference/api/io/#io.data_logger.RedisWriter.close","title":"<code>close()</code>","text":"<p>Close the connection to Redis (flag only; redis client is pooled).</p>"},{"location":"reference/api/io/#io.data_logger.RedisWriter.delete_table","title":"<code>delete_table()</code>","text":"<p>Delete all entries associated with the table name, including the counter.</p>"},{"location":"reference/api/io/#io.data_logger.RedisWriter.open","title":"<code>open()</code>","text":"<p>Establish connection to Redis and initialize counter if needed.</p>"},{"location":"reference/api/io/#io.data_logger.RedisWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p>"},{"location":"reference/api/io/#io.data_logger.RedisWriter.read_row","title":"<code>read_row(row_id)</code>","text":"<p>Read a row from Redis hash.</p>"},{"location":"reference/api/io/#io.data_logger.RedisWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Write a row to Redis hash under the specified table name.</p>"},{"location":"reference/api/io/#io.data_logger.lmdb_writer","title":"<code>lmdb_writer</code>","text":"<p>Classes:</p> Name Description <code>LMDBTableWriter</code> <p>Minimal, efficient row store with auto-increment row_id.</p>"},{"location":"reference/api/io/#io.data_logger.lmdb_writer.LMDBTableWriter","title":"<code>LMDBTableWriter</code>","text":"<p>Minimal, efficient row store with auto-increment row_id. - write_row(row_id=None, **kwargs) -&gt; returns the row_id (string) - read_row(row_id) -&gt; dict or None - query_rows() -&gt; DataFrame with row_id index - delete_table(), open(), close(), context manager</p> <p>Methods:</p> Name Description <code>query_rows</code> <p>Return all rows as a DataFrame with numeric-sorted row_id.</p> <code>write_row</code> <p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"reference/api/io/#io.data_logger.lmdb_writer.LMDBTableWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Return all rows as a DataFrame with numeric-sorted row_id.</p>"},{"location":"reference/api/io/#io.data_logger.lmdb_writer.LMDBTableWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Atomically allocate/increment row ID (if None) and write the row.</p>"},{"location":"reference/api/io/#io.data_logger.main","title":"<code>main</code>","text":"<p>Classes:</p> Name Description <code>DataLogger</code> <p>A class to handle logging of data using different backends: Redis, SQL, or LMDB.</p>"},{"location":"reference/api/io/#io.data_logger.main.DataLogger","title":"<code>DataLogger</code>","text":"<p>A class to handle logging of data using different backends: Redis, SQL, or LMDB. It supports logging dictionaries and retrieving logged data.</p> <p>Attributes:</p> Name Type Description <code>output_folder</code> <code>str</code> <p>Directory where files are stored (if applicable).</p> <code>delete_on_read</code> <code>bool</code> <p>Whether to delete the data after retrieving it.</p> <code>backend</code> <code>str</code> <p>The backend to use ('redis', 'sql', 'lmdb').</p> <p>Methods:</p> Name Description <code>get</code> <p>Retrieve logged data using the specified backend.</p> <code>get_writer</code> <p>Get the appropriate writer based on the backend.</p> <code>log_dict</code> <p>Log a dictionary of results using the specified backend.</p>"},{"location":"reference/api/io/#io.data_logger.main.DataLogger.get","title":"<code>get(func_name, keep=False)</code>","text":"<p>Retrieve logged data using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function whose data needs to be retrieved.</p> required <code>keep</code> <code>bool</code> <p>If True, do not delete the table even if delete_on_read is True.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: The DataFrame containing the logged data.</p>"},{"location":"reference/api/io/#io.data_logger.main.DataLogger.get_writer","title":"<code>get_writer(func_name)</code>","text":"<p>Get the appropriate writer based on the backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to create a writer for.</p> required <p>Returns:</p> Name Type Description <code>Writer</code> <p>An instance of the appropriate writer class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported backend is specified.</p>"},{"location":"reference/api/io/#io.data_logger.main.DataLogger.log_dict","title":"<code>log_dict(func_name, result)</code>","text":"<p>Log a dictionary of results using the specified backend.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to log the data for.</p> required <code>result</code> <code>dict</code> <p>Dictionary of results to log.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the result is not a dictionary.</p>"},{"location":"reference/api/io/#io.data_logger.redis_writer","title":"<code>redis_writer</code>","text":"<p>Classes:</p> Name Description <code>RedisWriter</code>"},{"location":"reference/api/io/#io.data_logger.redis_writer.RedisWriter","title":"<code>RedisWriter</code>","text":"<p>Methods:</p> Name Description <code>close</code> <p>Close the connection to Redis (flag only; redis client is pooled).</p> <code>delete_table</code> <p>Delete all entries associated with the table name, including the counter.</p> <code>open</code> <p>Establish connection to Redis and initialize counter if needed.</p> <code>query_rows</code> <p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p> <code>read_row</code> <p>Read a row from Redis hash.</p> <code>write_row</code> <p>Write a row to Redis hash under the specified table name.</p>"},{"location":"reference/api/io/#io.data_logger.redis_writer.RedisWriter.close","title":"<code>close()</code>","text":"<p>Close the connection to Redis (flag only; redis client is pooled).</p>"},{"location":"reference/api/io/#io.data_logger.redis_writer.RedisWriter.delete_table","title":"<code>delete_table()</code>","text":"<p>Delete all entries associated with the table name, including the counter.</p>"},{"location":"reference/api/io/#io.data_logger.redis_writer.RedisWriter.open","title":"<code>open()</code>","text":"<p>Establish connection to Redis and initialize counter if needed.</p>"},{"location":"reference/api/io/#io.data_logger.redis_writer.RedisWriter.query_rows","title":"<code>query_rows()</code>","text":"<p>Retrieve all rows from the Redis hash as a DataFrame with row_id as index.</p>"},{"location":"reference/api/io/#io.data_logger.redis_writer.RedisWriter.read_row","title":"<code>read_row(row_id)</code>","text":"<p>Read a row from Redis hash.</p>"},{"location":"reference/api/io/#io.data_logger.redis_writer.RedisWriter.write_row","title":"<code>write_row(row_id=None, **kwargs)</code>","text":"<p>Write a row to Redis hash under the specified table name.</p>"},{"location":"reference/api/io/#io.inputs","title":"<code>inputs</code>","text":"<p>Classes:</p> Name Description <code>DLY</code> <code>OPC</code> <code>SIT</code> <code>SOL</code>"},{"location":"reference/api/io/#io.inputs.DLY","title":"<code>DLY</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>load</code> <p>Load data from a DLY file into DataFrame.</p> <code>save</code> <p>Save DataFrame into a DLY file.</p> <code>to_monthly</code> <p>Save as monthly file</p> <code>validate</code> <p>Validate the DataFrame to ensure it contains a continuous range of dates </p>"},{"location":"reference/api/io/#io.inputs.DLY.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load data from a DLY file into DataFrame.</p>"},{"location":"reference/api/io/#io.inputs.DLY.save","title":"<code>save(path=None)</code>","text":"<p>Save DataFrame into a DLY file.</p>"},{"location":"reference/api/io/#io.inputs.DLY.to_monthly","title":"<code>to_monthly(path=None)</code>","text":"<p>Save as monthly file</p>"},{"location":"reference/api/io/#io.inputs.DLY.validate","title":"<code>validate(start_date, end_date)</code>","text":"<p>Validate the DataFrame to ensure it contains a continuous range of dates  between start_date and end_date, without duplicates.</p>"},{"location":"reference/api/io/#io.inputs.OPC","title":"<code>OPC</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>append</code> <p>Append another OPC or DataFrame to the current OPC instance.</p> <code>edit_crop_season</code> <p>Edit the planting and/or harvest dates for a given year and crop.</p> <code>edit_fertilizer_rate</code> <p>Edit the fertilizer rate for a given year.</p> <code>edit_harvest_date</code> <p>Edit the harvest date for a given year and crop.</p> <code>edit_operation_date</code> <p>Edit the operation date for a given year.</p> <code>edit_operation_value</code> <p>Edit the operation value for a given year.</p> <code>edit_plantation_date</code> <p>Edit the plantation date for a given year and crop.</p> <code>get_harvest_date</code> <p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <code>get_plantation_date</code> <p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <code>iter_seasons</code> <p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <code>load</code> <p>Load data from an OPC file into DataFrame.</p> <code>new</code> <p>Create a new OPC instance with an empty DataFrame with preset columns, a default header,</p> <code>remove</code> <p>Remove operation(s) from the OPC file that match all provided criteria.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>update</code> <p>Add or update an operation in the OPC file.</p> <code>update_phu</code> <p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <code>validate</code> <p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p>"},{"location":"reference/api/io/#io.inputs.OPC.IAUI","title":"<code>IAUI</code>  <code>property</code> <code>writable</code>","text":"<p>Get the auto-irrigation implement ID from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if auto-irrigation is enabled (72), False if disabled (0)</p>"},{"location":"reference/api/io/#io.inputs.OPC.LUN","title":"<code>LUN</code>  <code>property</code> <code>writable</code>","text":"<p>Get the land use number from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The land use number from the first 4 characters of header line 2</p>"},{"location":"reference/api/io/#io.inputs.OPC.append","title":"<code>append(second_opc)</code>","text":"<p>Append another OPC or DataFrame to the current OPC instance. Args:     second_opc (pd.DataFrame or OPC): The data to append. Returns:     OPC: A new OPC instance with combined data. Raises:     ValueError: If second_opc is not a pandas DataFrame or OPC instance.</p>"},{"location":"reference/api/io/#io.inputs.OPC.edit_crop_season","title":"<code>edit_crop_season(new_planting_date=None, new_harvest_date=None, crop_code=None)</code>","text":"<p>Edit the planting and/or harvest dates for a given year and crop.</p> <p>Parameters: year (int): Year. new_planting_date (datetime, optional): New planting date. If not provided, only harvest date will be updated. new_harvest_date (datetime, optional): New harvest date. If not provided, only planting date will be updated. crop_code (int, optional): Crop code. If not provided, changes the first crop found.</p>"},{"location":"reference/api/io/#io.inputs.OPC.edit_fertilizer_rate","title":"<code>edit_fertilizer_rate(rate, year=2020, month=None, day=None)</code>","text":"<p>Edit the fertilizer rate for a given year.</p> <p>Parameters: rate (float): Fertilizer rate to be set. year (int, optional): Year for the fertilizer rate application. Defaults to 2020. month (int, optional): Month for the fertilizer rate application. If not provided, the first instance is changed. day (int, optional): Day for the fertilizer rate application. Defaults to None.</p>"},{"location":"reference/api/io/#io.inputs.OPC.edit_harvest_date","title":"<code>edit_harvest_date(date, crop_code)</code>","text":"<p>Edit the harvest date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of harvest in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"reference/api/io/#io.inputs.OPC.edit_operation_date","title":"<code>edit_operation_date(code, year, month, day, crop_code=None)</code>","text":"<p>Edit the operation date for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. month (int): Month of operation. day (int): Day of operation. crop_code (int, optional): Crop code.</p>"},{"location":"reference/api/io/#io.inputs.OPC.edit_operation_value","title":"<code>edit_operation_value(code, year, value, crop_code=None)</code>","text":"<p>Edit the operation value for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. value (float): New operation value. crop_code (int, optional): Crop code.</p>"},{"location":"reference/api/io/#io.inputs.OPC.edit_plantation_date","title":"<code>edit_plantation_date(date, crop_code)</code>","text":"<p>Edit the plantation date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of plantation in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"reference/api/io/#io.inputs.OPC.get_harvest_date","title":"<code>get_harvest_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their harvest dates with row indices.</p>"},{"location":"reference/api/io/#io.inputs.OPC.get_plantation_date","title":"<code>get_plantation_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their plantation dates with row indices.</p>"},{"location":"reference/api/io/#io.inputs.OPC.iter_seasons","title":"<code>iter_seasons(start_year=None, end_year=None)</code>","text":"<p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <p>Parameters: start_year (int, optional): The starting year to consider. Defaults to None. end_year (int, optional): The ending year to consider. Defaults to None.</p> <p>dict: A dictionary containing:     - plantation_date: The date of plantation     - harvest_date: The date of harvest     - crop_code: The crop code     - operations: A subset of OPC rows for this season     - plantation_index: The index of the plantation row</p>"},{"location":"reference/api/io/#io.inputs.OPC.load","title":"<code>load(path, start_year=None)</code>  <code>classmethod</code>","text":"<p>Load data from an OPC file into DataFrame.</p> <p>Parameters: path (str): Path to the OPC file. start_year (int, optional): Start year for the OPC file. If not provided, it will be read from the file header.</p> <p>Returns: OPC: An instance of the OPC class containing the loaded data.</p>"},{"location":"reference/api/io/#io.inputs.OPC.new","title":"<code>new(name, start_year)</code>  <code>classmethod</code>","text":"<p>Create a new OPC instance with an empty DataFrame with preset columns, a default header, and provided name and start year.</p> <p>Parameters: name (str): The name for this OPC file/instance. start_year (int): The start year to assign in the OPC instance.</p> <p>Returns: OPC: An OPC instance with no data but with the required metadata set.</p>"},{"location":"reference/api/io/#io.inputs.OPC.remove","title":"<code>remove(opID=None, date=None, cropID=None, XMTU=None, fertID=None, year=None)</code>","text":"<p>Remove operation(s) from the OPC file that match all provided criteria.</p> <p>Parameters:</p> Name Type Description Default <code>opID</code> <code>int</code> <p>Operation ID to match</p> <code>None</code> <code>date</code> <code>str</code> <p>Date to match in format 'YYYY-MM-DD'</p> <code>None</code> <code>cropID</code> <code>int</code> <p>Crop ID to match</p> <code>None</code> <code>XMTU/LYR/pestID/fertID</code> <code>int</code> <p>Machine type/layer/pesticide ID/fertilizer ID to match</p> required <code>year</code> <code>int</code> <p>Year to match</p> <code>None</code>"},{"location":"reference/api/io/#io.inputs.OPC.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p> <p>Parameters: path (str): Path to save the OPC file.</p>"},{"location":"reference/api/io/#io.inputs.OPC.update","title":"<code>update(operation)</code>","text":"<p>Add or update an operation in the OPC file.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Dictionary containing operation details with keys: - opID: Operation ID (required) - cropID: Crop ID (required) - date: Operation date as string 'YYYY-MM-DD' (required) - XMTU/LYR/pestID/fertID: Machine type/years/pesticide ID/fertilizer ID (optional, default 0) - OPV1-OPV8: Additional operation values (optional, default 0)</p> required"},{"location":"reference/api/io/#io.inputs.OPC.update_phu","title":"<code>update_phu(dly, cropcom)</code>","text":"<p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <p>Parameters: dly (DLY): DLY object containing weather data. cropcom (DataFrame): DataFrame containing crop code and TBS values.</p>"},{"location":"reference/api/io/#io.inputs.OPC.validate","title":"<code>validate(duration=None)</code>","text":"<p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p> <p>Parameters: duration (int, optional): Duration of the simulation in years. If None, uses the maximum Yid.</p> <p>Returns: bool: True if the data is valid, False otherwise.</p>"},{"location":"reference/api/io/#io.inputs.SIT","title":"<code>SIT</code>","text":"<p>Methods:</p> Name Description <code>load</code> <p>Class method to load the .sit file and return a SiteFile instance.</p> <code>save</code> <p>Save the current site information to a .SIT file.</p>"},{"location":"reference/api/io/#io.inputs.SIT.elevation","title":"<code>elevation</code>  <code>property</code> <code>writable</code>","text":"<p>Get elevation value.</p>"},{"location":"reference/api/io/#io.inputs.SIT.lat","title":"<code>lat</code>  <code>property</code> <code>writable</code>","text":"<p>Get latitude value.</p>"},{"location":"reference/api/io/#io.inputs.SIT.lon","title":"<code>lon</code>  <code>property</code> <code>writable</code>","text":"<p>Get longitude value.</p>"},{"location":"reference/api/io/#io.inputs.SIT.slope","title":"<code>slope</code>  <code>property</code> <code>writable</code>","text":"<p>Get slope steep value.</p>"},{"location":"reference/api/io/#io.inputs.SIT.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Class method to load the .sit file and return a SiteFile instance.</p> <p>Parameters: file_path (str): Path to the .sit file.</p> <p>Returns: SiteFile: An instance of the SiteFile class with loaded data.</p>"},{"location":"reference/api/io/#io.inputs.SIT.save","title":"<code>save(output_dir)</code>","text":"<p>Save the current site information to a .SIT file.</p>"},{"location":"reference/api/io/#io.inputs.SOL","title":"<code>SOL</code>","text":"<p>Methods:</p> Name Description <code>from_sda</code> <p>Create a Soil object from Soil Data Access using a query.</p> <code>load</code> <p>Load soil data from a file and return a Soil object.</p> <code>save</code> <p>Save the soil data to a file using a template.</p>"},{"location":"reference/api/io/#io.inputs.SOL.from_sda","title":"<code>from_sda(query)</code>  <code>classmethod</code>","text":"<p>Create a Soil object from Soil Data Access using a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>int or str</code> <p>Query string for SoilDataAccess. (mukey or WKT str) ( \"POINT(-123.4567 45.6789)\" )</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from SDA.</p>"},{"location":"reference/api/io/#io.inputs.SOL.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Load soil data from a file and return a Soil object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the soil file.</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from the file.</p>"},{"location":"reference/api/io/#io.inputs.SOL.save","title":"<code>save(filepath, template=None)</code>","text":"<p>Save the soil data to a file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the soil file.</p> required <code>template</code> <code>list</code> <p>Optional list of template lines.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If soil properties DataFrame is empty.</p>"},{"location":"reference/api/io/#io.inputs.dly","title":"<code>dly</code>","text":"<p>Classes:</p> Name Description <code>DLY</code>"},{"location":"reference/api/io/#io.inputs.dly.DLY","title":"<code>DLY</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>load</code> <p>Load data from a DLY file into DataFrame.</p> <code>save</code> <p>Save DataFrame into a DLY file.</p> <code>to_monthly</code> <p>Save as monthly file</p> <code>validate</code> <p>Validate the DataFrame to ensure it contains a continuous range of dates </p>"},{"location":"reference/api/io/#io.inputs.dly.DLY.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load data from a DLY file into DataFrame.</p>"},{"location":"reference/api/io/#io.inputs.dly.DLY.save","title":"<code>save(path=None)</code>","text":"<p>Save DataFrame into a DLY file.</p>"},{"location":"reference/api/io/#io.inputs.dly.DLY.to_monthly","title":"<code>to_monthly(path=None)</code>","text":"<p>Save as monthly file</p>"},{"location":"reference/api/io/#io.inputs.dly.DLY.validate","title":"<code>validate(start_date, end_date)</code>","text":"<p>Validate the DataFrame to ensure it contains a continuous range of dates  between start_date and end_date, without duplicates.</p>"},{"location":"reference/api/io/#io.inputs.opc","title":"<code>opc</code>","text":"<p>Classes:</p> Name Description <code>OPC</code>"},{"location":"reference/api/io/#io.inputs.opc.OPC","title":"<code>OPC</code>","text":"<p>               Bases: <code>DataFrame</code></p> <p>Methods:</p> Name Description <code>append</code> <p>Append another OPC or DataFrame to the current OPC instance.</p> <code>edit_crop_season</code> <p>Edit the planting and/or harvest dates for a given year and crop.</p> <code>edit_fertilizer_rate</code> <p>Edit the fertilizer rate for a given year.</p> <code>edit_harvest_date</code> <p>Edit the harvest date for a given year and crop.</p> <code>edit_operation_date</code> <p>Edit the operation date for a given year.</p> <code>edit_operation_value</code> <p>Edit the operation value for a given year.</p> <code>edit_plantation_date</code> <p>Edit the plantation date for a given year and crop.</p> <code>get_harvest_date</code> <p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <code>get_plantation_date</code> <p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <code>iter_seasons</code> <p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <code>load</code> <p>Load data from an OPC file into DataFrame.</p> <code>new</code> <p>Create a new OPC instance with an empty DataFrame with preset columns, a default header,</p> <code>remove</code> <p>Remove operation(s) from the OPC file that match all provided criteria.</p> <code>save</code> <p>Save DataFrame into an OPC file.</p> <code>update</code> <p>Add or update an operation in the OPC file.</p> <code>update_phu</code> <p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <code>validate</code> <p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.IAUI","title":"<code>IAUI</code>  <code>property</code> <code>writable</code>","text":"<p>Get the auto-irrigation implement ID from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if auto-irrigation is enabled (72), False if disabled (0)</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.LUN","title":"<code>LUN</code>  <code>property</code> <code>writable</code>","text":"<p>Get the land use number from the OPC file header.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The land use number from the first 4 characters of header line 2</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.append","title":"<code>append(second_opc)</code>","text":"<p>Append another OPC or DataFrame to the current OPC instance. Args:     second_opc (pd.DataFrame or OPC): The data to append. Returns:     OPC: A new OPC instance with combined data. Raises:     ValueError: If second_opc is not a pandas DataFrame or OPC instance.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.edit_crop_season","title":"<code>edit_crop_season(new_planting_date=None, new_harvest_date=None, crop_code=None)</code>","text":"<p>Edit the planting and/or harvest dates for a given year and crop.</p> <p>Parameters: year (int): Year. new_planting_date (datetime, optional): New planting date. If not provided, only harvest date will be updated. new_harvest_date (datetime, optional): New harvest date. If not provided, only planting date will be updated. crop_code (int, optional): Crop code. If not provided, changes the first crop found.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.edit_fertilizer_rate","title":"<code>edit_fertilizer_rate(rate, year=2020, month=None, day=None)</code>","text":"<p>Edit the fertilizer rate for a given year.</p> <p>Parameters: rate (float): Fertilizer rate to be set. year (int, optional): Year for the fertilizer rate application. Defaults to 2020. month (int, optional): Month for the fertilizer rate application. If not provided, the first instance is changed. day (int, optional): Day for the fertilizer rate application. Defaults to None.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.edit_harvest_date","title":"<code>edit_harvest_date(date, crop_code)</code>","text":"<p>Edit the harvest date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of harvest in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.edit_operation_date","title":"<code>edit_operation_date(code, year, month, day, crop_code=None)</code>","text":"<p>Edit the operation date for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. month (int): Month of operation. day (int): Day of operation. crop_code (int, optional): Crop code.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.edit_operation_value","title":"<code>edit_operation_value(code, year, value, crop_code=None)</code>","text":"<p>Edit the operation value for a given year.</p> <p>Parameters: code (str): Operation code. year (int): Year. value (float): New operation value. crop_code (int, optional): Crop code.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.edit_plantation_date","title":"<code>edit_plantation_date(date, crop_code)</code>","text":"<p>Edit the plantation date for a given year and crop.</p> <p>Parameters: date (datetime or str): Date of plantation in datetime object or 'YYYY-MM-DD' string format. crop_code (int): Crop code.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.get_harvest_date","title":"<code>get_harvest_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the harvest date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their harvest dates with row indices.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.get_plantation_date","title":"<code>get_plantation_date(year=None, crop_code=None)</code>","text":"<p>Retrieve the plantation date(s) for a specific year and/or crop code.</p> <p>Parameters: year (int, optional): Year. Defaults to None. crop_code (int, optional): Crop code. Defaults to None.</p> <p>Returns: dict: A dictionary of crop codes and their plantation dates with row indices.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.iter_seasons","title":"<code>iter_seasons(start_year=None, end_year=None)</code>","text":"<p>Iterate over OPC data, yielding dictionaries containing information for each growing season.</p> <p>Parameters: start_year (int, optional): The starting year to consider. Defaults to None. end_year (int, optional): The ending year to consider. Defaults to None.</p> <p>dict: A dictionary containing:     - plantation_date: The date of plantation     - harvest_date: The date of harvest     - crop_code: The crop code     - operations: A subset of OPC rows for this season     - plantation_index: The index of the plantation row</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.load","title":"<code>load(path, start_year=None)</code>  <code>classmethod</code>","text":"<p>Load data from an OPC file into DataFrame.</p> <p>Parameters: path (str): Path to the OPC file. start_year (int, optional): Start year for the OPC file. If not provided, it will be read from the file header.</p> <p>Returns: OPC: An instance of the OPC class containing the loaded data.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.new","title":"<code>new(name, start_year)</code>  <code>classmethod</code>","text":"<p>Create a new OPC instance with an empty DataFrame with preset columns, a default header, and provided name and start year.</p> <p>Parameters: name (str): The name for this OPC file/instance. start_year (int): The start year to assign in the OPC instance.</p> <p>Returns: OPC: An OPC instance with no data but with the required metadata set.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.remove","title":"<code>remove(opID=None, date=None, cropID=None, XMTU=None, fertID=None, year=None)</code>","text":"<p>Remove operation(s) from the OPC file that match all provided criteria.</p> <p>Parameters:</p> Name Type Description Default <code>opID</code> <code>int</code> <p>Operation ID to match</p> <code>None</code> <code>date</code> <code>str</code> <p>Date to match in format 'YYYY-MM-DD'</p> <code>None</code> <code>cropID</code> <code>int</code> <p>Crop ID to match</p> <code>None</code> <code>XMTU/LYR/pestID/fertID</code> <code>int</code> <p>Machine type/layer/pesticide ID/fertilizer ID to match</p> required <code>year</code> <code>int</code> <p>Year to match</p> <code>None</code>"},{"location":"reference/api/io/#io.inputs.opc.OPC.save","title":"<code>save(path)</code>","text":"<p>Save DataFrame into an OPC file.</p> <p>Parameters: path (str): Path to save the OPC file.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.update","title":"<code>update(operation)</code>","text":"<p>Add or update an operation in the OPC file.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>dict</code> <p>Dictionary containing operation details with keys: - opID: Operation ID (required) - cropID: Crop ID (required) - date: Operation date as string 'YYYY-MM-DD' (required) - XMTU/LYR/pestID/fertID: Machine type/years/pesticide ID/fertilizer ID (optional, default 0) - OPV1-OPV8: Additional operation values (optional, default 0)</p> required"},{"location":"reference/api/io/#io.inputs.opc.OPC.update_phu","title":"<code>update_phu(dly, cropcom)</code>","text":"<p>Update the OPV1 value with the calculated PHU from the DLY data for all plantation dates.</p> <p>Parameters: dly (DLY): DLY object containing weather data. cropcom (DataFrame): DataFrame containing crop code and TBS values.</p>"},{"location":"reference/api/io/#io.inputs.opc.OPC.validate","title":"<code>validate(duration=None)</code>","text":"<p>Validate the OPC data to ensure it contains a continuous range of dates without duplicates.</p> <p>Parameters: duration (int, optional): Duration of the simulation in years. If None, uses the maximum Yid.</p> <p>Returns: bool: True if the data is valid, False otherwise.</p>"},{"location":"reference/api/io/#io.inputs.sit","title":"<code>sit</code>","text":"<p>Classes:</p> Name Description <code>SIT</code>"},{"location":"reference/api/io/#io.inputs.sit.SIT","title":"<code>SIT</code>","text":"<p>Methods:</p> Name Description <code>load</code> <p>Class method to load the .sit file and return a SiteFile instance.</p> <code>save</code> <p>Save the current site information to a .SIT file.</p>"},{"location":"reference/api/io/#io.inputs.sit.SIT.elevation","title":"<code>elevation</code>  <code>property</code> <code>writable</code>","text":"<p>Get elevation value.</p>"},{"location":"reference/api/io/#io.inputs.sit.SIT.lat","title":"<code>lat</code>  <code>property</code> <code>writable</code>","text":"<p>Get latitude value.</p>"},{"location":"reference/api/io/#io.inputs.sit.SIT.lon","title":"<code>lon</code>  <code>property</code> <code>writable</code>","text":"<p>Get longitude value.</p>"},{"location":"reference/api/io/#io.inputs.sit.SIT.slope","title":"<code>slope</code>  <code>property</code> <code>writable</code>","text":"<p>Get slope steep value.</p>"},{"location":"reference/api/io/#io.inputs.sit.SIT.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Class method to load the .sit file and return a SiteFile instance.</p> <p>Parameters: file_path (str): Path to the .sit file.</p> <p>Returns: SiteFile: An instance of the SiteFile class with loaded data.</p>"},{"location":"reference/api/io/#io.inputs.sit.SIT.save","title":"<code>save(output_dir)</code>","text":"<p>Save the current site information to a .SIT file.</p>"},{"location":"reference/api/io/#io.inputs.sol","title":"<code>sol</code>","text":"<p>Classes:</p> Name Description <code>SOL</code>"},{"location":"reference/api/io/#io.inputs.sol.SOL","title":"<code>SOL</code>","text":"<p>Methods:</p> Name Description <code>from_sda</code> <p>Create a Soil object from Soil Data Access using a query.</p> <code>load</code> <p>Load soil data from a file and return a Soil object.</p> <code>save</code> <p>Save the soil data to a file using a template.</p>"},{"location":"reference/api/io/#io.inputs.sol.SOL.from_sda","title":"<code>from_sda(query)</code>  <code>classmethod</code>","text":"<p>Create a Soil object from Soil Data Access using a query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>int or str</code> <p>Query string for SoilDataAccess. (mukey or WKT str) ( \"POINT(-123.4567 45.6789)\" )</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from SDA.</p>"},{"location":"reference/api/io/#io.inputs.sol.SOL.load","title":"<code>load(filepath)</code>  <code>classmethod</code>","text":"<p>Load soil data from a file and return a Soil object.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the soil file.</p> required <p>Returns:</p> Name Type Description <code>Soil</code> <p>A new Soil object populated with data from the file.</p>"},{"location":"reference/api/io/#io.inputs.sol.SOL.save","title":"<code>save(filepath, template=None)</code>","text":"<p>Save the soil data to a file using a template.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to save the soil file.</p> required <code>template</code> <code>list</code> <p>Optional list of template lines.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If soil properties DataFrame is empty.</p>"},{"location":"reference/api/io/#io.outputs","title":"<code>outputs</code>","text":"<p>Classes:</p> Name Description <code>ACY</code> <code>DGN</code> <code>DSL</code> <code>DWC</code>"},{"location":"reference/api/io/#io.outputs.ACY","title":"<code>ACY</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the ACY data.</p>"},{"location":"reference/api/io/#io.outputs.ACY.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the ACY data.</p>"},{"location":"reference/api/io/#io.outputs.DGN","title":"<code>DGN</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DGN data.</p>"},{"location":"reference/api/io/#io.outputs.DGN.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DGN data.</p>"},{"location":"reference/api/io/#io.outputs.DSL","title":"<code>DSL</code>","text":"<p>Methods:</p> Name Description <code>get_data</code> <p>Return stored water data.</p>"},{"location":"reference/api/io/#io.outputs.DSL.get_data","title":"<code>get_data()</code>","text":"<p>Return stored water data.</p>"},{"location":"reference/api/io/#io.outputs.DWC","title":"<code>DWC</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DWC data.</p>"},{"location":"reference/api/io/#io.outputs.DWC.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DWC data.</p>"},{"location":"reference/api/io/#io.outputs.all","title":"<code>all</code>","text":"<p>Classes:</p> Name Description <code>ACY</code> <code>DGN</code> <code>DWC</code>"},{"location":"reference/api/io/#io.outputs.all.ACY","title":"<code>ACY</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the ACY data.</p>"},{"location":"reference/api/io/#io.outputs.all.ACY.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the ACY data.</p>"},{"location":"reference/api/io/#io.outputs.all.DGN","title":"<code>DGN</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DGN data.</p>"},{"location":"reference/api/io/#io.outputs.all.DGN.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DGN data.</p>"},{"location":"reference/api/io/#io.outputs.all.DWC","title":"<code>DWC</code>","text":"<p>Methods:</p> Name Description <code>get_var</code> <p>Extract variable from the DWC data.</p>"},{"location":"reference/api/io/#io.outputs.all.DWC.get_var","title":"<code>get_var(varname)</code>","text":"<p>Extract variable from the DWC data.</p>"},{"location":"reference/api/io/#io.outputs.sw","title":"<code>sw</code>","text":"<p>Classes:</p> Name Description <code>DSL</code>"},{"location":"reference/api/io/#io.outputs.sw.DSL","title":"<code>DSL</code>","text":"<p>Methods:</p> Name Description <code>get_data</code> <p>Return stored water data.</p>"},{"location":"reference/api/io/#io.outputs.sw.DSL.get_data","title":"<code>get_data()</code>","text":"<p>Return stored water data.</p>"},{"location":"reference/api/io/#io.parm","title":"<code>parm</code>","text":"<p>Classes:</p> Name Description <code>Parm</code>"},{"location":"reference/api/io/#io.parm.Parm","title":"<code>Parm</code>","text":"<p>Methods:</p> Name Description <code>constraints</code> <p>Returns the constraints (min, max ranges) for the parameters.</p> <code>edit</code> <p>Updates the parameters in the DataFrame with new values.</p> <code>get_vars</code> <p>Returns the vars DataFrame with an additional column containing current values.</p> <code>read_parm</code> <p>Reads and constructs a DataFrame from a .DAT file.</p> <code>save</code> <p>Saves the current DataFrame to a .DAT file.</p> <code>set_sensitive</code> <p>Sets sensitive parameters based on a CSV path or list of parameter names.</p>"},{"location":"reference/api/io/#io.parm.Parm.current","title":"<code>current</code>  <code>property</code>","text":"<p>Returns the current values of parameters in the DataFrame.</p>"},{"location":"reference/api/io/#io.parm.Parm.constraints","title":"<code>constraints()</code>","text":"<p>Returns the constraints (min, max ranges) for the parameters.</p>"},{"location":"reference/api/io/#io.parm.Parm.edit","title":"<code>edit(values)</code>","text":"<p>Updates the parameters in the DataFrame with new values.</p>"},{"location":"reference/api/io/#io.parm.Parm.get_vars","title":"<code>get_vars()</code>","text":"<p>Returns the vars DataFrame with an additional column containing current values.</p>"},{"location":"reference/api/io/#io.parm.Parm.read_parm","title":"<code>read_parm(file_name)</code>","text":"<p>Reads and constructs a DataFrame from a .DAT file.</p>"},{"location":"reference/api/io/#io.parm.Parm.save","title":"<code>save(path)</code>","text":"<p>Saves the current DataFrame to a .DAT file.</p>"},{"location":"reference/api/io/#io.parm.Parm.set_sensitive","title":"<code>set_sensitive(parms_input, all=False)</code>","text":"<p>Sets sensitive parameters based on a CSV path or list of parameter names. If <code>all</code> is True, all parameters are considered sensitive.</p> <p>Parameters:</p> Name Type Description Default <code>parms_input</code> <code>str or list</code> <p>Either a CSV file path or list of parameter names to select</p> required <code>all</code> <code>bool</code> <p>If True, all parameters are considered sensitive regardless of input</p> <code>False</code>"},{"location":"reference/api/soil/","title":"Soil Module","text":""},{"location":"reference/api/soil/#soil","title":"<code>soil</code>","text":"<p>Classes:</p> Name Description <code>SoilDataAccess</code>"},{"location":"reference/api/soil/#soil.SoilDataAccess","title":"<code>SoilDataAccess</code>","text":"<p>Methods:</p> Name Description <code>fetch_properties</code> <p>Fetches soil data based on the input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <code>fetch_slope_length</code> <p>Fetches the slope length for a given input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <code>fetch_value</code> <p>Fetches specific values from a given table for a given input value.</p> <code>get_cokey_from_wkt</code> <p>Fetches a list of cokey (Component Key) values based on a WKT spatial location.</p> <code>get_mukey</code> <p>Fetches the mukey for a given WKT location.</p> <code>get_mukey_list</code> <p>Fetches the mukey for a given WKT location.</p> <code>query</code> <p>Performs a query to the NRCS Soil Data Access service.</p>"},{"location":"reference/api/soil/#soil.SoilDataAccess.fetch_properties","title":"<code>fetch_properties(input_value)</code>  <code>staticmethod</code>","text":"<p>Fetches soil data based on the input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <p>Args: input (int or str): The input value representing either a mukey (int) or a WKT location (str).</p> <p>Returns: pd.DataFrame: A DataFrame containing the soil data for the specified input.</p>"},{"location":"reference/api/soil/#soil.SoilDataAccess.fetch_slope_length","title":"<code>fetch_slope_length(input_value)</code>  <code>staticmethod</code>","text":"<p>Fetches the slope length for a given input value. If the input is an integer, it is used as mukey. If the input is a string, it is used as WKT.</p> <p>Args: input (int or str): The input value representing either a mukey (int) or a WKT location (str).</p> <p>Returns: float: The slope length for the specified input value.</p>"},{"location":"reference/api/soil/#soil.SoilDataAccess.fetch_value","title":"<code>fetch_value(input_value, values, table)</code>  <code>staticmethod</code>","text":"<p>Fetches specific values from a given table for a given input value.</p> <p>Args: input_value (int or str): The input value representing either a mukey (int) or a WKT location (str). values (List[str]): A list of column names for the values to fetch. table (str): The name of the table to query.</p> <p>Returns: List[Dict[str, Any]]: A list of dictionaries containing the key (mukey, cokey, etc.) and fetched values.</p>"},{"location":"reference/api/soil/#soil.SoilDataAccess.get_cokey_from_wkt","title":"<code>get_cokey_from_wkt(wkt_string)</code>  <code>staticmethod</code>","text":"<p>Fetches a list of cokey (Component Key) values based on a WKT spatial location.</p> <p>Args: wkt_string (str): A WKT string representing the spatial location (e.g., POLYGON, POINT).</p> <p>Returns: List[int]: A list of cokey values corresponding to the input WKT location.</p>"},{"location":"reference/api/soil/#soil.SoilDataAccess.get_mukey","title":"<code>get_mukey(wkt)</code>  <code>staticmethod</code>","text":"<p>Fetches the mukey for a given WKT location.</p> <p>Args: wkt (str): The WKT location.</p> <p>Returns: int: The mukey for the specified location.</p>"},{"location":"reference/api/soil/#soil.SoilDataAccess.get_mukey_list","title":"<code>get_mukey_list(wkt)</code>  <code>staticmethod</code>","text":"<p>Fetches the mukey for a given WKT location.</p> <p>Args: wkt (str): The WKT location.</p> <p>Returns: int: The mukey for the specified location.</p>"},{"location":"reference/api/soil/#soil.SoilDataAccess.query","title":"<code>query(query)</code>  <code>staticmethod</code>","text":"<p>Performs a query to the NRCS Soil Data Access service.</p> <p>Args: query (str): The SQL query to execute.</p> <p>Returns: pd.DataFrame: A DataFrame containing the results of the query.</p> <p>Raises: ValueError: If no data is found for the provided query. requests.RequestException: If there is an issue with the network request.</p>"},{"location":"reference/api/utils/","title":"Utils Module","text":""},{"location":"reference/api/utils/#utils","title":"<code>utils</code>","text":"<p>Classes:</p> Name Description <code>GeoInterface</code> <p>A class that provides a unified interface for working with geospatial data from various sources.</p> <p>Functions:</p> Name Description <code>sample_raster_nearest</code> <p>Sample a raster file at specific coordinates, taking the nearest pixel.</p> <code>reproject_crop_raster</code> <p>Reproject and crop a raster file.</p> <code>copy_file</code> <p>Copy a file from source to destination, optionally creating a symbolic link instead.</p> <code>parallel_executor</code> <p>Pebble-only parallel executor.</p> <code>read_gdb_layer</code> <p>Reads selected columns from a GDB layer and returns them in a pandas DataFrame.</p>"},{"location":"reference/api/utils/#utils.GeoInterface","title":"<code>GeoInterface</code>","text":"<p>A class that provides a unified interface for working with geospatial data from various sources.</p> <p>This class can load and process data from raster files (.tif/.tiff), CSV files, shapefiles, or pandas DataFrames. It provides methods for finding nearest neighbors using haversine distances between geographic coordinates.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>The loaded data, containing at minimum 'lat' and 'lon' columns.</p> <code>points_rad</code> <code>ndarray</code> <p>The latitude/longitude points converted to radians.</p> <code>tree</code> <code>BallTree</code> <p>A BallTree structure for efficient nearest neighbor queries.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>str or DataFrame</code> <p>The input data source. Can be: - Path to a raster file (.tif/.tiff) - Path to a CSV file (.csv) - Path to a shapefile (.shp) - A pandas DataFrame with 'lat' and 'lon' columns</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the data source format is unsupported or required columns are missing.</p> <p>Methods:</p> Name Description <code>find_nearest</code> <p>Find the nearest 'k' data points for each latitude and longitude provided separately.</p> <code>lookup</code> <p>Find the nearest data point to a single latitude and longitude.</p>"},{"location":"reference/api/utils/#utils.GeoInterface.find_nearest","title":"<code>find_nearest(lats, lons, k=1)</code>","text":"<p>Find the nearest 'k' data points for each latitude and longitude provided separately.</p> <p>Parameters:</p> Name Type Description Default <code>lats</code> <code>list of float</code> <p>A list of latitudes.</p> required <code>lons</code> <code>list of float</code> <p>A list of longitudes.</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors to find.</p> <code>1</code> <p>Returns:</p> Type Description <p>list or pandas.DataFrame: Depending on 'k', returns a DataFrame or a list of DataFrames with the nearest points.</p>"},{"location":"reference/api/utils/#utils.GeoInterface.lookup","title":"<code>lookup(lat, lon)</code>","text":"<p>Find the nearest data point to a single latitude and longitude.</p> <p>Parameters:</p> Name Type Description Default <code>lat</code> <code>float</code> <p>Latitude of the query point.</p> required <code>lon</code> <code>float</code> <p>Longitude of the query point.</p> required <p>Returns:</p> Type Description <p>pandas.Series: The row from the DataFrame corresponding to the nearest point.</p>"},{"location":"reference/api/utils/#utils.sample_raster_nearest","title":"<code>sample_raster_nearest(raster_file, coords, crs='EPSG:4326')</code>","text":"<p>Sample a raster file at specific coordinates, taking the nearest pixel.</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>str</code> <p>Path to the raster file.</p> required <code>coords</code> <code>list of tuples</code> <p>List of (x, y)/(lon, lat) tuples.</p> required <code>crs</code> <code>str</code> <p>The CRS the coords are in.</p> <code>'EPSG:4326'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with band names as keys and lists of pixel values at the given coordinates as values.</p>"},{"location":"reference/api/utils/#utils.reproject_crop_raster","title":"<code>reproject_crop_raster(src, dst, out_epsg, min_coords, max_coords)</code>","text":"<p>Reproject and crop a raster file. src_filename: Source file path. dst_filename: Destination file path. out_epsg: Output coordinate system as EPSG code. min_lon, min_lat, max_lon, max_lat: Bounding box coordinates.</p>"},{"location":"reference/api/utils/#utils.copy_file","title":"<code>copy_file(src, dest, symlink=False)</code>","text":"<p>Copy a file from source to destination, optionally creating a symbolic link instead.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>str</code> <p>Path to the source file</p> required <code>dest</code> <code>str</code> <p>Path to the destination file/link</p> required <code>symlink</code> <code>bool</code> <p>Whether to create a symbolic link instead of copying.  Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>str | None: Path to the destination file if successful, None if source doesn't exist</p> Note <p>If symlink is True and the destination already exists, it will be removed first.</p>"},{"location":"reference/api/utils/#utils.parallel_executor","title":"<code>parallel_executor(func, args, method='Process', max_workers=10, return_value=False, bar=True, timeout=None, verbose=True)</code>","text":"<p>Pebble-only parallel executor. - method: 'Process' or 'Thread' - args: iterable of items; each item can be a single arg or a tuple for *args - timeout: per-task timeout (sec). For ProcessPool it is enforced at schedule(),            for ThreadPool it's enforced at result(). Returns: (results, failed_indices)</p>"},{"location":"reference/api/utils/#utils.read_gdb_layer","title":"<code>read_gdb_layer(gdb_data, layer_name, columns=None, names=None)</code>","text":"<p>Reads selected columns from a GDB layer and returns them in a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>gdb</code> <code>gdb</code> <p>The GDB file opened by ogr.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer to read.</p> required <code>columns</code> <code>list</code> <p>List of column indices to read. If None, all columns are read.</p> <code>None</code> <code>names</code> <code>list</code> <p>List of column names corresponding to the indices in <code>columns</code>. If None, all column names are inferred from the layer definition.</p> <code>None</code> <p>Returns:</p> Type Description <p>pd.DataFrame: The resulting dataframe.</p>"},{"location":"reference/api/weather/","title":"Weather Module","text":""},{"location":"reference/api/weather/#weather","title":"<code>weather</code>","text":"<p>Functions:</p> Name Description <code>fetch_list</code> <p>Fetches weather data based on the input type which could be coordinates, a CSV file, or a shapefile.</p>"},{"location":"reference/api/weather/#weather.fetch_list","title":"<code>fetch_list(config_file, input_data, output_dir, raw=False)</code>","text":"<p>Fetches weather data based on the input type which could be coordinates, a CSV file, or a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>str</code> <p>Could be latitude and longitude as a string, path to a CSV file, or path to a shapefile.</p> required <code>output_dir</code> <code>str</code> <p>Directory or file path where the output should be saved.</p> required <code>raw</code> <code>bool</code> <p>Whether to save as raw CSV (True) or DLY format (False). Defaults to False.</p> <code>False</code>"},{"location":"tutorials/calibration/","title":"GeoEPIC Yield Calibration Tutorial","text":"<p>This tutorial outlines the steps to perform yield calibration for multiple sites using GeoEPIC, leveraging the setup from regional simulations. Calibration fine-tunes model parameters to better match simulated outputs (like crop yield) with observed data across a region.</p> <ol> <li> <p>Set Up the Workspace:     Before starting calibration, you must have a properly organized workspace containing all necessary input files (Site, Soil, Weather, Operations) for the multiple sites in your calibration region. Follow the procedures detailed in the Regional Simulation Tutorial, specifically Steps 1-3 (Set Up Environment, Prepare Input Files, Set Up Workspace). This ensures you have:</p> <ul> <li>A configured <code>geoEpic</code> environment.</li> <li>All required <code>.SIT</code>, <code>.SOL</code>, <code>.DLY</code>, and <code>.OPC</code> files for each site.</li> <li>A well-structured workspace directory (e.g., <code>workspace/</code>) containing these inputs and the EPIC model executable.</li> </ul> </li> <li> <p>Perform Calibration:     The core calibration process involves selecting parameters to adjust (e.g., crop parameters in <code>.OPC</code> or site/soil parameters), defining an objective function (how to measure the match between simulated and observed yields), and using a calibration algorithm or workflow to find optimal parameter values.     For detailed, step-by-step instructions on:</p> <ul> <li>Preparing observed yield data.</li> <li>Selecting appropriate parameters for calibration.</li> <li>Configuring and running a calibration workflow or tool (potentially using libraries like <code>spotpy</code> or custom <code>geoEpic</code> calibration utilities).</li> <li>Evaluating calibration performance. Please refer to the dedicated Calibration Procedure Guide. This guide provides the specific commands, scripts, and methodologies required to execute the yield calibration.</li> </ul> </li> <li> <p>Access Calibrated Parameters:     Once the calibration process (as described in the Calibration Procedure Guide) is complete, the optimized parameter values will typically be stored in output files or potentially within Python objects generated by the calibration script.</p> <ul> <li>The specific format and location depend on the calibration tool or script used. It might be a CSV file, a JSON file, or directly printed to the console. Example conceptual access (actual implementation depends on the calibration tool):   ``   # Example: Assuming calibration results are saved to a file   import pandas as pd</li> </ul> <p>try:       # Load parameters maybe saved by the calibration tool       calibrated_params = pd.read_csv('./workspace/calibration_outputs/best_parameters.csv')       print(\"Calibrated Parameters:\")       print(calibrated_params)</p> <pre><code>  # Or access parameters if stored in a Python object (conceptual)\n  # best_run_parameters = calibration_result.best_parameters\n  # print(best_run_parameters)\n</code></pre> <p>except FileNotFoundError:       print(\"Calibration output file not found. Check calibration process outputs.\")   except Exception as e:       print(f\"Error accessing calibration results: {e}\")</p> <p>`` Refer back to the Calibration Procedure Guide for specifics on where and how to retrieve the final calibrated parameter set generated by your chosen calibration method. These parameters can then be used for subsequent validated regional simulations.</p> </li> </ol>"},{"location":"tutorials/regional_simulation/","title":"GeoEPIC Regional Simulation Tutorial","text":"<p>This tutorial provides step-by-step guidance on setting up and running simulations for multiple sites across a region, often as part of a calibration process, using GeoEPIC.</p> <ol> <li> <p>Set Up Environment:     As with single-site simulations, the first step is to configure your local machine and install the necessary <code>geoEpic</code> components. This ensures your system is ready for both single-site and regional-scale tasks. For detailed instructions, please refer to the Installation Page. Completing this step is a prerequisite for all subsequent <code>geoEpic</code> operations.</p> </li> <li> <p>Prepare Input Files for Multiple Sites:     For regional simulations, you need the standard input files (Site - .SIT, Soil - .SOL, Weather - .DLY, Operation Schedule - .OPC) for each site within your study region. This typically involves batch processing or scripting to generate files for numerous locations. Consistent naming conventions (e.g., using site IDs) are crucial for managing these files.</p> <ul> <li>Refer to the Site File Section, Soil File Section, Weather File Section, and Operation Schedule File Section in the getting started menu for details on the format and generation of each file type.</li> <li>You might use scripts to loop through a list of site coordinates or identifiers to automate the generation and saving of these files. Example concept for downloading weather for multiple sites:     <pre><code>from geoEpic.weather import get_daymet_data\nfrom geoEpic.io import DLY\nimport os\n\nsites = [\n    {'id': 'siteA', 'lat': 35.1, 'lon': -78.5},\n    {'id': 'siteB', 'lat': 35.2, 'lon': -78.6},\n    # ... more sites\n]\nstart_date, end_date = '2015-01-01', '2019-12-31'\noutput_weather_dir = './workspace/weather/'\nos.makedirs(output_weather_dir, exist_ok=True)\n\nfor site_info in sites:\n    site_id = site_info['id']\n    lat, lon = site_info['lat'], site_info['lon']\n    print(f\"Processing weather for {site_id}...\")\n    try:\n        df = get_daymet_data(lat, lon, start_date, end_date)\n        dly_path = os.path.join(output_weather_dir, f\"{site_id}.DLY\")\n        DLY(df).save(dly_path)\n    except Exception as e:\n        print(f\"Error processing {site_id}: {e}\")\n</code></pre> Successfully preparing these files systematically for all sites is essential for regional analysis.</li> </ul> </li> <li> <p>Set Up Workspace and Folder Structure:     Managing inputs and outputs for numerous sites requires an organized workspace. A well-defined folder structure is critical for regional runs and calibration workflows.</p> <ul> <li>It's recommended to create a main <code>workspace</code> directory. Inside, organize subdirectories for each input type and for outputs, often with further subdirectories per site or run configuration.</li> <li>Example Structure:     <pre><code>workspace/\n\u251c\u2500\u2500 sites/             # .SIT files (e.g., siteA.SIT, siteB.SIT)\n\u251c\u2500\u2500 soil/              # .SOL files (e.g., siteA.SOL, siteB.SOL)\n\u251c\u2500\u2500 weather/           # .DLY files (e.g., siteA.DLY, siteB.DLY)\n\u251c\u2500\u2500 opc/               # .OPC files or templates\n\u2502   \u251c\u2500\u2500 files/         # Generated .OPC files (e.g., siteA.OPC)\n\u2502   \u2514\u2500\u2500 templates/     # OPC templates if using generate_opc\n\u251c\u2500\u2500 model/             # EPIC executable (e.g., EPIC1102.exe)\n\u2514\u2500\u2500 outputs/           # Simulation outputs, potentially nested\n    \u251c\u2500\u2500 run_config_1/\n    \u2502   \u251c\u2500\u2500 siteA/     # Outputs for siteA (siteA.ACY, siteA.DGN)\n    \u2502   \u2514\u2500\u2500 siteB/     # Outputs for siteB\n    \u2514\u2500\u2500 run_config_2/\n        \u2514\u2500\u2500 ...\n</code></pre></li> <li>Refer to the Workspace Setup Guide for best practices and potential templates provided by <code>geoEpic</code>. This organization facilitates batch processing and managing results across the region.</li> </ul> </li> <li> <p>Run Regional Simulation / Calibration:     Executing simulations for multiple sites often involves looping through your site list, configuring the model for each, and running it. Calibration adds complexity by adjusting model parameters based on observed data, typically requiring iterative runs.</p> <ul> <li>This process usually involves scripting to automate the setup and execution for each site within your organized workspace. You might use Python loops, parallel processing libraries (like <code>multiprocessing</code> or <code>dask</code>), or specialized functions within <code>geoEpic</code> if available for batch runs.</li> <li>For detailed strategies on handling multiple sites, managing parameters during calibration, potentially using parallel execution, and interpreting calibration results, please consult the Regional Simulations and Calibration Guide. This guide covers the advanced techniques needed for efficient and effective regional modeling.</li> </ul> </li> <li> <p>Process Outputs:     After running simulations for all sites, outputs (e.g., .ACY, .DGN files) will be available, likely organized within your workspace structure (e.g., in <code>workspace/outputs/run_config_X/siteY/</code>). Processing involves reading these numerous files, aggregating data, and performing regional analysis.</p> <ul> <li> <p>Reading Multiple Output Files:     Use scripts to loop through your output directories and load data for each site.     <pre><code>import os\nimport pandas as pd\nfrom geoEpic.io import ACY # Or DGN, etc.\n\noutput_base_dir = './workspace/outputs/run_config_1/'\nsites_to_process = [d for d in os.listdir(output_base_dir) if os.path.isdir(os.path.join(output_base_dir, d))] # Get site folders\n\nall_yields = []\n\nfor site_id in sites_to_process:\n    acy_path = os.path.join(output_base_dir, site_id, f\"{site_id}.ACY\") # Assuming output ACY is named after site_id\n    if os.path.exists(acy_path):\n        try:\n            yield_df = ACY(acy_path).get_var('YLDG')\n            yield_df['site_id'] = site_id # Add site identifier\n            all_yields.append(yield_df)\n        except Exception as e:\n            print(f\"Error reading {acy_path}: {e}\")\n\n# Combine data from all sites into a single DataFrame\nif all_yields:\n    regional_yields = pd.concat(all_yields, ignore_index=True)\n    print(regional_yields.head())\nelse:\n    print(\"No yield data processed.\")\n</code></pre></p> </li> <li> <p>Regional Analysis and Visualization:     Analyze aggregated data to understand regional patterns, average performance, or variability.     <pre><code>import matplotlib.pyplot as plt\nimport seaborn as sns # For potentially more complex plots\n\nif 'regional_yields' in locals() and not regional_yields.empty:\n    # Example: Boxplot of yields across sites for a specific year\n    year_to_plot = 2017\n    yields_year = regional_yields[regional_yields['YR'] == year_to_plot]\n\n    if not yields_year.empty:\n         plt.figure(figsize=(12, 7))\n         sns.boxplot(x='site_id', y='YLDG', data=yields_year)\n         plt.title(f'Regional Corn Yield Distribution ({year_to_plot})')\n         plt.xlabel('Site ID')\n         plt.ylabel('Yield (t/ha)')\n         plt.xticks(rotation=45, ha='right')\n         plt.tight_layout()\n         plt.show()\n    else:\n        print(f\"No data found for year {year_to_plot}\")\n\n    # Example: Calculate average yield per year across the region\n    avg_regional_yield = regional_yields.groupby('YR')['YLDG'].mean().reset_index()\n    print(\"\\nAverage Regional Yield per Year:\")\n    print(avg_regional_yield)\n</code></pre>     Visualizations might include maps of yields, comparative charts, or plots showing the range of outcomes across the region.</p> </li> <li> <p>Exporting Aggregated Results:     Save the combined regional data for reporting or further analysis.     <pre><code>if 'regional_yields' in locals() and not regional_yields.empty:\n    regional_yields.to_csv('regional_corn_yields_summary.csv', index=False)\n\n# You could also save figures\n# plt.savefig('regional_yield_boxplot_2017.png')\n</code></pre> This final step transforms individual site outputs into meaningful regional insights, often crucial for calibration validation and understanding spatial patterns.</p> </li> </ul> </li> </ol>"},{"location":"tutorials/runepic_old/","title":"Running an EPIC Experiment","text":""},{"location":"tutorials/runepic_old/#1-create-new-workspace","title":"1. Create new workspace","text":"<p><pre><code>epic_pkg workspace new -w Test\ncd Test\n</code></pre> It willl create a new workspace in the new directory named 'Test'. This 'Test' folder will automatically create sub-folders for EPIC model like model, opc, sites, soil, weather and a config.yml doc in it. You need to to go to Test folder before simulation starts.</p>"},{"location":"tutorials/runepic_old/#2-edit-config-file-as-needed","title":"2. Edit config file as needed","text":"<p>This package is mainly designed for study regions in the USA. In the config.yml file, update the settings in the config file based on your area of interest (AOI) and preferences. For example, if you need to run the model for the state of Maryland, USA, modify these rows as follows:</p> <pre><code>EXPName: Nitrogen Assessment (Your Experiment Name) \nRegion: Maryland \ncode: MD \nFields_of_Interests: ./CropRotations/MDRotFilt.shp\n</code></pre> <p>Note: The MDRotFilt.shp file is the same one downloaded and placed in the CropRotations folder in the previous step. <pre><code>soil: \n   ssurgo_gdb: ./soil/gSSURGO_MD.gdb\n</code></pre> Guideline: The rule of thumb is to edit the configuration file using the region code specific to your study region, as done here using \"MD\" throughout the config file.</p>"},{"location":"tutorials/runepic_old/#3-prepare-opc-file","title":"3. Prepare OPC File","text":"<p>OPC refers to the agricultural management practice files which is yet to automated. For now, you need to prpare management files and keep it in a new folder named 'OPC' inside the 'Test' directory.</p>"},{"location":"tutorials/runepic_old/#4-prepare-the-workspace","title":"4. Prepare the workspace","text":"<p><pre><code>epic_pkg workspace prepare\n</code></pre> This command will automatically pre-process the input files before simulation.</p>"},{"location":"tutorials/runepic_old/#5-and-execute-the-simulations","title":"5. And execute the simulations","text":"<p><pre><code>epic_pkg workspace run\n</code></pre> This command will simulate the operation/s and automatically save the results in a new folder named 'Output'. </p> <p>This command will also create a post_process.pynb doc which will have an example code to visualize the required parameters from ACY and DGN files. </p> <p>You can edit this code as per your requirements. You just have to identify the parameters and edit accordingly.</p>"},{"location":"tutorials/runepic_old/#example-visualization","title":"Example Visualization","text":""},{"location":"tutorials/runepic_old/#6-post-process-the-output-visualization","title":"6. Post-process the output visualization","text":"<p>You need to post-process the output files according to your interests. Generally, as an agricultural reserachers you need to process the DGN and ACY files.</p>"},{"location":"tutorials/runepic_old/#for-post-processing","title":"For post-processing","text":"<p><pre><code>epic_pkg workspace post_process\n</code></pre> This will run the example code post_process.pynb which has been created in the Test folder. It will take a variable called 'YLDG' which denotes the yearly yield in t/ha/yr for all the sites and put it in a sepearte column corresponding to all the site ids with creatinh a yldg.csv file.</p>"},{"location":"tutorials/runepic_old/#for-visualization","title":"For visualization","text":"<p><pre><code>epic_pkg workspace visualize\n</code></pre> It will simply plot the 'YLDG' variable corresponding to the site ids and crate a map for study region. </p>"},{"location":"tutorials/runepic_old/#your-plot-will-look-like-this","title":"Your plot will look like this:","text":""},{"location":"tutorials/runexp/","title":"Running the Model","text":""},{"location":"tutorials/runexp/#todo","title":"&lt;ToDo&gt;","text":"<pre><code>from geoEpic import Site, EpicModel\n\n# initialise a site object\nsite = Site(opc = './continuous_corn.OPC',  # management file\n            dly = './1123455.DLY',          # daily weather file \n            sol = './Andisol.SOL',          # soil file\n            sit = './1.SIT')                # site file\n\n\n# initialise the EPIC model\nmodel = EpicModel(path = './model/EPIC2301dt20230820')\nmodel.setup(start_year = 2014, duration = 10)\nmodel.set_output_types(['ACY', 'DGN'])\n\n# run the simulation for the site\nmodel.run(site)\n\n# get the required outputs\nacy = model.outputs['ACY']\nmodel.close()\n</code></pre> <p>Loading from config files:</p> <pre><code>site = Site.from_config(lat = , lon = , config = './config.yml')\nmodel = EpicModel.from_config(config = './config.yml')\n\nmodel.run(site)\nmodel.close()\n</code></pre> <p>Example config file: <pre><code># Model details\nEPICModel: ./model/EPIC2301dt20230820\nstart_year: 1995\nduration: 25\noutput_types:\n  - ACY  # Annual Crop data file\n  - DGN  # Daily general output file\nlog_dir: ./log\noutput_dir: ./output\n</code></pre></p> <ul> <li>To edit the OPC, SOL or files in the epic model folder, you could use the epic_editor. the following command will copy the epiceditor in to your current folder.</li> </ul> <pre><code>&gt;&gt; GeoEPIC workspace add epic_editor\n</code></pre>"},{"location":"tutorials/single_site_simulation/","title":"GeoEPIC Single Site Tutorial","text":"<p>This tutorial provides step-by-step guidance on setting up and running a single site simulation in GeoEPIC.</p> <ol> <li> <p>Set Up Environment:     This initial step involves configuring your local machine and installing the necessary <code>geoEpic</code> components. For detailed instructions on downloading and running the setup script (<code>epic_setup.bat</code>), please refer to the Installation Page. Completing this step ensures your system is ready to use <code>geoEpic</code> functionalities.</p> </li> <li> <p>Prepare Input Files:     This step involves gathering or creating the four essential input files required by the EPIC model: Site File (.SIT), Soil File (.SOL), Weather File (.DLY), and Operation Schedule File (.OPC). These files contain the specific environmental data and management practices for your simulation site.</p> <ul> <li>For detailed guidance on creating/downloading the Site (.SIT) file, refer to the Site File Section in the getting started menu. Example using <code>geoEpic</code>:     <pre><code>from geoEpic.io import SIT\nsit_file = SIT.load('./umstead.SIT')\nsit_file.area = 1.5\nsit_file.save('umstead_new.SIT')\n</code></pre></li> <li>For detailed guidance on creating/downloading the Soil (.SOL) file, refer to the Soil File Section in the getting started menu. Example using <code>geoEpic</code>:     <pre><code>from geoEpic.io import SOL\nfrom geoEpic.utils import Wicket\nlat, lon = 35.890, -78.750\nwicket = Wicket.from_wkt(f\"POINT({lon} {lat})\")\nsoil_file = SOL.from_sda(wicket)\nsoil_file.save('./soil/files/umstead.SOL')\n</code></pre></li> <li>For detailed guidance on creating/downloading the Weather (.DLY) file, refer to the Weather File Section in the getting started menu. Example using <code>geoEpic</code>:     <pre><code>from geoEpic.weather import get_daymet_data\nfrom geoEpic.io import DLY\nlat, lon = 35.890, -78.750\nstart_date, end_date = '2015-01-01', '2019-12-31'\ndf = get_daymet_data(lat, lon, start_date, end_date)\nDLY(df).save('./weather/NCRDU.DLY')\n</code></pre></li> <li>For detailed guidance on creating the Operation Schedule (.OPC) file, including using the <code>generate_opc</code> command, refer to the Operation Schedule File Section in the getting started menu. Example command:     <pre><code>geo_epic generate_opc -c crop_data.csv -t crop_templates -o ./files\n</code></pre> Successfully preparing these files provides the model with all necessary inputs for the specific site simulation.</li> </ul> </li> <li> <p>Run Simulation:     Once the input files are ready, this step involves configuring the simulation parameters (like start date, duration, desired outputs) and executing the EPIC model for your site. For detailed instructions on using the <code>Site</code> and <code>EPICModel</code> classes, setting parameters, and calling the <code>model.run(site)</code> method, please refer to the Running Simulations Guide. This step performs the core simulation process based on the prepared inputs.</p> </li> <li> <p>Process Outputs:     After the simulation completes, this step involves reading, analyzing, and visualizing the results generated by the EPIC model. The output files contain valuable information about crop yields, soil conditions, water balance, and other environmental metrics.</p> <ul> <li> <p>Reading Output Files:     EPIC generates various output files. Key ones include ACY (Annual Crop Yield) and DGN (Daily General). Use <code>geoEpic.io</code> classes to read them:     <pre><code>from geoEpic.io import ACY, DGN\nimport pandas as pd\n\n# Assuming 'site' object exists and model has run\n# site.outputs dictionary holds paths to output files.\n\n# Access Annual Crop Yield data\nyields = ACY(site.outputs['ACY']).get_var('YLDG')\nprint(yields)\n</code></pre>     Example output:     <pre><code>  index    YR   CPNM    YLDG\n0      0  2015   CORN   7.175\n1      1  2016   CORN   4.735\n2      2  2017   CORN   9.072\n3      3  2018   CORN   7.829\n4      4  2019   CORN   5.434\n</code></pre></p> </li> <li> <p>Visualizing Results:     Create plots to understand trends. Example: Plotting annual yields:     <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.bar(yields['YR'], yields['YLDG'], color='green')\nplt.title('Annual Corn Yield (2015-2019)')\nplt.xlabel('Year')\nplt.ylabel('Yield (t/ha)')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.xticks(yields['YR'])\nplt.tight_layout()\nplt.show()\n</code></pre>     Example: Plotting daily Leaf Area Index (LAI):     <pre><code># Access daily LAI data\nlai = DGN(site.outputs['DGN']).get_var('LAI')\n\nplt.figure(figsize=(12, 6))\nplt.plot(lai['Date'], lai['LAI'], color='darkgreen', linewidth=2)\nplt.title('Leaf Area Index (LAI) Over Time')\nplt.xlabel('Date')\nplt.ylabel('LAI')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</code></pre></p> </li> <li> <p>Advanced Analysis:     Combine different output variables for deeper insights. Example: Merging precipitation and soil water:     <pre><code># Extract precipitation and soil moisture\ndgn_data = DGN(site.outputs['DGN'])\nprecip = dgn_data.get_var('PRCP')\nsoil_water = dgn_data.get_var('SW') # Example variable code\n\n# Merge data\nmerged_data = pd.merge(precip, soil_water, on='Date')\n\n# Add your analysis code here...\nprint(merged_data.head())\n</code></pre></p> </li> <li> <p>Exporting Results:     Save your processed data for reports or further use:     <pre><code># Export annual yields to CSV\nyields.to_csv('corn_yields_2015_2019.csv', index=False)\n\n# Export daily data to Excel\nwith pd.ExcelWriter('simulation_results.xlsx') as writer:\n   yields.to_excel(writer, sheet_name='Annual_Yields', index=False)\n   lai.to_excel(writer, sheet_name='Daily_LAI', index=False)\n   # merged_data.to_excel(writer, sheet_name='Daily_Weather_Soil', index=False)\n</code></pre> This final step allows you to interpret the simulation outcomes and draw conclusions about the site's behavior under the simulated conditions.</p> </li> </ul> </li> </ol>"},{"location":"tutorials/yield_calibration/","title":"GeoEPIC Yield Calibration Tutorial","text":"<p>This tutorial outlines the steps to perform yield calibration for multiple sites using GeoEPIC, leveraging the setup from regional simulations. Calibration fine-tunes model parameters to better match simulated outputs (like crop yield) with observed data across a region.</p> <ol> <li> <p>Set Up the Workspace:     Before starting calibration, you must have a properly organized workspace containing all necessary input files (Site, Soil, Weather, Operations) for the multiple sites in your calibration region. Follow the procedures detailed in the Regional Simulation Tutorial, specifically Steps 1-3 (Set Up Environment, Prepare Input Files, Set Up Workspace). This ensures you have:</p> <ul> <li>A configured <code>geoEpic</code> environment.</li> <li>All required <code>.SIT</code>, <code>.SOL</code>, <code>.DLY</code>, and <code>.OPC</code> files for each site.</li> <li>A well-structured workspace directory (e.g., <code>workspace/</code>) containing these inputs and the EPIC model executable.</li> </ul> </li> <li> <p>Perform Calibration:     The core calibration process involves selecting parameters to adjust (e.g., crop parameters in <code>.OPC</code> or site/soil parameters), defining an objective function (how to measure the match between simulated and observed yields), and using a calibration algorithm or workflow to find optimal parameter values.     For detailed, step-by-step instructions on:</p> <ul> <li>Preparing observed yield data.</li> <li>Selecting appropriate parameters for calibration.</li> <li>Configuring and running a calibration workflow or tool (potentially using libraries like <code>spotpy</code> or custom <code>geoEpic</code> calibration utilities).</li> <li>Evaluating calibration performance. Please refer to the dedicated Calibration Procedure Guide. This guide provides the specific commands, scripts, and methodologies required to execute the yield calibration.</li> </ul> </li> <li> <p>Access Calibrated Parameters:     Once the calibration process (as described in the Calibration Procedure Guide) is complete, the optimized parameter values will typically be stored in output files or potentially within Python objects generated by the calibration script.</p> <ul> <li>The specific format and location depend on the calibration tool or script used. It might be a CSV file, a JSON file, or directly printed to the console. Example conceptual access (actual implementation depends on the calibration tool):   ``   # Example: Assuming calibration results are saved to a file   import pandas as pd</li> </ul> <p>try:       # Load parameters maybe saved by the calibration tool       calibrated_params = pd.read_csv('./workspace/calibration_outputs/best_parameters.csv')       print(\"Calibrated Parameters:\")       print(calibrated_params)</p> <pre><code>  # Or access parameters if stored in a Python object (conceptual)\n  # best_run_parameters = calibration_result.best_parameters\n  # print(best_run_parameters)\n</code></pre> <p>except FileNotFoundError:       print(\"Calibration output file not found. Check calibration process outputs.\")   except Exception as e:       print(f\"Error accessing calibration results: {e}\")</p> <p>`` Refer back to the Calibration Procedure Guide for specifics on where and how to retrieve the final calibrated parameter set generated by your chosen calibration method. These parameters can then be used for subsequent validated regional simulations.</p> </li> </ol>"}]}